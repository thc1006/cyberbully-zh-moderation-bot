# Semi-supervised Learning Configuration for Chinese Cyberbullying Detection
# Optimized for RTX 3050 4GB GPU with mixed precision training

# Model Configuration
model:
  name: "hfl/chinese-macbert-base"  # or "hfl/chinese-roberta-wwm-ext"
  num_labels_toxicity: 3  # none, toxic, severe
  num_labels_emotion: 3   # pos, neu, neg
  dropout_rate: 0.1
  max_length: 512

# Training Configuration
training:
  batch_size: 4  # Base batch size - will be adjusted dynamically
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 500
  gradient_accumulation_steps: 4  # Effective batch size = 4 * 4 = 16
  max_grad_norm: 1.0

  # Mixed precision training for memory efficiency
  use_fp16: true
  fp16_opt_level: "O1"

# Memory Optimization
memory:
  enable_gradient_checkpointing: true
  max_memory_fraction: 0.85  # Use 85% of GPU memory
  cache_cleanup_frequency: 50  # Clear cache every 50 batches

# Pseudo-labeling Configuration
pseudo_labeling:
  confidence_threshold: 0.9
  min_confidence_threshold: 0.7
  max_confidence_threshold: 0.95
  threshold_decay: 0.98
  max_pseudo_samples: 8000  # Reduced for memory constraints
  validation_metric: "f1_macro"
  early_stopping_patience: 3
  num_iterations: 5
  epochs_per_iteration: 2

  # Class balancing
  class_balance: true
  selection_strategy: "confidence"  # confidence, entropy, margin

# Self-training Configuration
self_training:
  teacher_update_frequency: 200  # Reduced frequency for memory
  student_teacher_ratio: 0.7
  distillation_temperature: 4.0
  ema_decay: 0.999
  consistency_weight: 1.0
  confidence_threshold: 0.8
  max_epochs: 8
  warmup_steps: 500

# Consistency Regularization Configuration
consistency:
  consistency_weight: 1.0
  consistency_ramp_up_epochs: 3
  max_consistency_weight: 5.0  # Reduced to prevent overwhelming
  augmentation_strength: 0.1
  temperature: 1.0
  use_confidence_masking: true
  confidence_threshold: 0.8
  max_epochs: 10

# Data Augmentation (for consistency training)
augmentation:
  token_dropout_prob: 0.1
  token_shuffle_prob: 0.05
  synonym_replacement_prob: 0.1
  max_augmentations_per_sample: 2

# Evaluation Configuration
evaluation:
  eval_frequency: 500  # Evaluate every N steps
  save_top_k: 3  # Save top 3 models
  metric_for_best_model: "f1_macro"

# Dataset Configuration
data:
  labeled_data_path: "data/processed/labeled_data.jsonl"
  unlabeled_data_path: "data/processed/unlabeled_data.jsonl"
  validation_data_path: "data/processed/validation_data.jsonl"
  test_data_path: "data/processed/test_data.jsonl"

  # Data splits
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1

  # Preprocessing
  max_length: 512
  truncation: true
  padding: "max_length"

# Logging Configuration
logging:
  log_level: "INFO"
  log_dir: "logs/semi_supervised"
  wandb:
    enabled: false  # Set to true if using wandb
    project: "cyberpuppy-semi-supervised"
    entity: "your-wandb-entity"

# Output Configuration
output:
  model_dir: "models/semi_supervised"
  checkpoint_dir: "checkpoints/semi_supervised"
  results_dir: "results/semi_supervised"

# Hardware-specific Optimizations (RTX 3050 4GB)
hardware:
  device: "cuda"
  num_workers: 2  # Reduced for memory
  pin_memory: true
  persistent_workers: true

  # Memory management
  empty_cache_frequency: 100
  max_split_size_mb: 128  # Prevent memory fragmentation

# Advanced Features
advanced:
  # Uncertainty estimation
  use_mc_dropout: true
  mc_dropout_passes: 5

  # Model ensembling
  ensemble_models: false
  ensemble_size: 3

  # Progressive training
  progressive_unfreezing: false
  freeze_embeddings: false

# Experiment Tracking
experiment:
  name: "semi_supervised_chinese_cyberbullying"
  description: "Semi-supervised learning for Chinese cyberbullying detection using pseudo-labeling, self-training, and consistency regularization"
  tags: ["semi-supervised", "chinese", "cyberbullying", "nlp"]

# Hyperparameter Search (optional)
hyperparameter_search:
  enabled: false
  method: "random"  # random, grid, bayesian
  n_trials: 20

  # Search space
  search_space:
    learning_rate: [1e-5, 5e-5]
    consistency_weight: [0.5, 2.0]
    confidence_threshold: [0.7, 0.9]
    distillation_temperature: [2.0, 6.0]