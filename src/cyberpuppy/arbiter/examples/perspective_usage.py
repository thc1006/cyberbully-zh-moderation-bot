"""
Perspective API ä½¿ç”¨ç¯„ä¾‹
å±•ç¤ºå¦‚ä½•æ•´åˆ Perspective API åˆ° CyberPuppy ç³»çµ±ä¸­
"""

import asyncio
import logging
import os

from ..integration import validate_with_arbiter
# å°å…¥ CyberPuppy æ¨¡çµ„
from ..perspective import PerspectiveAPI, UncertaintyDetector

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def basic_perspective_example():
    """åŸºæœ¬ Perspective API ä½¿ç”¨ç¯„ä¾‹"""

    logger.info("=== åŸºæœ¬ Perspective API ä½¿ç”¨ç¯„ä¾‹ ===")

    # æª¢æŸ¥ API Key
    api_key = os.getenv("PERSPECTIVE_API_KEY")
    if not api_key:
        logger.warning("æœªè¨­å®š PERSPECTIVE_API_KEYï¼Œè·³éæ­¤ç¯„ä¾‹")
        return

    # æ¸¬è©¦æ–‡æœ¬
    test_texts = [
        "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼",  # æ­£é¢æ–‡æœ¬
        "é€™å€‹ç”¢å“çœŸçš„å¾ˆç³Ÿç³•",  # è¼•å¾®è² é¢
        "ä½ é€™å€‹ç™½ç—´ï¼Œçµ¦æˆ‘æ»¾é–‹ï¼",  # æ˜é¡¯æ¯’æ€§
        "I hate you and want to hurt you badly",  # è‹±æ–‡å¨è„…æ€§æ–‡æœ¬
    ]

    async with PerspectiveAPI(api_key=api_key) as perspective:

        for i, text in enumerate(test_texts, 1):
            logger.info(f"\n--- æ¸¬è©¦æ–‡æœ¬ {i}: {text} ---")

            try:
                # åˆ†ææ–‡æœ¬
                result = await perspective.analyze_comment(
                    text=text,
                    lang=("zh" if any(ord(c) > 127 for c in text) else "en"),
                )

                # é¡¯ç¤ºçµæœ
                logger.info(f"æ¯’æ€§åˆ†æ•¸: {result.toxicity_score:.3f}")
                logger.info(f"åš´é‡æ¯’æ€§: {result.severe_toxicity_score:.3f}")
                logger.info(f"å¨è„…åˆ†æ•¸: {result.threat_score:.3f}")
                logger.info(f"ä¾®è¾±åˆ†æ•¸: {result.insult_score:.3f}")
                logger.info(f"è™•ç†æ™‚é–“: {result.processing_time_ms:.1f}ms")

                if result.language_detected:
                    logger.info(f"æª¢æ¸¬èªè¨€: {result.language_detected}")

                # åˆ†æä¿¡å¿ƒåº¦
                max_score = max(
                    result.toxicity_score,
                    result.severe_toxicity_score,
                    result.threat_score,
                    result.insult_score,
                )

                if max_score > 0.7:
                    logger.info("âš ï¸  é«˜æ¯’æ€§é¢¨éšª")
                elif max_score > 0.4:
                    logger.info("âš¡ ä¸­ç­‰æ¯’æ€§é¢¨éšª")
                else:
                    logger.info("âœ… ä½æ¯’æ€§é¢¨éšª")

            except Exception as e:
                logger.error(f"åˆ†æå¤±æ•—: {e}")

        # å–å¾—é…é¡ç‹€æ…‹
        quota = await perspective.get_quota_status()
        logger.info("\n=== API é…é¡ç‹€æ…‹ ===")
        logger.info(f"ä»Šæ—¥å·²ä½¿ç”¨: {quota['daily_requests_used']}")
        logger.info(f"æ¯æ—¥é™åˆ¶: {quota['daily_requests_limit']}")


async def uncertainty_detection_example():
    """ä¸ç¢ºå®šæ€§æª¢æ¸¬ç¯„ä¾‹"""

    logger.info("\n=== ä¸ç¢ºå®šæ€§æª¢æ¸¬ç¯„ä¾‹ ===")

    detector = UncertaintyDetector(
        uncertainty_threshold=0.4, confidence_threshold=0.6, min_confidence_gap=0.1
    )

    # æ¨¡æ“¬ä¸åŒçš„æœ¬åœ°æ¨¡å‹é æ¸¬æƒ…æ³
    test_cases = [
        {
            "name": "é«˜ä¿¡å¿ƒåº¦é æ¸¬ï¼ˆç„¡éœ€å¤–éƒ¨é©—è­‰ï¼‰",
            "prediction": {
                "toxicity": "none",
                "scores": {"toxicity": {"none": 0.85, "toxic": 0.12, "severe": 0.03}},
                "emotion": "neu",
            },
        },
        {
            "name": "é‚Šç•Œåˆ†æ•¸ï¼ˆéœ€è¦å¤–éƒ¨é©—è­‰ï¼‰",
            "prediction": {
                "toxicity": "none",
                "scores": {"toxicity": {"none": 0.45, "toxic": 0.4, "severe": 0.15}},
                "emotion": "neu",
            },
        },
        {
            "name": "æƒ…ç·’è¡çªä¿¡è™Ÿï¼ˆéœ€è¦é©—è­‰ï¼‰",
            "prediction": {
                "toxicity": "none",
                "scores": {"toxicity": {"none": 0.8, "toxic": 0.15, "severe": 0.05}},
                "emotion": "neg",
                "emotion_strength": 4,
            },
        },
        {
            "name": "ä½ä¿¡å¿ƒåº¦å·®è·",
            "prediction": {
                "toxicity": "toxic",
                "scores": {"toxicity": {"none": 0.48, "toxic": 0.47, "severe": 0.05}},
                "emotion": "neu",
            },
        },
    ]

    for case in test_cases:
        logger.info(f"\n--- {case['name']} ---")

        should_use, analysis = detector.should_use_perspective(case["prediction"])

        logger.info(f"æ˜¯å¦éœ€è¦å¤–éƒ¨é©—è­‰: {should_use}")
        logger.info(f"ä¿¡å¿ƒåº¦åˆ†æ•¸: {analysis.confidence_score:.3f}")
        logger.info(f"ä¸ç¢ºå®šæ€§åŸå› : {[reason.value for reason in analysis.reasons]}")
        logger.info(f"å»ºè­°: {analysis.recommendation}")

        if analysis.threshold_details:
            logger.info(f"é–¾å€¼è©³æƒ…: {analysis.threshold_details}")


async def integrated_validation_example():
    """æ•´åˆé©—è­‰ç¯„ä¾‹"""

    logger.info("\n=== æ•´åˆé©—è­‰ç¯„ä¾‹ ===")

    # æ¨¡æ“¬éœ€è¦å¤–éƒ¨é©—è­‰çš„æƒ…æ³
    uncertain_prediction = {
        "toxicity": "none",
        "bullying": "none",
        "role": "none",
        "emotion": "neg",
        "emotion_strength": 3,
        "scores": {"toxicity": {"none": 0.45, "toxic": 0.4, "severe": 0.15}},
    }

    test_texts = [
        "é€™å€‹ç”¢å“çœŸçš„å¾ˆçˆ›ï¼Œå®Œå…¨ä¸æ¨è–¦",
        "ä½ ç‚ºä»€éº¼è¦é€™æ¨£å°æˆ‘ï¼Ÿå¥½é›£é...",
        "å»æ­»å§ï¼Œæˆ‘æ¨æ­»ä½ äº†ï¼",
    ]

    for text in test_texts:
        logger.info(f"\n--- åˆ†ææ–‡æœ¬: {text} ---")

        try:
            enhanced_prediction, metadata = await validate_with_arbiter(
                text=text, local_prediction=uncertain_prediction.copy()
            )

            logger.info(f"åŸå§‹æ¯’æ€§é æ¸¬: {uncertain_prediction['toxicity']}")
            logger.info(f"æ˜¯å¦ä½¿ç”¨å¤–éƒ¨é©—è­‰: {metadata['used_external_validation']}")

            if metadata["uncertainty_analysis"]:
                ua = metadata["uncertainty_analysis"]
                logger.info(
                    f"ä¸ç¢ºå®šæ€§åˆ†æ: {ua['is_uncertain']} " f"({ua['confidence_score']:.3f})"
                )
                logger.info(f"ä¸ç¢ºå®šåŸå› : {ua['reasons']}")

            if metadata["used_external_validation"]:
                pr = metadata["perspective_result"]
                logger.info(f"Perspective æ¯’æ€§åˆ†æ•¸: {pr['toxicity_score']:.3f}")
                logger.info(f"Perspective å¨è„…åˆ†æ•¸: {pr['threat_score']:.3f}")
                logger.info(f"ä¿¡å¿ƒåº¦è©•ä¼°: {pr['confidence_assessment']['confidence_level']}")

                if "confidence_adjustment" in enhanced_prediction:
                    logger.info(f"ä¿¡å¿ƒåº¦èª¿æ•´: {enhanced_prediction['confidence_adjustment']}")
                    logger.info(f"é©—è­‰å‚™è¨»: {enhanced_prediction['validation_note']}")

            logger.info(f"æœ€çµ‚å»ºè­°: {metadata['recommendation']}")

        except Exception as e:
            logger.error(f"æ•´åˆé©—è­‰å¤±æ•—: {e}")


async def performance_analysis_example():
    """æ•ˆèƒ½åˆ†æç¯„ä¾‹"""

    logger.info("\n=== æ•ˆèƒ½åˆ†æç¯„ä¾‹ ===")

    # æª¢æŸ¥ API Key
    if not os.getenv("PERSPECTIVE_API_KEY"):
        logger.warning("æœªè¨­å®š PERSPECTIVE_API_KEYï¼Œè·³éæ•ˆèƒ½æ¸¬è©¦")
        return

    import time

    async with PerspectiveAPI() as perspective:

        # æ¸¬è©¦å¤šå€‹è«‹æ±‚çš„æ•ˆèƒ½
        test_texts = [
            "æ¸¬è©¦æ–‡æœ¬ 1",
            "Testing text 2",
            "ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ 3",
            "Texto de prueba 4",
            "Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ 5",
        ]

        logger.info("åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦...")
        start_time = time.time()

        results = []
        for i, text in enumerate(test_texts, 1):
            logger.info(f"è™•ç†æ–‡æœ¬ {i}/{len(test_texts)}")

            try:
                result = await perspective.analyze_comment(text)
                results.append(result)
                logger.info(f"  è™•ç†æ™‚é–“: {result.processing_time_ms:.1f}ms")

            except Exception as e:
                logger.error(f"  è™•ç†å¤±æ•—: {e}")

        total_time = time.time() - start_time
        logger.info("\næ•ˆèƒ½çµ±è¨ˆ:")
        logger.info("ç¸½è™•ç†æ™‚é–“: %.2fs" % total_time)
        logger.info("å¹³å‡æ¯å€‹è«‹æ±‚: %.2fs" % (total_time / len(test_texts)))

        if results:
            avg_api_time = sum(r.processing_time_ms for r in results) / len(results)
            logger.info(f"å¹³å‡ API è™•ç†æ™‚é–“: {avg_api_time:.1f}ms")

        # é…é¡ä½¿ç”¨æƒ…æ³
        quota = await perspective.get_quota_status()
        logger.info(
            f"API é…é¡ä½¿ç”¨: {quota['daily_requests_used']}/" f"{quota['daily_requests_limit']}"
        )


async def main():
    """ä¸»è¦ç¯„ä¾‹åŸ·è¡Œå‡½å¼"""

    logger.info("ğŸ• CyberPuppy Perspective API æ•´åˆç¯„ä¾‹")
    logger.info("=" * 50)

    try:
        # åŸ·è¡Œæ‰€æœ‰ç¯„ä¾‹
        await basic_perspective_example()
        await uncertainty_detection_example()
        await integrated_validation_example()
        await performance_analysis_example()

        logger.info("\nâœ… æ‰€æœ‰ç¯„ä¾‹åŸ·è¡Œå®Œæˆ")

    except KeyboardInterrupt:
        logger.info("\nâŒ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logger.error(f"\nğŸ’¥ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    asyncio.run(main())
