"""
è³‡æ–™ç®¡é“æ•´åˆæ¸¬è©¦
æ¸¬è©¦å®Œæ•´çš„è³‡æ–™è™•ç†æµç¨‹ï¼š
- è³‡æ–™ä¸‹è¼‰ â†’ æ¸…ç† â†’ æ­£è¦åŒ– â†’ æ¨™ç±¤çµ±ä¸€ â†’ è¨“ç·´/é©—è­‰åˆ†å‰²
- æ¨¡å‹è¨“ç·´ â†’ è©•ä¼° â†’ å„²å­˜
- æ•´é«”ç®¡é“å®Œæ•´æ€§èˆ‡éŒ¯èª¤è™•ç†
"""

import json
import subprocess
from pathlib import Path

import pytest

from tests.integration import PROJECT_ROOT, TIMEOUT_SECONDS


@pytest.mark.pipeline
@pytest.mark.slow
class TestDataPipeline:
    """å®Œæ•´è³‡æ–™ç®¡é“æ¸¬è©¦"""

    def test_download_clean_normalize_pipeline(self, temp_dir):
        """æ¸¬è©¦ä¸‹è¼‰ â†’ æ¸…ç† â†’ æ­£è¦åŒ–ç®¡é“"""
        raw_dir = temp_dir / "raw"
        processed_dir = temp_dir / "processed"
        raw_dir.mkdir()
        processed_dir.mkdir()

        # å»ºç«‹æ¨¡æ“¬åŸå§‹è³‡æ–™
        raw_data = [
            {"text": "ä½ å¥½ï¼ä»Šå¤©å¤©æ°£çœŸå¥½ğŸ˜Š", "label": "æ­£é¢"},
            {"text": "é€™å€‹ç¬¨è›‹çœŸè¨å­", "label": "è² é¢"},
            {"text": "æˆ‘å¾ˆç”Ÿæ°£ï¼Œæƒ³æ‰“äºº", "label": "è² é¢"},
            {"text": "è¬è¬ä½ çš„å¹«åŠ©", "label": "æ­£é¢"},
            {"text": "   ç©ºç™½æ¸¬è©¦   ", "label": "ä¸­æ€§"}
        ]

        raw_file = raw_dir / "test_data.jsonl"
        with open(raw_file, "w", encoding="utf-8") as f:
            for item in raw_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        # åŸ·è¡Œæ¸…ç†è…³æœ¬
        clean_script = PROJECT_ROOT / "scripts" / "clean_normalize.py"
        if clean_script.exists():
            result = subprocess.run([
                "python", str(clean_script),
                "--input", str(raw_file),
                "--output", str(processed_dir / "cleaned.jsonl"),
                "--normalize", "--remove-duplicates"
            ], capture_output=True, text=True, timeout=TIMEOUT_SECONDS)

            if result.returncode == 0:
                # é©—è­‰æ¸…ç†çµæœ
                cleaned_file = processed_dir / "cleaned.jsonl"
                assert cleaned_file.exists()

                with open(cleaned_file, "r", encoding="utf-8") as f:
                    cleaned_data = [json.loads(line) for line in f]

                # é©—è­‰è³‡æ–™æ¸…ç†æ•ˆæœ
                assert len(cleaned_data) > 0
                for item in cleaned_data:
                    assert "text" in item
                    # æª¢æŸ¥æ–‡æœ¬å·²æ¸…ç†ï¼ˆå»é™¤å¤šé¤˜ç©ºç™½ï¼‰
                    assert item["text"].strip() == item["text"]

    def test_label_mapping_pipeline(self, temp_dir):
        """æ¸¬è©¦æ¨™ç±¤æ˜ å°„èˆ‡çµ±ä¸€ç®¡é“"""
        # å»ºç«‹ä¸åŒä¾†æºçš„è³‡æ–™ï¼Œæ¨¡æ“¬ä¸åŒæ¨™ç±¤æ ¼å¼
        data_sources = [
            # COLD æ ¼å¼
            {"text": "ä½ å¾ˆç¬¨", "toxicity": 1, "source": "cold"},
            {"text": "ä»Šå¤©å¤©æ°£å¥½", "toxicity": 0, "source": "cold"},

            # ChnSentiCorp æ ¼å¼
            {"text": "å¾ˆå¥½çš„ç”¢å“", "sentiment": "positive", "source": "chnsenti"},
            {"text": "ç³Ÿç³•çš„æœå‹™", "sentiment": "negative", "source": "chnsenti"},

            # è‡ªå®šç¾©æ ¼å¼
            {"text": "æˆ‘è¦æä½ ", "label": "threat", "source": "custom"},
        ]

        input_file = temp_dir / "mixed_labels.jsonl"
        with open(input_file, "w", encoding="utf-8") as f:
            for item in data_sources:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        # åŸ·è¡Œæ¨™ç±¤çµ±ä¸€è…³æœ¬
        mapping_script = PROJECT_ROOT / "scripts" / "label_mapping.py" 
        output_file = temp_dir / "unified_labels.jsonl"

        if mapping_script.exists():
            result = subprocess.run([
                "python", str(mapping_script),
                "--input", str(input_file),
                "--output", str(output_file)
            ], capture_output=True, text=True, timeout=TIMEOUT_SECONDS)

            if result.returncode == 0:
                # é©—è­‰æ¨™ç±¤çµ±ä¸€çµæœ
                assert output_file.exists()

                with open(output_file, "r", encoding="utf-8") as f:
                    unified_data = [json.loads(line) for line in f]

                # æª¢æŸ¥çµ±ä¸€æ¨™ç±¤æ ¼å¼
                for item in unified_data:
                    assert "text" in item
                    # æ‡‰è©²åŒ…å«çµ±ä¸€çš„æ¨™ç±¤æ¬„ä½
                    expected_fields = ["toxicity", "bullying", "emotion"]
                    for field in expected_fields:
                        if field in item:
                            assert item[field] in ["no"
                                "ne", 
                                   item[field] in ["no"
                                       "ne", 
                                   item[field] in ["pos", "neu", "neg"]

    def test_train_test_split_pipeline(self, temp_dir):
        """æ¸¬è©¦è¨“ç·´/æ¸¬è©¦è³‡æ–™åˆ†å‰²ç®¡é“"""
        # å»ºç«‹æ¸¬è©¦è³‡æ–™
        train_data = []
        for i in range(100):
            train_data.append({
                "text": f"æ¸¬è©¦æ–‡æœ¬ {i}",
                "toxicity": "toxic" if i % 3 == 0 else "none",
                "emotion": "neg" if i % 3 == 0 else "pos",
                "bullying": "harassment" if i % 5 == 0 else "none"
            })

        full_data_file = temp_dir / "full_dataset.jsonl"
        with open(full_data_file, "w", encoding="utf-8") as f:
            for item in train_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        # åŸ·è¡Œåˆ†å‰²è…³æœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        split_script = PROJECT_ROOT / "scripts" / "split_train_test.py"
        if split_script.exists():
            result = subprocess.run([
                "python", str(split_script),
                "--input", str(full_data_file),
                "--output-dir", str(temp_dir),
                "--train-ratio", "0.8",
                "--stratify"
            ], capture_output=True, text=True, timeout=TIMEOUT_SECONDS)

            if result.returncode == 0:
                # é©—è­‰åˆ†å‰²çµæœ
                train_file = temp_dir / "train.jsonl"
                test_file = temp_dir / "test.jsonl"

                assert train_file.exists()
                assert test_file.exists()

                # æª¢æŸ¥åˆ†å‰²æ¯”ä¾‹
                with open(train_file, "r", encoding="utf-8") as f:
                    train_count = sum(1 for _ in f)
                with open(test_file, "r", encoding="utf-8") as f:
                    test_count = sum(1 for _ in f)

                total_count = train_count + test_count
                train_ratio = train_count / total_count
                assert 0.75 <= train_ratio <= 0.85  # å…è¨±å°èª¤å·®


@pytest.mark.pipeline
@pytest.mark.slow
class TestModelTrainingPipeline:
    """æ¨¡å‹è¨“ç·´ç®¡é“æ¸¬è©¦"""

    def test_training_pipeline_e2e(self, temp_dir, trained_model_path):
        """æ¸¬è©¦ç«¯åˆ°ç«¯è¨“ç·´ç®¡é“"""
        # å»ºç«‹è¨“ç·´è³‡æ–™
        train_data = self._create_training_data(100)
        test_data = self._create_training_data(20)

        train_file = temp_dir / "train.jsonl"
        test_file = temp_dir / "test.jsonl"

        with open(train_file, "w", encoding="utf-8") as f:
            for item in train_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        with open(test_file, "w", encoding="utf-8") as f:
            for item in test_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        # åŸ·è¡Œè¨“ç·´è…³æœ¬
        train_script = PROJECT_ROOT / "train.py"
        model_output_dir = temp_dir / "models"

        if train_script.exists():
            result = subprocess.run([
                "python", str(train_script),
                "--train-data", str(train_file),
                "--test-data", str(test_file),
                "--output-dir", str(model_output_dir),
                "--epochs", "1",  # å¿«é€Ÿæ¸¬è©¦
                "--model-name", "test_model",
                "--device", "cpu"  # å¼·åˆ¶ä½¿ç”¨ CPU
            ], capture_output=True, text=True, timeout=TIMEOUT_SECONDS * 10)

            if result.returncode == 0:
                # é©—è­‰æ¨¡å‹æª”æ¡ˆç”Ÿæˆ
                assert model_output_dir.exists()

                # æª¢æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æª”æ¡ˆ
                model_files = list(model_output_dir.glob("**/*.pth")) + \
                             list(model_output_dir.glob("**/*.bin")) + \
                             list(model_output_dir.glob("**/*.safetensors"))

                assert len(model_files) > 0, "No model files found"

                # æª¢æŸ¥é…ç½®æª”æ¡ˆ
                config_files = list(model_output_dir.glob("**/config.json"))
                assert len(config_files) > 0, "No config files found"

    def test_evaluation_pipeline(self, temp_dir):
        """æ¸¬è©¦è©•ä¼°ç®¡é“"""
        # å»ºç«‹è©•ä¼°è³‡æ–™
        eval_data = self._create_training_data(50)
        eval_file = temp_dir / "eval.jsonl"

        with open(eval_file, "w", encoding="utf-8") as f:
            for item in eval_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        # åŸ·è¡Œè©•ä¼°è…³æœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        eval_script = PROJECT_ROOT / "scripts" / "evaluate_model.py"
        results_file = temp_dir / "eval_results.json"

        if eval_script.exists():
            result = subprocess.run([
                "python", str(eval_script),
                "--data", str(eval_file),
                "--model", "dummy",  # ä½¿ç”¨è™›æ“¬æ¨¡å‹
                "--output", str(results_file)
            ], capture_output=True, text=True, timeout=TIMEOUT_SECONDS * 2)

            if result.returncode == 0:
                # é©—è­‰è©•ä¼°çµæœ
                assert results_file.exists()

                with open(results_file, "r") as f:
                    results = json.load(f)

                # æª¢æŸ¥è©•ä¼°æŒ‡æ¨™
                expected_metrics = ["accuracy", "precision", "recall", "f1"]
                for metric in expected_metrics:
                    if metric in results:
                        assert isinstance(results[metric], (int, float))
                        assert 0 <= results[metric] <= 1

    def _create_training_data(self, count: int) -> List[Dict[str, Any]]:
        """å»ºç«‹è¨“ç·´è³‡æ–™"""
        data = []
        patterns = [
            {"text": "This is a normal text", "toxicity": "none", "emotion": "neu"},
            {"text": "This is toxic content", "toxicity": "toxic", "emotion": "neg"},
            {"text": "Happy positive message", "toxicity": "none", "emotion": "pos"},
            {"text": "Severe bullying text", "toxicity": "severe", "emotion": "neg"},
        ]

        for i in range(count):
            pattern = patterns[i % len(patterns)]
            data.append({
                "text": f"{pattern['text']} {i}",
                "toxicity": pattern["toxicity"],
                "emotion": pattern["emotion"],
                "bullying": pattern["bullying"]
            })

        return data


@pytest.mark.pipeline
class TestPipelineIntegrity:
    """ç®¡é“å®Œæ•´æ€§æ¸¬è©¦"""

    def test_pipeline_error_handling(self, temp_dir):
        """æ¸¬è©¦ç®¡é“éŒ¯èª¤è™•ç†"""
        # æ¸¬è©¦ç©ºæª”æ¡ˆè™•ç†
        empty_file = temp_dir / "empty.jsonl"
        empty_file.touch()

        # æ¸¬è©¦ç„¡æ•ˆæ ¼å¼æª”æ¡ˆè™•ç†
        invalid_file = temp_dir / "invalid.jsonl"
        invalid_file.write_text("not json content", encoding="utf-8")

        # åŸ·è¡Œè™•ç†è…³æœ¬ï¼Œæ‡‰è©²èƒ½è™•ç†éŒ¯èª¤
        clean_script = PROJECT_ROOT / "scripts" / "clean_normalize.py"
        if clean_script.exists():
            # æ¸¬è©¦ç©ºæª”æ¡ˆ
            result = subprocess.run([
                "python", str(clean_script),
                "--input", str(empty_file),
                "--output", str(temp_dir / "empty_output.jsonl")
            ], capture_output=True, text=True, timeout=TIMEOUT_SECONDS)

            # æ‡‰è©²æœ‰é©ç•¶çš„éŒ¯èª¤è™•ç†ï¼Œä¸æœƒå´©æ½°
            assert result.returncode == 0 or "empty" in result.stderr.lower()

            # æ¸¬è©¦ç„¡æ•ˆæª”æ¡ˆ
            result = subprocess.run([
                "python", str(clean_script),
                "--input", str(invalid_file),
                "--output", str(temp_dir / "invalid_output.jsonl")
            ], capture_output=True, text=True, timeout=TIMEOUT_SECONDS)

            # æ‡‰è©²æœ‰é©ç•¶çš„éŒ¯èª¤è™•ç†
            assert result.returncode == 0 or "json" in result.stderr.lower()

    def test_pipeline_data_consistency(self, temp_dir):
        """æ¸¬è©¦ç®¡é“è³‡æ–™ä¸€è‡´æ€§"""
        # å»ºç«‹æ¸¬è©¦è³‡æ–™
        original_data = [
            {"text": "åŸå§‹æ–‡æœ¬1", "label": "positive"},
            {"text": "åŸå§‹æ–‡æœ¬2", "label": "negative"},
            {"text": "åŸå§‹æ–‡æœ¬3", "label": "neutral"}
        ]

        input_file = temp_dir / "consistency_input.jsonl"
        with open(input_file, "w", encoding="utf-8") as f:
            for item in original_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        # åŸ·è¡Œå¤šéšæ®µè™•ç†
        stage1_output = temp_dir / "stage1.jsonl"
        stage2_output = temp_dir / "stage2.jsonl"

        # éšæ®µ1ï¼šæ¸…ç†
        clean_script = PROJECT_ROOT / "scripts" / "clean_normalize.py"
        if clean_script.exists():
            subprocess.run([
                "python", str(clean_script),
                "--input", str(input_file),
                "--output", str(stage1_output)
            ], capture_output=True, timeout=TIMEOUT_SECONDS)

        # éšæ®µ2ï¼šæ¨™ç±¤çµ±ä¸€
        if stage1_output.exists():
            mapping_script = PROJECT_ROOT / "scripts" / "label_mapping.py" 
            if mapping_script.exists():
                subprocess.run([
                    "python", str(mapping_script),
                    "--input", str(stage1_output),
                    "--output", str(stage2_output)
                ], capture_output=True, timeout=TIMEOUT_SECONDS)

        # é©—è­‰è³‡æ–™ä¸€è‡´æ€§
        if stage2_output.exists():
            with open(stage2_output, "r", encoding="utf-8") as f:
                final_data = [json.loads(line) for line in f]

            # æª¢æŸ¥è³‡æ–™ç­†æ•¸ä¸€è‡´
            assert len(final_data) <= len(original_data)  # å¯èƒ½æœ‰é‡è¤‡è³‡æ–™è¢«ç§»é™¤

            # æª¢æŸ¥æ–‡æœ¬å…§å®¹ä¿æŒ
            original_texts = {item["text"] for item in original_data}
            final_texts = {item["te"
                "xt"] for item in final_data if 

            # è‡³å°‘æ‡‰è©²æœ‰éƒ¨åˆ†æ–‡æœ¬ä¿ç•™
            assert len(final_texts) > 0
            assert len(final_texts.intersection(original_texts)) > 0

    def test_pipeline_memory_efficiency(self, temp_dir):
        """æ¸¬è©¦ç®¡é“è¨˜æ†¶é«”æ•ˆç‡"""
        import psutil
        import os

        # å»ºç«‹è¼ƒå¤§çš„æ¸¬è©¦è³‡æ–™é›†
        large_data = []
        for i in range(1000):  # 1000 ç­†è³‡æ–™
            large_data.append({
                "text": f"å¤§é‡æ¸¬è©¦è³‡æ–™å…§å®¹ {'x' * 100} {i}",  # è¼ƒé•·çš„æ–‡æœ¬
                "label": "positive" if i % 2 == 0 else "negative"
            })

        large_file = temp_dir / "large_dataset.jsonl"
        with open(large_file, "w", encoding="utf-8") as f:
            for item in large_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        # ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
        process = subprocess.Popen([
            "python", "-c", f"""
import json
import time
input_file = '{large_file}'
output_file = '{temp_dir / "memory_test_output.jsonl"}'

# æ¨¡æ“¬è³‡æ–™è™•ç†
with open(input_file, 'r', encoding='utf-8') as f_in:
    with open(output_file, 'w', encoding='utf-8') as f_out:
        for line in f_in:
            data = json.loads(line)
            # ç°¡å–®è™•ç†
            data['processed'] = True
            f_out.write(json.dumps(data, ensure_ascii=False) + '\\n')
            time.sleep(0.001)  # æ¨¡æ“¬è™•ç†æ™‚é–“
"""
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        # ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
        max_memory = 0
        try:
            p = psutil.Process(process.pid)
            while process.poll() is None:
                try:
                    memory_info = p.memory_info()
                    max_memory = max(max_memory, memory_info.rss)
                except psutil.NoSuchProcess:
                    break
                time.sleep(0.1)

            process.wait(timeout=TIMEOUT_SECONDS)
        except subprocess.TimeoutExpired:
            process.kill()
            process.wait()

        # è¨˜æ†¶é«”ä½¿ç”¨æ‡‰è©²åˆç†ï¼ˆä¸è¶…é 200MBï¼‰
        max_memory_mb = max_memory / 1024 / 1024
        assert max_memory_mb < 200, f"Memory usage too high:"
            " {max_memory_mb:.2f}MB"

        # é©—è­‰è¼¸å‡ºæª”æ¡ˆ
        output_file = temp_dir / "memory_test_output.jsonl"
        if output_file.exists():
            with open(output_file, "r", encoding="utf-8") as f:
                processed_count = sum(1 for _ in f)
                assert processed_count == 1000


@pytest.mark.pipeline
@pytest.mark.slow
class TestFullSystemPipeline:
    """å®Œæ•´ç³»çµ±ç®¡é“æ¸¬è©¦"""

    async def test_end_to_end_system_pipeline(self, temp_dir, api_server):
        """æ¸¬è©¦ç«¯åˆ°ç«¯ç³»çµ±ç®¡é“"""
        # 1. å»ºç«‹åŸå§‹è³‡æ–™
        raw_data = self._create_realistic_dataset()
        raw_file = temp_dir / "raw_system_data.jsonl"

        with open(raw_file, "w", encoding="utf-8") as f:
            for item in raw_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")

        # 2. è³‡æ–™è™•ç†éšæ®µ
        processed_file = temp_dir / "processed_system_data.jsonl"
        self._simulate_data_processing(raw_file, processed_file)

        # 3. æ¨¡å‹è¨“ç·´éšæ®µï¼ˆç°¡åŒ–ï¼‰
        model_dir = temp_dir / "system_models"
        self._simulate_model_training(processed_file, model_dir)

        # 4. API éƒ¨ç½²æ¸¬è©¦
        if api_server:
            await self._test_api_integration(api_server, raw_data[:5])

        # 5. é©—è­‰æ•´é«”ç³»çµ±å®Œæ•´æ€§
        self._validate_system_integrity(model_dir, processed_file)

    def _create_realistic_dataset(self) -> List[Dict[str, Any]]:
        """å»ºç«‹ç¾å¯¦ä¸»ç¾©çš„æ¸¬è©¦è³‡æ–™é›†"""
        return [
            # æ­£é¢æ¡ˆä¾‹
            {"text": "ä»Šå¤©å¤©æ°£çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ£’ï¼", "expected_toxicity": "none"},
            {"text": "è¬è¬å¤§å®¶çš„æ”¯æŒå’Œé¼“å‹µ", "expected_toxicity": "none"},
            {"text": "é€™å€‹æƒ³æ³•å¾ˆæœ‰å‰µæ„", "expected_toxicity": "none"},

            # è¼•å¾®æ¯’æ€§æ¡ˆä¾‹
            {"text": "ä½ æ€éº¼é€™éº¼ç¬¨å•Š", "expected_toxicity": "toxic"},
            {"text": "çœŸæ˜¯å€‹å»¢ç‰©", "expected_toxicity": "toxic"},
            {"text": "æ»¾é–‹ï¼Œåˆ¥ç…©æˆ‘", "expected_toxicity": "toxic"},

            # åš´é‡æ¯’æ€§æ¡ˆä¾‹
            {"text": "æˆ‘è¦æ®ºäº†ä½ ", "expected_toxicity": "severe"},
            {"text": "å»æ­»å§ï¼Œæ²’äººæœƒæƒ³å¿µä½ ", "expected_toxicity": "severe"},
            {"text": "æˆ‘çŸ¥é“ä½ ä½å“ªè£¡ï¼Œå°å¿ƒé»", "expected_toxicity": "severe"},

            # é‚Šç•Œæ¡ˆä¾‹
            {"text": "é€™å€‹æ±ºå®šçœŸçš„è®“äººç”Ÿæ°£", "expected_toxicity": "none"},
            {"text": "æˆ‘å°æ­¤æ„Ÿåˆ°å¤±æœ›", "expected_toxicity": "none"},
            {"text": "èƒ½ä¸èƒ½èªçœŸé»ï¼Ÿ", "expected_toxicity": "none"},
        ]

    def _simulate_data_processing(self, input_file: Path, output_file: Path):
        """æ¨¡æ“¬è³‡æ–™è™•ç†éšæ®µ"""
        with open(input_file, "r", encoding="utf-8") as f_in:
            with open(output_file, "w", encoding="utf-8") as f_out:
                for line in f_in:
                    data = json.loads(line.strip())
                    # ç°¡å–®çš„è™•ç†é‚è¼¯
                    processed_data = {
                        "text": data["text"].strip(),
                        "toxicity": data.get("expected_toxicity", "none"),
                        "emo"
                            "tion": 
                                 "n"
                                     "eg" if 
                        "processed": True
                    }
                    f_out.write(json.dumps(processed_data, ensure_ascii=False) + "\"
                        "n")

    def _simulate_model_training(self, data_file: Path, model_dir: Path):
        """æ¨¡æ“¬æ¨¡å‹è¨“ç·´éšæ®µ"""
        model_dir.mkdir(exist_ok=True)

        # å»ºç«‹æ¨¡æ“¬çš„æ¨¡å‹æª”æ¡ˆ
        config = {
            "model_type": "cyberpuppy",
            "num_labels": {"toxicity": 3, "emotion": 3, "bullying": 3},
            "trained_on": str(data_file),
            "training_completed": True
        }

        (model_dir / "config.json").write_text(
            json.dumps(config, indent=2, ensure_ascii=False)
        )

        # æ¨¡æ“¬æ¨¡å‹æ¬Šé‡æª”æ¡ˆ
        (model_dir / "model.pth").write_bytes(b"fake model weights")

    async def _test_api_integration(
        self,
        api_server: str,
        test_data: List[Dict]
    ):
        """æ¸¬è©¦ API æ•´åˆ"""
        import httpx

        async with httpx.AsyncClient() as client:
            for item in test_data:
                payload = {"text": item["text"]}

                try:
                    response = await client.post(
                        f"{api_server}/analyze",
                        json=payload,
                        timeout=30.0
                    )

                    if response.status_code == 200:
                        result = response.json()
                        # åŸºæœ¬é©—è­‰
                        assert "toxicity" in result
                        assert "timestamp" in result
                except Exception as e:
                    # API å¯èƒ½æœªå®Œå…¨åˆå§‹åŒ–ï¼Œè¨˜éŒ„ä½†ä¸å¤±æ•—
                    print(f"API integration test warning: {e}")

    def _validate_system_integrity(self, model_dir: Path, data_file: Path):
        """é©—è­‰ç³»çµ±å®Œæ•´æ€§"""
        # é©—è­‰æ¨¡å‹æª”æ¡ˆå­˜åœ¨
        assert (model_dir / "config.json").exists()
        assert (model_dir / "model.pth").exists()

        # é©—è­‰è™•ç†éçš„è³‡æ–™
        assert data_file.exists()
        with open(data_file, "r", encoding="utf-8") as f:
            processed_data = [json.loads(line) for line in f]
            assert len(processed_data) > 0

            for item in processed_data:
                assert "text" in item
                assert "toxicity" in item
                assert "processed" in item
                assert item["processed"] is True
