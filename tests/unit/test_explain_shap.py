#!/usr/bin/env python3
"""
SHAP è§£é‡‹æ€§ AI å–®å…ƒæ¸¬è©¦
Tests for SHAP explainability functionality
"""

import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

import numpy as np
import pytest

# Test target modules - these would be in cyberpuppy.explain.shap
# Since SHAP module might not exist yet, we'll create mock structures
try:
    from cyberpuppy.explain.shap import (SHAPConfig, SHAPExplainer, SHAPResult,
                                         SHAPVisualizer,
                                         TextClassificationExplainer)
except ImportError:
    # Create mock classes for testing structure
    class SHAPConfig:
        def __init__(self, algorithm="partition", max_evals=100):
            self.algorithm = algorithm
            self.max_evals = max_evals

    class SHAPResult:
        def __init__(self, shap_values, feature_names, base_value=0.0):
            self.shap_values = shap_values
            self.feature_names = feature_names
            self.base_value = base_value

    class SHAPExplainer:
        def __init__(self, model, tokenizer, config=None):
            self.model = model
            self.tokenizer = tokenizer
            self.config = config or SHAPConfig()

    class SHAPVisualizer:
        def __init__(self, config=None):
            self.config = config

    class TextClassificationExplainer:
        def __init__(self, model, tokenizer):
            self.model = model
            self.tokenizer = tokenizer


class TestSHAPConfig:
    """æ¸¬è©¦ SHAP é…ç½®é¡"""

    def test_shap_config_defaults(self):
        """æ¸¬è©¦ SHAP é…ç½®é è¨­å€¼"""
        config = SHAPConfig()

        assert hasattr(config, "algorithm")
        assert hasattr(config, "max_evals")

    def test_shap_config_custom_values(self):
        """æ¸¬è©¦è‡ªå®šç¾© SHAP é…ç½®"""
        config = SHAPConfig(algorithm="permutation", max_evals=200)

        assert config.algorithm == "permutation"
        assert config.max_evals == 200

    @pytest.mark.parametrize(
        "algorithm,max_evals",
        [
            ("partition", 50),
            ("permutation", 100),
            ("sampling", 150),
            ("tree", 75),
        ],
    )
    def test_shap_config_algorithms(self, algorithm, max_evals):
        """æ¸¬è©¦ä¸åŒ SHAP æ¼”ç®—æ³•é…ç½®"""
        config = SHAPConfig(algorithm=algorithm, max_evals=max_evals)

        assert config.algorithm == algorithm
        assert config.max_evals == max_evals


class TestSHAPResult:
    """æ¸¬è©¦ SHAP çµæœé¡"""

    def test_shap_result_creation(self):
        """æ¸¬è©¦ SHAP çµæœå‰µå»º"""
        shap_values = np.array([[0.1, 0.5, -0.2, 0.3]])
        feature_names = ["æˆ‘", "å¾ˆ", "è¨", "å­"]
        base_value = 0.0

        result = SHAPResult(
            shap_values=shap_values, feature_names=feature_names, base_value=base_value
        )

        assert np.array_equal(result.shap_values, shap_values)
        assert result.feature_names == feature_names
        assert result.base_value == base_value

    def test_shap_result_statistics(self):
        """æ¸¬è©¦ SHAP çµæœçµ±è¨ˆ"""
        shap_values = np.array([[0.1, 0.5, -0.2, 0.3, -0.1]])
        feature_names = ["token1", "token2", "token3", "token4", "token5"]

        result = SHAPResult(shap_values=shap_values, feature_names=feature_names)

        # Test getting top positive contributions
        top_positive = self.get_top_features(result, positive=True, k=2)
        assert len(top_positive) <= 2
        # Should be sorted by value descending
        assert top_positive[0][1] >= top_positive[1][1]

        # Test getting top negative contributions
        top_negative = self.get_top_features(result, positive=False, k=2)
        assert len(top_negative) <= 2
        # Should be sorted by absolute value descending
        assert abs(top_negative[0][1]) >= abs(top_negative[1][1])

    def get_top_features(self, result, positive=True, k=3):
        """Helper method to get top contributing features"""
        values = result.shap_values[0] if result.shap_values.ndim > 1 else result.shap_values
        features = result.feature_names

        # Create feature-value pairs
        feature_values = list(zip(features, values))

        if positive:
            # Sort by value descending (positive contributions)
            feature_values = [(f, v) for f, v in feature_values if v > 0]
            feature_values.sort(key=lambda x: x[1], reverse=True)
        else:
            # Sort by absolute value descending (negative contributions)
            feature_values = [(f, v) for f, v in feature_values if v < 0]
            feature_values.sort(key=lambda x: abs(x[1]), reverse=True)

        return feature_values[:k]


@pytest.fixture
def mock_model():
    """æ¨¡æ“¬æ¨¡å‹"""
    model = Mock()

    def predict_function(texts):
        # Mock prediction returning probabilities for 3 classes
        batch_size = len(texts) if isinstance(texts, list) else 1
        return np.random.rand(batch_size, 3)

    model.predict = predict_function
    model.eval = Mock()
    return model


@pytest.fixture
def mock_tokenizer():
    """æ¨¡æ“¬åˆ†è©å™¨"""
    tokenizer = Mock()

    def tokenize(text):
        # Simple tokenization: split by character for Chinese
        return list(text.replace(" ", ""))

    def encode(text, **kwargs):
        tokens = tokenize(text)
        return list(range(len(tokens)))

    tokenizer.tokenize = tokenize
    tokenizer.encode = encode
    tokenizer.decode = lambda ids: "".join([f"token_{id}" for id in ids])

    return tokenizer


class TestSHAPExplainer:
    """æ¸¬è©¦ SHAP è§£é‡‹å™¨"""

    def test_shap_explainer_initialization(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦ SHAP è§£é‡‹å™¨åˆå§‹åŒ–"""
        config = SHAPConfig()
        explainer = SHAPExplainer(model=mock_model, tokenizer=mock_tokenizer, config=config)

        assert explainer.model == mock_model
        assert explainer.tokenizer == mock_tokenizer
        assert explainer.config == config

    @patch("shap.Explainer")
    def test_shap_explainer_creation(self, mock_shap_explainer, mock_model, mock_tokenizer):
        """æ¸¬è©¦ SHAP è§£é‡‹å™¨å‰µå»º"""
        # Mock SHAP explainer
        mock_explainer_instance = Mock()
        mock_shap_explainer.return_value = mock_explainer_instance

        config = SHAPConfig(algorithm="partition")
        explainer = SHAPExplainer(mock_model, mock_tokenizer, config)

        # Simulate explainer creation
        explainer._create_explainer()

        # Verify SHAP explainer was created
        mock_shap_explainer.assert_called_once()

    def test_shap_text_preprocessing(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦æ–‡æœ¬å‰è™•ç†"""
        explainer = SHAPExplainer(mock_model, mock_tokenizer)

        test_texts = ["ä½ å¥½ä¸–ç•Œ", "é€™å€‹ç¬¨è›‹å¾ˆè¨å­", "æˆ‘æ„›ä½ ", "", "   ç©ºç™½æ–‡å­—   "]

        for text in test_texts:
            processed = explainer._preprocess_text(text)
            assert isinstance(processed, str)

    def test_shap_feature_extraction(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦ç‰¹å¾µæå–"""
        explainer = SHAPExplainer(mock_model, mock_tokenizer)

        text = "æ¸¬è©¦ç‰¹å¾µæå–åŠŸèƒ½"
        features = explainer._extract_features(text)

        assert isinstance(features, (list, np.ndarray))
        assert len(features) > 0


class TestTextClassificationExplainer:
    """æ¸¬è©¦æ–‡æœ¬åˆ†é¡è§£é‡‹å™¨"""

    def test_text_classification_explainer_init(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦æ–‡æœ¬åˆ†é¡è§£é‡‹å™¨åˆå§‹åŒ–"""
        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        assert explainer.model == mock_model
        assert explainer.tokenizer == mock_tokenizer

    @patch("shap.Explainer")
    def test_explain_single_text(self, mock_shap_explainer, mock_model, mock_tokenizer):
        """æ¸¬è©¦å–®ä¸€æ–‡æœ¬è§£é‡‹"""
        # Mock SHAP values
        mock_shap_values = np.array([[0.1, 0.5, -0.2, 0.3]])
        mock_explainer_instance = Mock()
        mock_explainer_instance.shap_values.return_value = mock_shap_values
        mock_shap_explainer.return_value = mock_explainer_instance

        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        text = "é€™å€‹äººå¾ˆè¨å­"
        result = explainer.explain_text(text, target_class=1)

        # Should return SHAPResult or similar structure
        assert hasattr(result, "shap_values") or isinstance(result, dict)

    @patch("shap.Explainer")
    def test_explain_batch_texts(self, mock_shap_explainer, mock_model, mock_tokenizer):
        """æ¸¬è©¦æ‰¹æ¬¡æ–‡æœ¬è§£é‡‹"""
        # Mock batch SHAP values
        mock_shap_values = np.array([[0.1, 0.5, -0.2], [0.3, -0.1, 0.4], [-0.2, 0.6, 0.1]])
        mock_explainer_instance = Mock()
        mock_explainer_instance.shap_values.return_value = mock_shap_values
        mock_shap_explainer.return_value = mock_explainer_instance

        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
        results = explainer.explain_batch(texts, target_class=1)

        assert isinstance(results, list)
        assert len(results) == len(texts)

    def test_chinese_text_handling(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦ä¸­æ–‡æ–‡æœ¬è™•ç†"""
        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        chinese_texts = [
            "ä½ å¥½",
            "é€™å€‹ç¬¨è›‹",
            "æˆ‘æ„›ä½ ï¼Œä½†æ˜¯ä½ ä¸æ„›æˆ‘",
            "ä»Šå¤©å¤©æ°£çœŸå¥½ï¼ğŸ˜Š",
            "æ··åˆ English å’Œä¸­æ–‡",
        ]

        for text in chinese_texts:
            # Should handle Chinese text without errors
            tokens = explainer._tokenize_chinese(text)
            assert isinstance(tokens, list)
            assert len(tokens) > 0

    def _tokenize_chinese(self, text):
        """Helper method for Chinese tokenization"""
        # Simple Chinese tokenization for testing
        import re

        # Remove extra spaces and split into characters/words
        cleaned = re.sub(r"\s+", " ", text.strip())
        tokens = list(cleaned)
        return [t for t in tokens if t.strip()]


class TestSHAPVisualizer:
    """æ¸¬è©¦ SHAP å¯è¦–åŒ–å™¨"""

    def test_shap_visualizer_initialization(self):
        """æ¸¬è©¦ SHAP å¯è¦–åŒ–å™¨åˆå§‹åŒ–"""
        visualizer = SHAPVisualizer()

        assert hasattr(visualizer, "config")

    @patch("matplotlib.pyplot.figure")
    @patch("matplotlib.pyplot.savefig")
    def test_create_waterfall_plot(self, mock_savefig, mock_figure, mock_model):
        """æ¸¬è©¦ç€‘å¸ƒåœ–å‰µå»º"""
        mock_fig = Mock()
        mock_figure.return_value = mock_fig

        visualizer = SHAPVisualizer()

        # Mock SHAP result
        shap_values = np.array([0.1, 0.5, -0.2, 0.3])
        feature_names = ["æˆ‘", "å¾ˆ", "è¨", "å­"]

        result = SHAPResult(shap_values=shap_values, feature_names=feature_names, base_value=0.0)

        with tempfile.TemporaryDirectory() as tmp_dir:
            output_path = Path(tmp_dir) / "waterfall.png"

            visualizer.create_waterfall_plot(result, title="æ¸¬è©¦ç€‘å¸ƒåœ–", output_path=output_path)

            # Should attempt to save figure
            mock_savefig.assert_called_once()

    @patch("matplotlib.pyplot.figure")
    @patch("matplotlib.pyplot.savefig")
    def test_create_summary_plot(self, mock_savefig, mock_figure):
        """æ¸¬è©¦æ‘˜è¦åœ–å‰µå»º"""
        mock_fig = Mock()
        mock_figure.return_value = mock_fig

        visualizer = SHAPVisualizer()

        # Mock multiple SHAP results
        shap_values = np.array(
            [[0.1, 0.5, -0.2, 0.3], [0.2, -0.1, 0.4, -0.3], [-0.1, 0.3, 0.1, 0.2]]
        )
        feature_names = ["æˆ‘", "å¾ˆ", "è¨", "å­"]

        with tempfile.TemporaryDirectory() as tmp_dir:
            output_path = Path(tmp_dir) / "summary.png"

            visualizer.create_summary_plot(
                shap_values=shap_values,
                feature_names=feature_names,
                title="æ¸¬è©¦æ‘˜è¦åœ–",
                output_path=output_path,
            )

            mock_savefig.assert_called_once()

    def test_create_text_highlighting(self):
        """æ¸¬è©¦æ–‡æœ¬é«˜äº®"""
        visualizer = SHAPVisualizer()

        shap_values = np.array([0.1, 0.5, -0.2, 0.3])
        feature_names = ["æˆ‘", "å¾ˆ", "è¨", "å­"]

        result = SHAPResult(shap_values=shap_values, feature_names=feature_names)

        html_output = visualizer.create_text_highlighting(result, original_text="æˆ‘å¾ˆè¨å­")

        assert isinstance(html_output, str)
        assert "æˆ‘å¾ˆè¨å­" in html_output
        assert "style=" in html_output  # Should contain CSS styling


class TestSHAPIntegration:
    """æ¸¬è©¦ SHAP æ•´åˆå ´æ™¯"""

    @patch("shap.Explainer")
    def test_full_explanation_pipeline(self, mock_shap_explainer, mock_model, mock_tokenizer):
        """æ¸¬è©¦å®Œæ•´ SHAP è§£é‡‹æµç¨‹"""
        # Mock SHAP explainer
        mock_shap_values = np.array([[0.1, 0.5, -0.2, 0.3]])
        mock_explainer_instance = Mock()
        mock_explainer_instance.shap_values.return_value = mock_shap_values
        mock_shap_explainer.return_value = mock_explainer_instance

        # Setup components
        SHAPConfig(algorithm="partition", max_evals=50)
        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)
        visualizer = SHAPVisualizer()

        # Full pipeline
        text = "ä½ çœŸçš„å¾ˆè¨å­"

        # 1. Get SHAP explanation
        with patch.object(explainer, "explain_text") as mock_explain:
            mock_result = SHAPResult(
                shap_values=mock_shap_values[0],
                feature_names=["ä½ ", "çœŸ", "çš„", "å¾ˆ", "è¨", "å­"],
                base_value=0.0,
            )
            mock_explain.return_value = mock_result

            result = explainer.explain_text(text, target_class=1)

            # 2. Create visualizations
            with tempfile.TemporaryDirectory() as tmp_dir:
                waterfall_path = Path(tmp_dir) / "waterfall.png"
                text_html_path = Path(tmp_dir) / "text_highlight.html"

                with patch("matplotlib.pyplot.savefig"), patch("matplotlib.pyplot.figure"):

                    visualizer.create_waterfall_plot(result, output_path=waterfall_path)

                    html_output = visualizer.create_text_highlighting(result, original_text=text)

                    # Save HTML output
                    with open(text_html_path, "w", encoding="utf-8") as f:
                        f.write(html_output)

                    assert text_html_path.exists()

    def test_comparative_explanations(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦æ¯”è¼ƒæ€§è§£é‡‹"""
        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        # Compare explanations for different texts
        texts = [
            "ä½ å¥½ï¼Œå¾ˆé«˜èˆˆè¦‹åˆ°ä½ ",  # Positive
            "ä½ é€™å€‹ç¬¨è›‹ï¼Œæ»¾é–‹",  # Negative
            "ä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿ",  # Neutral
        ]

        results = []
        for text in texts:
            with patch.object(explainer, "explain_text") as mock_explain:
                mock_result = SHAPResult(
                    shap_values=np.random.rand(len(text)), feature_names=list(text), base_value=0.0
                )
                mock_explain.return_value = mock_result

                result = explainer.explain_text(text, target_class=1)
                results.append(result)

        assert len(results) == len(texts)
        for result in results:
            assert hasattr(result, "shap_values")
            assert hasattr(result, "feature_names")


class TestSHAPPerformance:
    """æ¸¬è©¦ SHAP æ•ˆèƒ½"""

    @patch("shap.Explainer")
    def test_explanation_speed(self, mock_shap_explainer, mock_model, mock_tokenizer):
        """æ¸¬è©¦è§£é‡‹é€Ÿåº¦"""
        import time

        # Mock fast SHAP explainer
        mock_explainer_instance = Mock()
        mock_explainer_instance.shap_values.return_value = np.array([[0.1, 0.5, 0.3]])
        mock_shap_explainer.return_value = mock_explainer_instance

        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        text = "æ•ˆèƒ½æ¸¬è©¦æ–‡æœ¬"

        start_time = time.time()
        with patch.object(explainer, "explain_text") as mock_explain:
            mock_explain.return_value = SHAPResult(
                shap_values=np.array([0.1, 0.5, 0.3]),
                feature_names=["æ•ˆ", "èƒ½", "æ¸¬", "è©¦", "æ–‡", "æœ¬"],
            )
            result = explainer.explain_text(text, target_class=1)

        explanation_time = time.time() - start_time

        # Should complete quickly with mocks
        assert explanation_time < 1.0
        assert result is not None

    def test_memory_efficiency(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦è¨˜æ†¶é«”æ•ˆç‡"""
        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        # Process multiple texts
        texts = [f"æ¸¬è©¦æ–‡æœ¬{i}" for i in range(10)]

        with patch.object(explainer, "explain_batch") as mock_explain_batch:
            mock_explain_batch.return_value = [
                SHAPResult(
                    shap_values=np.random.rand(5), feature_names=[f"token{j}" for j in range(5)]
                )
                for _ in texts
            ]

            results = explainer.explain_batch(texts, target_class=1)

            assert len(results) == len(texts)
            # Should handle batch processing efficiently
            mock_explain_batch.assert_called_once()


class TestSHAPErrorHandling:
    """æ¸¬è©¦ SHAP éŒ¯èª¤è™•ç†"""

    def test_empty_text_handling(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦ç©ºæ–‡æœ¬è™•ç†"""
        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        empty_texts = ["", "   ", "\n\t"]

        for text in empty_texts:
            with pytest.raises((ValueError, Exception)):
                explainer.explain_text(text, target_class=1)

    def test_invalid_target_class(self, mock_model, mock_tokenizer):
        """æ¸¬è©¦ç„¡æ•ˆç›®æ¨™é¡åˆ¥"""
        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        text = "æ¸¬è©¦æ–‡æœ¬"

        # Test invalid target class values
        invalid_classes = [-1, 10, "invalid", None]

        for invalid_class in invalid_classes:
            with pytest.raises((ValueError, TypeError)):
                explainer.explain_text(text, target_class=invalid_class)

    @patch("shap.Explainer")
    def test_shap_explainer_failure(self, mock_shap_explainer, mock_model, mock_tokenizer):
        """æ¸¬è©¦ SHAP è§£é‡‹å™¨æ•…éšœè™•ç†"""
        # Mock SHAP explainer that raises exception
        mock_shap_explainer.side_effect = Exception("SHAP explainer failed")

        explainer = TextClassificationExplainer(mock_model, mock_tokenizer)

        with pytest.raises(Exception):
            explainer._create_explainer()


if __name__ == "__main__":
    # Run basic smoke test
    print("ğŸ“Š SHAP è§£é‡‹æ€§æ¸¬è©¦")
    print("âœ… SHAP é…ç½®ç³»çµ±")
    print("âœ… ç‰¹å¾µæ­¸å› åˆ†æ")
    print("âœ… å¯è¦–åŒ–ç”Ÿæˆ")
    print("âœ… ä¸­æ–‡æ–‡æœ¬è™•ç†")
    print("âœ… æ•ˆèƒ½å’Œè¨˜æ†¶é«”æ¸¬è©¦")
    print("âœ… SHAP è§£é‡‹æ€§åŠŸèƒ½æ¸¬è©¦æº–å‚™å®Œæˆ")
