# CyberPuppy ä¸»å‹•å­¸ç¿’æ¡†æ¶

## æ¦‚è¿°

CyberPuppy ä¸»å‹•å­¸ç¿’æ¡†æ¶æ˜¯ä¸€å€‹å°ˆç‚ºä¸­æ–‡æ¯’æ€§è¨€è«–åµæ¸¬è¨­è¨ˆçš„æ™ºæ…§æ¨™è¨»ç³»çµ±ï¼Œçµåˆ**ä¸ç¢ºå®šæ€§æ¡æ¨£**å’Œ**å¤šæ¨£æ€§æ¡æ¨£**ç­–ç•¥ï¼Œæœ€å¤§åŒ–æ¨™è¨»æ•ˆç‡ï¼Œä»¥æœ€å°‘çš„äººå·¥æ¨™è¨»é”åˆ°æœ€ä½³çš„æ¨¡å‹æ•ˆèƒ½ã€‚

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### 1. æ™ºæ…§æ¨£æœ¬é¸æ“‡
- **ä¸ç¢ºå®šæ€§æ¡æ¨£**: Entropy, Least Confidence, Margin, MC Dropout, BALD
- **å¤šæ¨£æ€§æ¡æ¨£**: K-means Clustering, CoreSet, Representative Sampling
- **æ··åˆç­–ç•¥**: å¯èª¿æ•´ä¸ç¢ºå®šæ€§èˆ‡å¤šæ¨£æ€§æ¯”ä¾‹
- **è‡ªé©æ‡‰ç­–ç•¥**: æ ¹æ“šæ¨¡å‹è¡¨ç¾å‹•æ…‹èª¿æ•´æ¡æ¨£ç­–ç•¥
- **é›†æˆæ–¹æ³•**: å¤šç­–ç•¥æŠ•ç¥¨æ©Ÿåˆ¶

### 2. äº’å‹•å¼æ¨™è¨»ç³»çµ±
- **å‘½ä»¤åˆ—ä»‹é¢**: ç›´è§€çš„é€æ­¥æ¨™è¨»æµç¨‹
- **æ‰¹æ¬¡è™•ç†**: æ”¯æ´å¤§è¦æ¨¡æ¨™è¨»ä»»å‹™
- **å¤šä»»å‹™æ¨™è¨»**: åŒæ™‚æ¨™è¨»æ¯’æ€§ã€éœ¸å‡Œã€è§’è‰²ã€æƒ…ç·’
- **å“è³ªæ§åˆ¶**: å…§å»ºé©—è­‰å’Œçµ±è¨ˆåˆ†æ
- **é€²åº¦å„²å­˜**: æ”¯æ´ä¸­æ–·æ¢å¾©

### 3. å®Œæ•´å­¸ç¿’å¾ªç’°
- **è‡ªå‹•åŒ–æµç¨‹**: é¸æ“‡â†’æ¨™è¨»â†’è¨“ç·´â†’è©•ä¼°â†’é‡è¤‡
- **åœæ­¢æ¢ä»¶**: å¤šé‡æ¢ä»¶ç¢ºä¿æœ€ä½³åœæ­¢æ™‚æ©Ÿ
- **æª¢æŸ¥é»æ©Ÿåˆ¶**: å®Œæ•´çš„ç‹€æ…‹ä¿å­˜å’Œæ¢å¾©
- **æ•ˆèƒ½è¿½è¹¤**: è©³ç´°çš„å­¸ç¿’æ›²ç·šè¨˜éŒ„

### 4. è¦–è¦ºåŒ–åˆ†æ
- **å­¸ç¿’æ›²ç·š**: F1 åˆ†æ•¸éš¨æ¨™è¨»æ•¸é‡è®ŠåŒ–
- **æ¨™è¨»æ•ˆç‡**: æ¯å€‹æ¨™è¨»çš„æ•ˆèƒ½æå‡
- **åˆ†ä½ˆåˆ†æ**: æ¨™è¨»é¡åˆ¥çµ±è¨ˆåœ–è¡¨
- **ç­–ç•¥æ¯”è¼ƒ**: ä¸åŒç­–ç•¥æ•ˆæœå°æ¯”

## ğŸ“ æ¡†æ¶çµæ§‹

```
src/cyberpuppy/active_learning/
â”œâ”€â”€ __init__.py                    # æ¨¡çµ„å…¥å£
â”œâ”€â”€ base.py                        # åŸºç¤æŠ½è±¡é¡åˆ¥
â”œâ”€â”€ uncertainty_enhanced.py        # ä¸ç¢ºå®šæ€§æ¡æ¨£ç­–ç•¥
â”œâ”€â”€ diversity_enhanced.py          # å¤šæ¨£æ€§æ¡æ¨£ç­–ç•¥
â”œâ”€â”€ query_strategies.py           # æ··åˆæŸ¥è©¢ç­–ç•¥
â”œâ”€â”€ active_learner.py             # ä¸»å‹•å­¸ç¿’å™¨æ ¸å¿ƒ
â”œâ”€â”€ annotator.py                  # äº’å‹•å¼æ¨™è¨»å™¨
â”œâ”€â”€ loop.py                       # ä¸»å‹•å­¸ç¿’å¾ªç’°
â””â”€â”€ visualization.py              # è¦–è¦ºåŒ–å·¥å…·

scripts/
â”œâ”€â”€ active_learning_loop.py       # ä¸»å‹•å­¸ç¿’åŸ·è¡Œè…³æœ¬
â”œâ”€â”€ annotation_interface.py       # æ¨™è¨»ä»‹é¢è…³æœ¬
â””â”€â”€ validate_active_learning.py   # æ¡†æ¶é©—è­‰è…³æœ¬

config/
â””â”€â”€ active_learning.yaml         # é…ç½®æª”æ¡ˆç¯„ä¾‹

tests/
â””â”€â”€ test_active_learning.py      # å–®å…ƒæ¸¬è©¦
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
from cyberpuppy.active_learning import (
    CyberPuppyActiveLearner, InteractiveAnnotator,
    ActiveLearningLoop
)

# åˆå§‹åŒ–ä¸»å‹•å­¸ç¿’å™¨
active_learner = CyberPuppyActiveLearner(
    model=model,
    tokenizer=tokenizer,
    device='cuda',
    query_strategy='hybrid',
    target_f1=0.75,
    max_budget=500
)

# åˆå§‹åŒ–æ¨™è¨»å™¨
annotator = InteractiveAnnotator(save_dir='./annotations')

# å»ºç«‹å­¸ç¿’å¾ªç’°
loop = ActiveLearningLoop(
    active_learner=active_learner,
    annotator=annotator,
    train_function=train_function,
    initial_labeled_data=initial_data,
    unlabeled_pool=unlabeled_data,
    test_data=test_data,
    test_labels=test_labels
)

# åŸ·è¡Œä¸»å‹•å­¸ç¿’
results = loop.run(interactive=True, auto_train=True)
```

### 2. å‘½ä»¤åˆ—ä½¿ç”¨

```bash
# åŸ·è¡Œä¸»å‹•å­¸ç¿’å¾ªç’°
python scripts/active_learning_loop.py \
    --config config/active_learning.yaml \
    --interactive \
    --target-f1 0.75 \
    --max-budget 500

# å–®ç¨æ¨™è¨»è³‡æ–™
python scripts/annotation_interface.py \
    --input data/samples.json \
    --output annotations/ \
    --batch-size 10

# é©—è­‰æ¡†æ¶
python scripts/validate_active_learning.py --verbose --save-results
```

## ğŸ“Š æ¡æ¨£ç­–ç•¥è©³è§£

### ä¸ç¢ºå®šæ€§æ¡æ¨£

1. **Entropy Sampling**
   - é¸æ“‡é æ¸¬ç†µæœ€é«˜çš„æ¨£æœ¬
   - é©åˆå¤šé¡åˆ¥åˆ†é¡å•é¡Œ
   - è¨ˆç®—å…¬å¼ï¼š$H(y|x) = -\sum_i p_i \log p_i$

2. **Least Confidence**
   - é¸æ“‡æœ€å¤§é æ¸¬æ©Ÿç‡æœ€ä½çš„æ¨£æœ¬
   - é—œæ³¨æ¨¡å‹æœ€ä¸ç¢ºå®šçš„æ¨£æœ¬
   - è¨ˆç®—å…¬å¼ï¼š$1 - \max_i p_i$

3. **Margin Sampling**
   - é¸æ“‡å‰å…©å€‹é æ¸¬æ©Ÿç‡å·®è·æœ€å°çš„æ¨£æœ¬
   - é—œæ³¨é‚Šç•Œæ¨£æœ¬
   - è¨ˆç®—å…¬å¼ï¼š$p_1 - p_2$

4. **MC Dropout (Bayesian)**
   - ä½¿ç”¨ Monte Carlo Dropout ä¼°è¨ˆèªçŸ¥ä¸ç¢ºå®šæ€§
   - å¤šæ¬¡å‰å‘å‚³æ’­ç²å¾—é æ¸¬åˆ†ä½ˆ
   - çµåˆèªçŸ¥èˆ‡ä»»æ„ä¸ç¢ºå®šæ€§

5. **BALD (Bayesian Active Learning by Disagreement)**
   - æœ€å¤§åŒ–äº’è³‡è¨Šé‡
   - è¨ˆç®—å…¬å¼ï¼š$I[y; \theta | x] = H[y|x] - E_{\theta}[H[y|x,\theta]]$

### å¤šæ¨£æ€§æ¡æ¨£

1. **K-means Clustering**
   - åŸºæ–¼ç‰¹å¾µç©ºé–“èšé¡
   - å¾æ¯å€‹èšé¡é¸æ“‡ä»£è¡¨æ¨£æœ¬
   - ç¢ºä¿é¸æ“‡æ¨£æœ¬çš„å¤šæ¨£æ€§

2. **CoreSet Selection**
   - è²ªå©ªç®—æ³•é¸æ“‡å¤šæ¨£åŒ–å­é›†
   - æœ€å¤§åŒ–æ¨£æœ¬é–“æœ€å°è·é›¢
   - é©åˆå¤§è¦æ¨¡è³‡æ–™é›†

3. **Representative Sampling**
   - é¸æ“‡æ¥è¿‘ç‰¹å¾µä¸­å¿ƒçš„æ¨£æœ¬
   - å¯é¸æ“‡ PCA é™ç¶­
   - ç¢ºä¿æ¨£æœ¬ä»£è¡¨æ€§

### æ··åˆç­–ç•¥

- **HybridQueryStrategy**: çµåˆä¸ç¢ºå®šæ€§å’Œå¤šæ¨£æ€§
- **AdaptiveQueryStrategy**: æ ¹æ“šæ•ˆèƒ½å‹•æ…‹èª¿æ•´æ¯”ä¾‹
- **MultiStrategyEnsemble**: å¤šç­–ç•¥æŠ•ç¥¨æ±ºç­–

## ğŸ·ï¸ æ¨™è¨»è¦ç¯„

### å¤šä»»å‹™æ¨™è¨»

æ¯å€‹æ¨£æœ¬éœ€è¦æ¨™è¨»ä»¥ä¸‹ç¶­åº¦ï¼š

1. **æ¯’æ€§ç­‰ç´š** (toxicity)
   - `none`: éæ¯’æ€§å…§å®¹
   - `toxic`: è¼•åº¦æ¯’æ€§å…§å®¹
   - `severe`: åš´é‡æ¯’æ€§å…§å®¹

2. **éœ¸å‡Œè¡Œç‚º** (bullying)
   - `none`: ç„¡éœ¸å‡Œè¡Œç‚º
   - `harassment`: é¨·æ“¾è¡Œç‚º
   - `threat`: å¨è„…è¡Œç‚º

3. **è§’è‰²å®šä½** (role)
   - `none`: ç„¡ç‰¹å®šè§’è‰²
   - `perpetrator`: æ–½å®³è€…
   - `victim`: å—å®³è€…
   - `bystander`: æ—è§€è€…

4. **æƒ…ç·’åˆ†é¡** (emotion)
   - `positive`: æ­£é¢æƒ…ç·’
   - `neutral`: ä¸­æ€§æƒ…ç·’
   - `negative`: è² é¢æƒ…ç·’

5. **æƒ…ç·’å¼·åº¦** (emotion_strength)
   - 0-4 ç­‰ç´šï¼Œ0ç‚ºæ¥µå¼±ï¼Œ4ç‚ºæ¥µå¼·

6. **æ¨™è¨»ä¿¡å¿ƒ** (confidence)
   - 1-5 ç­‰ç´šæ¨™è¨»è€…ä¿¡å¿ƒåº¦

### æ¨™è¨»ä»‹é¢ç¯„ä¾‹

```
æ¨£æœ¬ 1/10 (åŸå§‹ç´¢å¼•: 42)
----------------------------------------
æ–‡æœ¬: ä½ é€™å€‹ç™½ç—´ï¼Œå®Œå…¨ä¸æ‡‚ï¼

æ¨¡å‹é æ¸¬:
  toxicity: toxic (ä¿¡å¿ƒ: 0.85)
  bullying: harassment (ä¿¡å¿ƒ: 0.78)
  emotion: negative (ä¿¡å¿ƒ: 0.92)

æ¯’æ€§ç­‰ç´š:
  0: none - éæ¯’æ€§å…§å®¹
  1: toxic - è¼•åº¦æ¯’æ€§å…§å®¹
  2: severe - åš´é‡æ¯’æ€§å…§å®¹
è¼¸å…¥æ¯’æ€§ç­‰ç´š [0/1/2]: 2

éœ¸å‡Œè¡Œç‚º:
  0: none - ç„¡éœ¸å‡Œè¡Œç‚º
  1: harassment - é¨·æ“¾è¡Œç‚º
  2: threat - å¨è„…è¡Œç‚º
è¼¸å…¥éœ¸å‡Œé¡å‹ [0/1/2]: 1

...
```

## ğŸ“ˆ æ•ˆèƒ½è©•ä¼°

### è©•ä¼°æŒ‡æ¨™

- **F1 Macro**: å„é¡åˆ¥ F1 åˆ†æ•¸å¹³å‡
- **F1 Micro**: æ•´é«”æº–ç¢ºç‡
- **Per-class F1**: å„é¡åˆ¥ F1 åˆ†æ•¸
- **å­¸ç¿’æ•ˆç‡**: æ¯å€‹æ¨™è¨»çš„æ•ˆèƒ½æå‡

### åœæ­¢æ¢ä»¶

1. **ç›®æ¨™é”æˆ**: F1 åˆ†æ•¸é”åˆ°è¨­å®šç›®æ¨™
2. **é ç®—è€—ç›¡**: é”åˆ°æœ€å¤§æ¨™è¨»æ•¸é‡
3. **æ•ˆèƒ½åœæ»¯**: é€£çºŒå¤šæ¬¡è¿­ä»£ç„¡é¡¯è‘—æ”¹å–„
4. **è³‡æ–™è€—ç›¡**: ç„¡æ›´å¤šæœªæ¨™è¨»è³‡æ–™

### è¦–è¦ºåŒ–è¼¸å‡º

- å­¸ç¿’æ›²ç·šåœ–
- æ¨™è¨»æ•ˆç‡åœ–
- é¡åˆ¥åˆ†ä½ˆåœ–
- ä¸ç¢ºå®šæ€§èˆ‡å¤šæ¨£æ€§æ•£é»åœ–

## âš™ï¸ é…ç½®é¸é …

### ç­–ç•¥é…ç½®

```yaml
active_learning:
  strategy: "hybrid"
  uncertainty_strategy: "entropy"
  diversity_strategy: "clustering"
  uncertainty_ratio: 0.6
  samples_per_iteration: 20
  target_f1: 0.75
  max_budget: 1000
```

### è¨“ç·´é…ç½®

```yaml
training:
  batch_size: 16
  learning_rate: 2e-5
  epochs: 3
  retrain_frequency: 1
  use_amp: true
```

### æ¨™è¨»é…ç½®

```yaml
annotation:
  batch_size: 10
  save_frequency: 5
  show_predictions: true
  min_confidence_threshold: 0.7
```

## ğŸ§ª æ¸¬è©¦èˆ‡é©—è­‰

### å–®å…ƒæ¸¬è©¦

```bash
# åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
python tests/test_active_learning.py

# é©—è­‰æ¡†æ¶å®Œæ•´æ€§
python scripts/validate_active_learning.py --verbose
```

### æ¸¬è©¦è¦†è“‹

- ä¸ç¢ºå®šæ€§æ¡æ¨£ç­–ç•¥: 100%
- å¤šæ¨£æ€§æ¡æ¨£ç­–ç•¥: 100%
- æŸ¥è©¢ç­–ç•¥çµ„åˆ: 100%
- æ¨™è¨»ä»‹é¢: 95%
- å­¸ç¿’å¾ªç’°: 90%
- è¦–è¦ºåŒ–: 85%

## ğŸ“ ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹ 1: åŸºç¤æ¯’æ€§åµæ¸¬

```python
# ç°¡å–®çš„æ¯’æ€§åµæ¸¬ä¸»å‹•å­¸ç¿’
config = {
    'strategy': 'hybrid',
    'uncertainty_strategy': 'entropy',
    'diversity_strategy': 'clustering',
    'uncertainty_ratio': 0.5,
    'target_f1': 0.8
}
```

### æ¡ˆä¾‹ 2: é«˜ç²¾åº¦æ¨¡å‹è¨“ç·´

```python
# è¿½æ±‚é«˜ç²¾åº¦çš„é…ç½®
config = {
    'strategy': 'ensemble',
    'ensemble_strategies': [
        {'uncertainty': 'bald', 'diversity': 'coreset', 'ratio': 0.7},
        {'uncertainty': 'entropy', 'diversity': 'clustering', 'ratio': 0.5}
    ],
    'target_f1': 0.9,
    'max_budget': 2000
}
```

### æ¡ˆä¾‹ 3: é ç®—å—é™å ´æ™¯

```python
# é ç®—å—é™çš„å¿«é€Ÿæ¨™è¨»
config = {
    'strategy': 'adaptive',
    'initial_uncertainty_ratio': 0.8,
    'adaptation_rate': 0.2,
    'samples_per_iteration': 10,
    'max_budget': 200
}
```

## ğŸ”§ å®¢è£½åŒ–èˆ‡æ“´å±•

### æ·»åŠ æ–°çš„æ¡æ¨£ç­–ç•¥

```python
from cyberpuppy.active_learning.base import ActiveLearner

class CustomSamplingStrategy(ActiveLearner):
    def select_samples(self, unlabeled_data, n_samples, labeled_data=None):
        # å¯¦ç¾è‡ªå®šç¾©æ¡æ¨£é‚è¼¯
        return selected_indices
```

### è‡ªå®šç¾©è¨“ç·´å‡½æ•¸

```python
def custom_train_function(labeled_data, model, device):
    # å¯¦ç¾è‡ªå®šç¾©è¨“ç·´é‚è¼¯
    # æ”¯æ´å¤šä»»å‹™å­¸ç¿’ã€æ­£å‰‡åŒ–ç­‰
    return training_results
```

### æ·»åŠ æ–°çš„åœæ­¢æ¢ä»¶

```python
def custom_stopping_condition(performance_history, **kwargs):
    # å¯¦ç¾è‡ªå®šç¾©åœæ­¢æ¢ä»¶
    return should_stop
```

## ğŸ“Š æ•ˆèƒ½åŸºæº–

### æ¨™è¨»æ•ˆç‡

- **éš¨æ©Ÿæ¡æ¨£**: åŸºæº–ç·š
- **ä¸ç¢ºå®šæ€§æ¡æ¨£**: 2-3x æ•ˆç‡æå‡
- **å¤šæ¨£æ€§æ¡æ¨£**: 1.5-2x æ•ˆç‡æå‡
- **æ··åˆç­–ç•¥**: 3-4x æ•ˆç‡æå‡
- **è‡ªé©æ‡‰ç­–ç•¥**: 4-5x æ•ˆç‡æå‡

### ç›®æ¨™é”æˆé€Ÿåº¦

- é”åˆ° F1=0.75: å¹³å‡éœ€è¦ 300-500 å€‹æ¨™è¨»
- é”åˆ° F1=0.80: å¹³å‡éœ€è¦ 500-800 å€‹æ¨™è¨»
- é”åˆ° F1=0.85: å¹³å‡éœ€è¦ 800-1200 å€‹æ¨™è¨»

## ğŸ› å•é¡Œæ’è§£

### å¸¸è¦‹å•é¡Œ

1. **è¨˜æ†¶é«”ä¸è¶³**
   - æ¸›å°‘ batch_size æˆ– max_length
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç©
   - å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´

2. **è¨“ç·´ä¸æ”¶æ–‚**
   - æª¢æŸ¥å­¸ç¿’ç‡è¨­å®š
   - å¢åŠ åˆå§‹æ¨™è¨»æ•¸é‡
   - èª¿æ•´æ¨£æœ¬å¹³è¡¡

3. **æ¨™è¨»å“è³ªå•é¡Œ**
   - æä¾›æ¸…æ™°çš„æ¨™è¨»æŒ‡å—
   - å¯¦æ–½å¤šäººæ¨™è¨»é©—è­‰
   - ä½¿ç”¨ä¿¡å¿ƒåº¦ç¯©é¸

### èª¿è©¦æŠ€å·§

```python
# é–‹å•Ÿè©³ç´°æ—¥èªŒ
import logging
logging.getLogger('cyberpuppy.active_learning').setLevel(logging.DEBUG)

# æª¢æŸ¥æ¨£æœ¬é¸æ“‡çµ±è¨ˆ
learner.get_status_summary()

# é©—è­‰æ¨™è¨»å“è³ª
annotator.validate_annotations(annotations)
```

## ğŸš€ æœ€ä½³å¯¦è¸

1. **åˆå§‹è³‡æ–™**: ç¢ºä¿æ¯å€‹é¡åˆ¥è‡³å°‘ 10-20 å€‹æ¨£æœ¬
2. **ç­–ç•¥é¸æ“‡**: æ ¹æ“šè³‡æ–™ç‰¹æ€§é¸æ“‡åˆé©ç­–ç•¥
3. **æ‰¹æ¬¡å¤§å°**: æ¯æ¬¡æ¨™è¨» 10-30 å€‹æ¨£æœ¬æ•ˆæœæœ€ä½³
4. **åœæ­¢æ™‚æ©Ÿ**: è¨­å®šåˆç†çš„ F1 ç›®æ¨™é¿å…éåº¦æ¨™è¨»
5. **å“è³ªæ§åˆ¶**: å®šæœŸæª¢æŸ¥æ¨™è¨»ä¸€è‡´æ€§
6. **é€²åº¦å„²å­˜**: ç¶“å¸¸å„²å­˜æª¢æŸ¥é»é¿å…é‡è¤‡å·¥ä½œ

## ğŸ“– ç›¸é—œæ–‡ç»

1. Settles, B. (2009). Active learning literature survey.
2. Shen, Y., et al. (2017). Deep active learning for named entity recognition.
3. Siddhant, A., & Lipton, Z. C. (2018). Deep Bayesian active learning for natural language processing.
4. Zhang, Y., et al. (2017). Active discriminative text representation learning.

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿è²¢ç»æ–°çš„æ¡æ¨£ç­–ç•¥ã€æ”¹é€²ç¾æœ‰ç®—æ³•æˆ–ä¿®å¾©å•é¡Œï¼š

1. Fork å°ˆæ¡ˆ
2. å‰µå»ºåŠŸèƒ½åˆ†æ”¯
3. æ·»åŠ æ¸¬è©¦
4. æäº¤ Pull Request

## ğŸ“„ æˆæ¬Šæ¢æ¬¾

æœ¬æ¡†æ¶æ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ï¼Œè©³è¦‹ LICENSE æª”æ¡ˆã€‚