# CyberPuppy è¨“ç·´ç³»çµ±ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

CyberPuppy è¨“ç·´ç³»çµ±æ˜¯ä¸€å€‹å°ˆç‚ºä¸­æ–‡éœ¸å‡Œåµæ¸¬è¨­è¨ˆçš„å®Œæ•´è¨“ç·´ç®¡ç†å¹³å°ï¼Œç‰¹åˆ¥é‡å° RTX 3050 4GB ç­‰ä½è¨˜æ†¶é«” GPU é€²è¡Œäº†å„ªåŒ–ã€‚

## ä¸»è¦åŠŸèƒ½

### ğŸš€ æ ¸å¿ƒç‰¹æ€§
- **è¨˜æ†¶é«”å„ªåŒ–**: å°ˆç‚º RTX 3050 4GB è¨­è¨ˆï¼Œæ”¯æ´è‡ªå‹•æ‰¹æ¬¡å¤§å°èª¿æ•´
- **æ··åˆç²¾åº¦è¨“ç·´**: FP16 å¤§å¹…é™ä½è¨˜æ†¶é«”ä½”ç”¨
- **å‹•æ…‹æ‰¹æ¬¡ç®¡ç†**: è‡ªå‹•è™•ç† OOM éŒ¯èª¤ä¸¦èª¿æ•´æ‰¹æ¬¡å¤§å°
- **å¯¦é©—è¿½è¹¤**: å®Œæ•´çš„è¨“ç·´æŒ‡æ¨™è¨˜éŒ„å’Œå¯¦é©—ç®¡ç†
- **æ—©åœæ©Ÿåˆ¶**: é˜²æ­¢éæ“¬åˆï¼Œç¯€çœè¨“ç·´æ™‚é–“
- **æª¢æŸ¥é»æ¢å¾©**: æ”¯æ´æ–·é»çºŒè¨“ï¼Œé¿å…æ„å¤–ä¸­æ–·æå¤±

### ğŸ“Š ç›£æ§ç³»çµ±
- **å³æ™‚æŒ‡æ¨™**: è¨“ç·´æå¤±ã€å­¸ç¿’ç‡ã€GPUè¨˜æ†¶é«”ä½¿ç”¨
- **TensorBoard**: è¦–è¦ºåŒ–è¨“ç·´éç¨‹
- **è¨˜æ†¶é«”ç›£æ§**: å³æ™‚ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
- **é€²åº¦è¿½è¹¤**: å½©è‰²é€²åº¦æ¢é¡¯ç¤ºè¨“ç·´ç‹€æ…‹

## å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒæº–å‚™

```bash
# å®‰è£ä¾è³´
pip install torch torchvision transformers
pip install scikit-learn tqdm tensorboard
pip install pyyaml psutil

# æª¢æŸ¥ GPU ç’°å¢ƒ
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

### 2. åŸºæœ¬è¨“ç·´

```bash
# ä½¿ç”¨é è¨­é…ç½®é–‹å§‹è¨“ç·´
python scripts/train_improved_model.py

# ä½¿ç”¨ RTX 3050 å„ªåŒ–é…ç½®
python scripts/train_improved_model.py --config configs/training/rtx3050_optimized.yaml

# å¿«é€Ÿé–‹ç™¼æ¸¬è©¦
python scripts/train_improved_model.py --template fast_dev --experiment-name my_test
```

### 3. è‡ªå®šç¾©åƒæ•¸

```bash
# èª¿æ•´é—œéµåƒæ•¸
python scripts/train_improved_model.py \
    --model-name hfl/chinese-roberta-wwm-ext \
    --batch-size 6 \
    --learning-rate 3e-5 \
    --num-epochs 15 \
    --experiment-name roberta_experiment

# å•Ÿç”¨ GPU å’Œæ··åˆç²¾åº¦
python scripts/train_improved_model.py \
    --gpu \
    --fp16 \
    --experiment-name gpu_fp16_test
```

## é…ç½®ç³»çµ±

### é…ç½®æ¨¡æ¿

ç³»çµ±æä¾› 4 å€‹é è¨­æ¨¡æ¿ï¼š

1. **default**: å¹³è¡¡çš„é è¨­é…ç½®
2. **fast_dev**: å¿«é€Ÿé–‹ç™¼æ¸¬è©¦ï¼ˆ3 epochsï¼‰
3. **production**: ç”Ÿç”¢ç’°å¢ƒé…ç½®ï¼ˆ20 epochsï¼‰
4. **memory_efficient**: RTX 3050 å°ˆç”¨å„ªåŒ–

### YAML é…ç½®æª”æ¡ˆ

æ¯å€‹é…ç½®æª”æ¡ˆåŒ…å« 6 å€‹ä¸»è¦éƒ¨åˆ†ï¼š

```yaml
model:
  name: "hfl/chinese-macbert-base"
  num_labels: 3
  dropout_rate: 0.1
  max_sequence_length: 512

data:
  train_path: "data/processed/train.json"
  val_path: "data/processed/val.json"
  batch_size: 8
  num_workers: 2

training:
  num_epochs: 10
  learning_rate: 2.0e-5
  fp16: true
  gradient_accumulation_steps: 4

optimization:
  gradient_checkpointing: true
  max_grad_norm: 1.0
  optimizer: "AdamW"

callbacks:
  early_stopping_patience: 3
  early_stopping_metric: "eval_f1_macro"
  save_top_k: 2

experiment:
  name: "my_experiment"
  seed: 42
  log_level: "INFO"
```

## RTX 3050 4GB å„ªåŒ–ç­–ç•¥

### è‡ªå‹•è¨˜æ†¶é«”å„ªåŒ–

ç³»çµ±æœƒè‡ªå‹•æª¢æ¸¬ GPU è¨˜æ†¶é«”ä¸¦æ‡‰ç”¨å„ªåŒ–ï¼š

```python
# è‡ªå‹•å•Ÿç”¨çš„å„ªåŒ–é …ç›®
- æ‰¹æ¬¡å¤§å°é™åˆ¶åœ¨ 8 ä»¥ä¸‹
- æ¢¯åº¦ç´¯ç©æ­¥æ•¸å¢åŠ åˆ° 2+
- å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
- ç¦ç”¨ pin_memory
- å•Ÿç”¨ FP16 æ··åˆç²¾åº¦
```

### æ‰‹å‹•å„ªåŒ–é…ç½®

é‡å° 4GB è¨˜æ†¶é«”çš„æœ€ä½³å¯¦è¸ï¼š

```yaml
data:
  batch_size: 4                    # å°æ‰¹æ¬¡
  gradient_accumulation_steps: 8   # é«˜ç´¯ç©
  num_workers: 1                   # é™ä½ CPU è² è¼‰
  pin_memory: false               # é‡‹æ”¾è¨˜æ†¶é«”

model:
  max_sequence_length: 384        # ç¸®çŸ­åºåˆ—é•·åº¦
  gradient_checkpointing: true    # å¿…é ˆå•Ÿç”¨

training:
  fp16: true                      # å¿…é ˆå•Ÿç”¨

optimization:
  memory_efficient_attention: true
```

### è¨˜æ†¶é«”ç›£æ§

è¨“ç·´éç¨‹ä¸­å¯¦æ™‚é¡¯ç¤ºè¨˜æ†¶é«”ä½¿ç”¨ï¼š

```
Step 100 | Loss: 0.4521 | LR: 1.25e-05 | GPU Mem: 3.2GB/4GB
```

ç•¶è¨˜æ†¶é«”ä½¿ç”¨è¶…é 3.5GB æ™‚æœƒè‡ªå‹•ç™¼å‡ºè­¦å‘Šã€‚

## å¯¦é©—ç®¡ç†

### å¯¦é©—ç›®éŒ„çµæ§‹

```
experiments/
â”œâ”€â”€ my_experiment_20241127_143052/
â”‚   â”œâ”€â”€ config.yaml              # å®Œæ•´é…ç½®
â”‚   â”œâ”€â”€ config.json             # JSON æ ¼å¼é…ç½®
â”‚   â”œâ”€â”€ checkpoints/            # æª¢æŸ¥é»æª”æ¡ˆ
â”‚   â”œâ”€â”€ tensorboard/            # TensorBoard æ—¥èªŒ
â”‚   â”œâ”€â”€ metrics.json           # è¨“ç·´æŒ‡æ¨™
â”‚   â”œâ”€â”€ training_summary.json  # è¨“ç·´æ‘˜è¦
â”‚   â””â”€â”€ final_model.pt         # æœ€çµ‚æ¨¡å‹
```

### æª¢æŸ¥é»ç®¡ç†

ç³»çµ±è‡ªå‹•ä¿å­˜æœ€ä½³æª¢æŸ¥é»ï¼š

```python
# æª¢æŸ¥é»åŒ…å«å…§å®¹
- æ¨¡å‹æ¬Šé‡
- å„ªåŒ–å™¨ç‹€æ…‹
- èª¿åº¦å™¨ç‹€æ…‹
- è¨“ç·´æŒ‡æ¨™
- å®Œæ•´é…ç½®
```

### æ–·é»çºŒè¨“

```bash
# å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´
python scripts/train_improved_model.py \
    --config experiments/my_exp/config.yaml \
    --resume-from-checkpoint experiments/my_exp/checkpoints/best_model.pt
```

## å¤šä»»å‹™è¨“ç·´

### æ”¯æ´çš„ä»»å‹™

1. **æ¯’æ€§åµæ¸¬**: 3 é¡åˆ¥ï¼ˆç„¡ã€æœ‰æ¯’ã€åš´é‡ï¼‰
2. **éœ¸å‡Œåµæ¸¬**: 3 é¡åˆ¥ï¼ˆç„¡ã€é¨·æ“¾ã€å¨è„…ï¼‰
3. **è§’è‰²è­˜åˆ¥**: 4 é¡åˆ¥ï¼ˆç„¡ã€æ–½æš´è€…ã€å—å®³è€…ã€æ—è§€è€…ï¼‰
4. **æƒ…ç·’åˆ†æ**: 3 é¡åˆ¥ï¼ˆæ­£é¢ã€ä¸­æ€§ã€è² é¢ï¼‰

### ä»»å‹™æ¬Šé‡é…ç½®

```yaml
model:
  use_multitask: true
  task_weights:
    toxicity: 1.0     # ä¸»è¦ä»»å‹™
    bullying: 1.0     # ä¸»è¦ä»»å‹™
    emotion: 0.5      # è¼”åŠ©ä»»å‹™
    role: 0.5         # è¼”åŠ©ä»»å‹™
```

## è³‡æ–™æ ¼å¼

### è¨“ç·´è³‡æ–™æ ¼å¼

```json
[
  {
    "text": "é€™æ˜¯è¦åˆ†æçš„æ–‡æœ¬å…§å®¹",
    "toxicity_labels": 0,     // 0: ç„¡æ¯’, 1: æœ‰æ¯’, 2: åš´é‡
    "bullying_labels": 0,     // 0: ç„¡éœ¸å‡Œ, 1: é¨·æ“¾, 2: å¨è„…
    "emotion_labels": 1,      // 0: è² é¢, 1: ä¸­æ€§, 2: æ­£é¢
    "role_labels": 0          // 0: ç„¡, 1: æ–½æš´, 2: å—å®³, 3: æ—è§€
  }
]
```

### ç°¡åŒ–æ ¼å¼ï¼ˆå–®ä»»å‹™ï¼‰

```json
[
  {
    "text": "é€™æ˜¯è¦åˆ†æçš„æ–‡æœ¬å…§å®¹",
    "labels": 1  // ä¸»è¦ä»»å‹™æ¨™ç±¤
  }
]
```

## é€²éšåŠŸèƒ½

### è‡ªå‹•æ‰¹æ¬¡å¤§å°å°‹æ‰¾

```yaml
training:
  auto_batch_size: true      # å•Ÿç”¨è‡ªå‹•å°‹æ‰¾
  batch_size: 8             # èµ·å§‹å¤§å°
```

ç³»çµ±æœƒè‡ªå‹•æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°ï¼Œæ‰¾åˆ°è¨˜æ†¶é«”å…è¨±çš„æœ€å¤§å€¼ã€‚

### å‹•æ…‹æ‰¹æ¬¡èª¿æ•´

è¨“ç·´éç¨‹ä¸­é‡åˆ° OOM æ™‚è‡ªå‹•é™ä½æ‰¹æ¬¡å¤§å°ï¼š

```
OOM detected, reducing batch size from 8 to 4
```

### å­¸ç¿’ç‡èª¿åº¦

æ”¯æ´å¤šç¨®èª¿åº¦ç­–ç•¥ï¼š

```yaml
training:
  lr_scheduler: "cosine"    # cosine, linear, polynomial
  warmup_ratio: 0.1        # 10% é ç†±
```

### æ¢¯åº¦ç´¯ç©

æ¨¡æ“¬å¤§æ‰¹æ¬¡è¨“ç·´ï¼š

```yaml
training:
  batch_size: 4
  gradient_accumulation_steps: 8
  # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 4 Ã— 8 = 32
```

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **OOM éŒ¯èª¤**
   ```bash
   # è§£æ±ºæ–¹æ¡ˆï¼šé™ä½æ‰¹æ¬¡å¤§å°æˆ–å•Ÿç”¨æ›´å¤šå„ªåŒ–
   --batch-size 2 --fp16
   ```

2. **è¨“ç·´ç·©æ…¢**
   ```bash
   # è§£æ±ºæ–¹æ¡ˆï¼šå¢åŠ  num_workersï¼ˆä½†ä¸è¦è¶…é CPU æ ¸å¿ƒæ•¸ï¼‰
   --num-workers 2
   ```

3. **æŒ‡æ¨™ä¸æ”¶æ–‚**
   ```bash
   # è§£æ±ºæ–¹æ¡ˆï¼šèª¿æ•´å­¸ç¿’ç‡æˆ–å¢åŠ é ç†±
   --learning-rate 1e-5 --warmup-ratio 0.2
   ```

### æ•ˆèƒ½èª¿å„ª

1. **è¨˜æ†¶é«”å„ªåŒ–**
   - å•Ÿç”¨ gradient_checkpointing
   - ä½¿ç”¨ FP16 æ··åˆç²¾åº¦
   - é™ä½ max_sequence_length
   - æ¸›å°‘ num_workers

2. **é€Ÿåº¦å„ªåŒ–**
   - å¢åŠ  batch_sizeï¼ˆåœ¨è¨˜æ†¶é«”å…è¨±ä¸‹ï¼‰
   - å•Ÿç”¨ pin_memoryï¼ˆå¤§è¨˜æ†¶é«”ç³»çµ±ï¼‰
   - ä½¿ç”¨å¿«é€Ÿçš„ SSD å„²å­˜

3. **ç©©å®šæ€§å„ªåŒ–**
   - é©ç•¶çš„å­¸ç¿’ç‡é ç†±
   - æ¢¯åº¦è£å‰ª
   - æ—©åœæ©Ÿåˆ¶

## ç›£æ§å’Œå¯è¦–åŒ–

### TensorBoard

```bash
# å•Ÿå‹• TensorBoard
tensorboard --logdir experiments/my_experiment/tensorboard

# åœ¨ç€è¦½å™¨ä¸­æŸ¥çœ‹
http://localhost:6006
```

### æŒ‡æ¨™è¿½è¹¤

ç³»çµ±è‡ªå‹•è¨˜éŒ„çš„æŒ‡æ¨™ï¼š

- è¨“ç·´æå¤± (train_loss)
- é©—è­‰æå¤± (eval_loss)
- F1 åˆ†æ•¸ (eval_f1_macro)
- æº–ç¢ºç‡ (eval_accuracy)
- å­¸ç¿’ç‡ (learning_rate)
- GPU è¨˜æ†¶é«”ä½¿ç”¨ (gpu_memory)

## æœ€ä½³å¯¦è¸

### å¯¦é©—è¨­è¨ˆ

1. **èµ·å§‹å¯¦é©—**: ä½¿ç”¨ `fast_dev` æ¨¡æ¿å¿«é€Ÿé©—è­‰
2. **åƒæ•¸æœç´¢**: ä½¿ç”¨ `hyperparameter_search` é…ç½®
3. **æ­£å¼è¨“ç·´**: ä½¿ç”¨ `production` é…ç½®
4. **è¨˜æ†¶é«”å—é™**: ä½¿ç”¨ `memory_efficient` é…ç½®

### è³‡æ–™æº–å‚™

1. **è³‡æ–™æ¸…ç†**: ç§»é™¤éçŸ­æˆ–éé•·çš„æ–‡æœ¬
2. **æ¨™ç±¤å¹³è¡¡**: ç¢ºä¿å„é¡åˆ¥æ¨£æœ¬æ•¸é‡å‡è¡¡
3. **äº¤å‰é©—è­‰**: ä½¿ç”¨åˆ†å±¤æŠ½æ¨£åˆ†å‰²è³‡æ–™

### è¨“ç·´ç­–ç•¥

1. **æ¼¸é€²å¼è¨“ç·´**: å¾å°æ¨¡å‹é–‹å§‹ï¼Œé€æ­¥å¢åŠ è¤‡é›œåº¦
2. **å¤šéšæ®µè¨“ç·´**: å…ˆé è¨“ç·´å†å¾®èª¿
3. **é›†æˆå­¸ç¿’**: è¨“ç·´å¤šå€‹æ¨¡å‹ä¸¦èåˆçµæœ

## æ“´å±•å’Œè‡ªè¨‚

### æ·»åŠ æ–°ä»»å‹™

1. ä¿®æ”¹æ¨¡å‹æ¶æ§‹
2. æ›´æ–°è³‡æ–™è¼‰å…¥å™¨
3. èª¿æ•´æå¤±å‡½æ•¸
4. ä¿®æ”¹è©•ä¼°æŒ‡æ¨™

### è‡ªè¨‚å›èª¿

```python
from src.cyberpuppy.training.callbacks import BaseCallback

class CustomCallback(BaseCallback):
    def on_epoch_end(self, trainer, **kwargs):
        # è‡ªè¨‚é‚è¼¯
        pass
```

### æ•´åˆå¤–éƒ¨å·¥å…·

- MLflow å¯¦é©—è¿½è¹¤
- Weights & Biases å¯è¦–åŒ–
- Optuna è¶…åƒæ•¸å„ªåŒ–

é€™å€‹è¨“ç·´ç³»çµ±ç‚º CyberPuppy æä¾›äº†ä¸€å€‹å®Œæ•´ã€é«˜æ•ˆã€æ˜“ç”¨çš„è¨“ç·´è§£æ±ºæ–¹æ¡ˆï¼Œç‰¹åˆ¥é©åˆè³‡æºå—é™çš„ç’°å¢ƒã€‚