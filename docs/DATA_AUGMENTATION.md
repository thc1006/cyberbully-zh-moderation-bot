# ä¸­æ–‡éœ¸å‡Œåµæ¸¬è³‡æ–™å¢å¼·ç³»çµ±

## æ¦‚è¿°

CyberPuppy è³‡æ–™å¢å¼·ç³»çµ±æ˜¯å°ˆç‚ºä¸­æ–‡éœ¸å‡Œåµæ¸¬ä»»å‹™è¨­è¨ˆçš„ç¶œåˆæ€§è³‡æ–™å¢å¼·è§£æ±ºæ–¹æ¡ˆã€‚ç³»çµ±è§£æ±ºäº† 100% åˆæˆæ¨™ç±¤å•é¡Œï¼Œé€šéå¤šç¨®ç­–ç•¥æå‡è³‡æ–™å¤šæ¨£æ€§å’ŒçœŸå¯¦æ€§ï¼ŒåŒæ™‚ç¢ºä¿æ¨™ç±¤ä¸€è‡´æ€§å’Œå“è³ªæ§åˆ¶ã€‚

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### å¤šç­–ç•¥å¢å¼·
- **åŒç¾©è©æ›¿æ›** - åŸºæ–¼ NTUSD æƒ…æ„Ÿè©å…¸çš„èªç¾©ä¸€è‡´æ›¿æ›
- **å›è­¯å¢å¼·** - ä¸­è‹±æ–‡äº’è­¯å¢åŠ è‡ªç„¶è®ŠåŒ–
- **ä¸Šä¸‹æ–‡æ“¾å‹•** - ä½¿ç”¨ MacBERT [MASK] é æ¸¬çš„èªå¢ƒæ„ŸçŸ¥æ›¿æ›
- **EDA æ“ä½œ** - éš¨æ©Ÿæ’å…¥ã€åˆªé™¤ã€äº¤æ›çš„è¼•é‡ç´šå¢å¼·

### å“è³ªä¿è­‰
- **æ¨™ç±¤ä¸€è‡´æ€§é©—è­‰** - ç¢ºä¿å¢å¼·å¾Œæ¨™ç±¤èˆ‡å…§å®¹åŒ¹é…
- **èªç¾©ç›¸ä¼¼åº¦æª¢æŸ¥** - ç¶­æŒèˆ‡åŸæ–‡çš„èªç¾©é—œè¯
- **å“è³ªé–€æª»æ§åˆ¶** - éæ¿¾ä½å“è³ªå¢å¼·æ¨£æœ¬
- **åˆ†ä½ˆå¹³è¡¡ç¶­è­·** - ä¿æŒå„é¡åˆ¥æ¨™ç±¤åˆ†ä½ˆ

### éˆæ´»é…ç½®
- **å¯é…ç½®å¼·åº¦** - è¼•åº¦ã€ä¸­åº¦ã€é‡åº¦å¢å¼·é è¨­
- **æ‰¹æ¬¡è™•ç†æ”¯æ´** - é«˜æ•ˆçš„å¤§è¦æ¨¡è³‡æ–™è™•ç†
- **å¤šç¨‹åºåŠ é€Ÿ** - æ”¯æ´ä¸¦è¡Œè™•ç†
- **è©³ç´°çµ±è¨ˆå ±å‘Š** - å¢å¼·éç¨‹ç›£æ§å’Œåˆ†æ

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from cyberpuppy.data_augmentation import AugmentationPipeline

# å‰µå»ºå¢å¼·ç®¡é“
pipeline = AugmentationPipeline()

# æº–å‚™è³‡æ–™
texts = ["é€™å€‹å¾ˆè¨å­", "æˆ‘å¾ˆé–‹å¿ƒ", "ä»Šå¤©å¤©æ°£ä¸éŒ¯"]
labels = [
    {'toxicity': 'toxic', 'emotion': 'neg'},
    {'toxicity': 'none', 'emotion': 'pos'},
    {'toxicity': 'none', 'emotion': 'neu'}
]

# åŸ·è¡Œå¢å¼·
augmented_texts, augmented_labels = pipeline.augment(texts, labels)

print(f"åŸå§‹æ¨£æœ¬æ•¸: {len(texts)}")
print(f"å¢å¼·å¾Œæ¨£æœ¬æ•¸: {len(augmented_texts)}")
```

### DataFrame è™•ç†

```python
import pandas as pd
from cyberpuppy.data_augmentation import AugmentationPipeline

# è¼‰å…¥è³‡æ–™
df = pd.read_csv('data/processed/cold_dataset.csv')

# å‰µå»ºç®¡é“
pipeline = AugmentationPipeline()

# å¢å¼· DataFrame
augmented_df = pipeline.augment_dataframe(
    df,
    text_column='text',
    label_columns=['toxicity', 'bullying', 'emotion']
)

# å„²å­˜çµæœ
augmented_df.to_csv('data/processed/cold_augmented.csv', index=False)
```

### ä½¿ç”¨åŸ·è¡Œè…³æœ¬

```bash
# åŸºæœ¬å¢å¼·
python scripts/augment_bullying_data.py \
    --input data/processed/cold_dataset.csv \
    --output data/processed/cold_augmented.csv \
    --intensity medium

# è‡ªè¨‚é…ç½®
python scripts/augment_bullying_data.py \
    --input data/processed/cold_dataset.csv \
    --output data/processed/cold_augmented.csv \
    --strategies synonym contextual eda \
    --augmentation-ratio 0.4 \
    --augmentations-per-text 3 \
    --save-analysis analysis_report.yaml
```

## ğŸ“‹ å¢å¼·ç­–ç•¥è©³è§£

### 1. åŒç¾©è©æ›¿æ› (SynonymAugmenter)

åŸºæ–¼ NTUSD æƒ…æ„Ÿè©å…¸çš„èªç¾©ä¸€è‡´åŒç¾©è©æ›¿æ›ã€‚

**ç‰¹é»:**
- ä¿æŒæƒ…æ„Ÿæ¥µæ€§ä¸€è‡´
- æ”¯æ´æ­£é¢ã€è² é¢ã€ä¸­æ€§è©å½™
- éœ¸å‡Œç›¸é—œè©å½™å°ˆé–€è™•ç†

**ç¤ºä¾‹:**
```python
from cyberpuppy.data_augmentation import SynonymAugmenter

augmenter = SynonymAugmenter()
original = "ä½ å¾ˆç¬¨"
augmented = augmenter.augment(original, num_augmentations=3)
# å¯èƒ½çµæœ: ["ä½ å¾ˆè ¢", "ä½ å¾ˆæ„šç¬¨", "ä½ å¾ˆç„¡è…¦"]
```

### 2. å›è­¯å¢å¼· (BackTranslationAugmenter)

é€šéä¸­æ–‡â†’è‹±æ–‡â†’ä¸­æ–‡çš„ç¿»è­¯éç¨‹å¼•å…¥è‡ªç„¶è®ŠåŒ–ã€‚

**ç‰¹é»:**
- ä¿æŒèªç¾©æ ¸å¿ƒä¸è®Š
- å¢åŠ è¡¨é”å¤šæ¨£æ€§
- æ¨¡æ“¬çœŸå¯¦èªè¨€è®ŠåŒ–

**ç¤ºä¾‹:**
```python
from cyberpuppy.data_augmentation import BackTranslationAugmenter

augmenter = BackTranslationAugmenter()
original = "æˆ‘è¨å­ä½ "
augmented = augmenter.augment(original, num_augmentations=2)
# å¯èƒ½çµæœ: ["æˆ‘ä¸å–œæ­¡ä½ ", "æˆ‘æ†æ¨ä½ "]
```

### 3. ä¸Šä¸‹æ–‡æ“¾å‹• (ContextualAugmenter)

ä½¿ç”¨ MacBERT æ¨¡å‹çš„ [MASK] é æ¸¬é€²è¡Œèªå¢ƒæ„ŸçŸ¥æ›¿æ›ã€‚

**ç‰¹é»:**
- èªå¢ƒæ„ŸçŸ¥çš„è©å½™æ›¿æ›
- åŸºæ–¼å¤§è¦æ¨¡é è¨“ç·´æ¨¡å‹
- ä¿æŒå¥æ³•çµæ§‹å®Œæ•´

**ç¤ºä¾‹:**
```python
from cyberpuppy.data_augmentation import ContextualAugmenter

augmenter = ContextualAugmenter()
original = "é€™å€‹äººå¾ˆè¨å­"
augmented = augmenter.augment(original, num_augmentations=2)
# å¯èƒ½çµæœ: ["é€™å€‹äººå¾ˆç…©äºº", "é€™å€‹äººå¾ˆå¯æƒ¡"]
```

### 4. EDA æ“ä½œ (EDAugmenter)

è¼•é‡ç´šçš„éš¨æ©Ÿæ“ä½œï¼šæ’å…¥ã€åˆªé™¤ã€äº¤æ›ã€‚

**ç‰¹é»:**
- è¨ˆç®—æˆæœ¬ä½
- æ“ä½œç°¡å–®ç›´æ¥
- é©åˆå¤§è¦æ¨¡è™•ç†

**ç¤ºä¾‹:**
```python
from cyberpuppy.data_augmentation import EDAugmenter

augmenter = EDAugmenter()
original = "æˆ‘å¾ˆè¨å­é€™å€‹"
augmented = augmenter.augment(original, num_augmentations=3)
# å¯èƒ½çµæœ: ["æˆ‘å¾ˆè¨å­é€™å€‹çš„", "æˆ‘è¨å­é€™å€‹", "é€™å€‹æˆ‘å¾ˆè¨å­"]
```

## âš™ï¸ é…ç½®é¸é …

### AugmentationConfig

æ§åˆ¶å„å€‹å¢å¼·å™¨çš„è¡Œç‚ºã€‚

```python
from cyberpuppy.data_augmentation import AugmentationConfig

config = AugmentationConfig(
    synonym_prob=0.1,        # åŒç¾©è©æ›¿æ›æ©Ÿç‡
    backtrans_prob=0.3,      # å›è­¯æ©Ÿç‡
    contextual_prob=0.15,    # ä¸Šä¸‹æ–‡é®ç½©æ©Ÿç‡
    eda_prob=0.1,           # EDA æ“ä½œæ©Ÿç‡
    max_augmentations=5,     # æœ€å¤§å¢å¼·æ•¸é‡
    preserve_entities=True,  # ä¿ç•™ç‰¹æ®Šç¬¦è™Ÿ
    quality_threshold=0.7    # å“è³ªé–€æª»
)
```

### PipelineConfig

æ§åˆ¶å¢å¼·ç®¡é“çš„æ•´é«”è¡Œç‚ºã€‚

```python
from cyberpuppy.data_augmentation import PipelineConfig

config = PipelineConfig(
    # ç­–ç•¥é¸æ“‡
    use_synonym=True,
    use_backtranslation=True,
    use_contextual=True,
    use_eda=True,

    # å¢å¼·å¼·åº¦
    augmentation_ratio=0.3,      # å¢å¼·è³‡æ–™æ¯”ä¾‹
    augmentations_per_text=2,    # æ¯æ–‡æœ¬å¢å¼·æ•¸
    max_total_augmentations=10000,

    # å“è³ªæ§åˆ¶
    quality_threshold=0.3,
    max_length_ratio=2.0,
    min_length_ratio=0.5,

    # æ¨™ç±¤å¹³è¡¡
    preserve_label_distribution=True,
    target_balance_ratio=1.0,

    # è™•ç†é¸é …
    batch_size=32,
    num_workers=4,
    use_multiprocessing=True,
    random_seed=42
)
```

### é è¨­å¼·åº¦é…ç½®

```python
from cyberpuppy.data_augmentation import create_augmentation_pipeline

# è¼•åº¦å¢å¼· - ä¿å®ˆç­–ç•¥
light_pipeline = create_augmentation_pipeline('light')
# - augmentation_ratio=0.1
# - augmentations_per_text=1
# - quality_threshold=0.5

# ä¸­åº¦å¢å¼· - å¹³è¡¡ç­–ç•¥
medium_pipeline = create_augmentation_pipeline('medium')
# - augmentation_ratio=0.3
# - augmentations_per_text=2
# - quality_threshold=0.3

# é‡åº¦å¢å¼· - ç©æ¥µç­–ç•¥
heavy_pipeline = create_augmentation_pipeline('heavy')
# - augmentation_ratio=0.5
# - augmentations_per_text=3
# - quality_threshold=0.2
```

## ğŸ” å“è³ªé©—è­‰ç³»çµ±

### æ¨™ç±¤ä¸€è‡´æ€§é©—è­‰

```python
from cyberpuppy.data_augmentation.validation import (
    LabelConsistencyValidator,
    LabelConsistencyConfig
)

# é…ç½®é©—è­‰å™¨
config = LabelConsistencyConfig(
    min_toxicity_confidence=0.7,
    min_bullying_confidence=0.7,
    min_emotion_confidence=0.6,
    max_length_change=0.5,
    min_semantic_similarity=0.3
)

validator = LabelConsistencyValidator(config)

# é©—è­‰å–®å€‹æ¨£æœ¬
result = validator.validate_single_sample(
    original_text="æˆ‘å¾ˆè¨å­ä½ ",
    augmented_text="æˆ‘å¾ˆæ†æ¨ä½ ",
    original_labels={'toxicity': 'toxic', 'emotion': 'neg'},
    augmented_labels={'toxicity': 'toxic', 'emotion': 'neg'}
)

print(f"é©—è­‰é€šé: {result.is_valid}")
print(f"ä¿¡å¿ƒåº¦: {result.confidence:.3f}")
print(f"é•è¦é …ç›®: {result.violations}")
```

### æ‰¹æ¬¡é©—è­‰å’Œå ±å‘Š

```python
# é©—è­‰æ•´å€‹è³‡æ–™é›†
results, batch_stats = validator.validate_batch(
    original_texts, augmented_texts,
    original_labels, augmented_labels
)

# ç”Ÿæˆå ±å‘Š
from cyberpuppy.data_augmentation.validation import QualityAssuranceReport

report = QualityAssuranceReport.generate_validation_report(results, batch_stats)
print(report)

# å„²å­˜å ±å‘Š
QualityAssuranceReport.save_report(report, 'validation_report.txt')
```

## ğŸ“Š æ€§èƒ½ç›£æ§

### çµ±è¨ˆè³‡è¨Šæ”¶é›†

```python
# åŸ·è¡Œå¢å¼·
pipeline = AugmentationPipeline()
augmented_texts, augmented_labels = pipeline.augment(texts, labels)

# ç²å–çµ±è¨ˆè³‡è¨Š
stats = pipeline.get_statistics()

print(f"è™•ç†æ¨£æœ¬æ•¸: {stats['total_processed']}")
print(f"å¢å¼·æ¨£æœ¬æ•¸: {stats['total_augmented']}")
print(f"å“è³ªéæ¿¾æ•¸: {stats['quality_filtered']}")
print(f"å¢å¼·æˆåŠŸç‡: {stats['augmentation_ratio']:.2%}")
print(f"å“è³ªé€šéç‡: {stats['quality_pass_rate']:.2%}")

# ç­–ç•¥ä½¿ç”¨åˆ†ä½ˆ
for strategy, percentage in stats['strategy_percentages'].items():
    print(f"{strategy}: {percentage:.1f}%")
```

### è©³ç´°åˆ†æç¯„ä¾‹

```python
import matplotlib.pyplot as plt
import seaborn as sns

# åˆ†ææ–‡æœ¬é•·åº¦åˆ†ä½ˆ
original_lengths = [len(text) for text in original_texts]
augmented_lengths = [len(text) for text in augmented_texts]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(original_lengths, bins=30, alpha=0.7, label='åŸå§‹')
plt.hist(augmented_lengths, bins=30, alpha=0.7, label='å¢å¼·')
plt.xlabel('æ–‡æœ¬é•·åº¦')
plt.ylabel('é »ç‡')
plt.legend()
plt.title('æ–‡æœ¬é•·åº¦åˆ†ä½ˆ')

plt.subplot(1, 2, 2)
strategy_counts = list(stats['strategy_usage'].values())
strategy_names = list(stats['strategy_usage'].keys())
plt.pie(strategy_counts, labels=strategy_names, autopct='%1.1f%%')
plt.title('ç­–ç•¥ä½¿ç”¨åˆ†ä½ˆ')

plt.tight_layout()
plt.show()
```

## ğŸ—ï¸ é€²éšä½¿ç”¨æ¡ˆä¾‹

### è‡ªè¨‚å¢å¼·ç­–ç•¥

```python
from cyberpuppy.data_augmentation.augmenters import BaseAugmenter

class CustomAugmenter(BaseAugmenter):
    def augment(self, text: str, **kwargs) -> List[str]:
        # è‡ªè¨‚å¢å¼·é‚è¼¯
        augmented = []
        # ... å¯¦ä½œå¢å¼·é‚è¼¯
        return augmented

# æ•´åˆåˆ°ç®¡é“
pipeline = AugmentationPipeline()
pipeline.augmenters['custom'] = CustomAugmenter()
```

### ç‰¹å®šé ˜åŸŸé©é…

```python
# ç‚ºç‰¹å®šéœ¸å‡Œé¡å‹å®¢è£½åŒ–
cyberbullying_config = PipelineConfig(
    # é‡å°éœ¸å‡Œåµæ¸¬å„ªåŒ–ç­–ç•¥æ¬Šé‡
    strategy_weights={
        'synonym': 0.4,      # åŠ é‡åŒç¾©è©æ›¿æ›
        'backtranslation': 0.1,  # æ¸›å°‘å›è­¯
        'contextual': 0.4,   # åŠ é‡èªå¢ƒå¢å¼·
        'eda': 0.1          # æ¸›å°‘éš¨æ©Ÿæ“ä½œ
    },
    preserve_label_distribution=True,
    target_balance_ratio=1.5  # å¢å¼·å°‘æ•¸é¡åˆ¥
)

pipeline = AugmentationPipeline(cyberbullying_config)
```

### å¤šè³‡æ–™é›†æ•´åˆ

```python
import pandas as pd

# è™•ç†å¤šå€‹è³‡æ–™é›†
datasets = ['cold', 'sccd', 'chnci']
all_augmented = []

for dataset_name in datasets:
    df = pd.read_csv(f'data/processed/{dataset_name}_dataset.csv')

    # é‡å°ä¸åŒè³‡æ–™é›†èª¿æ•´é…ç½®
    if dataset_name == 'cold':
        intensity = 'heavy'  # COLD è³‡æ–™è¼ƒå°‘ï¼Œé‡åº¦å¢å¼·
    else:
        intensity = 'medium'

    pipeline = create_augmentation_pipeline(intensity)
    augmented_df = pipeline.augment_dataframe(
        df, 'text', ['toxicity', 'bullying', 'emotion']
    )

    augmented_df['source_dataset'] = dataset_name
    all_augmented.append(augmented_df)

# åˆä½µæ‰€æœ‰å¢å¼·è³‡æ–™
final_df = pd.concat(all_augmented, ignore_index=True)
final_df.to_csv('data/processed/all_augmented.csv', index=False)
```

## ğŸ§ª æ¸¬è©¦å’Œé©—è­‰

### åŸ·è¡Œæ¸¬è©¦å¥—ä»¶

```bash
# åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
python -m pytest tests/test_augmentation.py -v

# åŸ·è¡Œç‰¹å®šæ¸¬è©¦
python -m pytest tests/test_augmentation.py::TestSynonymAugmenter -v

# æ¸¬è©¦è¦†è“‹ç‡
python -m pytest tests/test_augmentation.py --cov=cyberpuppy.data_augmentation
```

### æ‰‹å‹•é©—è­‰ç¯„ä¾‹

```python
# æ¸¬è©¦å€‹åˆ¥å¢å¼·å™¨
from cyberpuppy.data_augmentation import SynonymAugmenter

augmenter = SynonymAugmenter()
test_cases = [
    "ä½ å¾ˆç¬¨",
    "æˆ‘è¨å­é€™å€‹",
    "ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½",
    "é€™å€‹äººå¾ˆç…©"
]

for text in test_cases:
    augmented = augmenter.augment(text, num_augmentations=3)
    print(f"åŸæ–‡: {text}")
    for i, aug in enumerate(augmented, 1):
        print(f"  å¢å¼·{i}: {aug}")
    print()
```

## ğŸš€ æœ€ä½³å¯¦å‹™

### 1. åˆ†éšæ®µå¢å¼·

```python
# ç¬¬ä¸€éšæ®µï¼šè¼•åº¦å¢å¼·ï¼Œæª¢æŸ¥å“è³ª
light_pipeline = create_augmentation_pipeline('light')
sample_augmented = light_pipeline.augment(sample_texts[:100], sample_labels[:100])

# é©—è­‰å“è³ª
is_valid, report = validate_augmented_dataset(
    sample_texts[:100], sample_augmented[0],
    sample_labels[:100], sample_augmented[1]
)

if is_valid:
    # ç¬¬äºŒéšæ®µï¼šå®Œæ•´å¢å¼·
    full_pipeline = create_augmentation_pipeline('medium')
    final_augmented = full_pipeline.augment(all_texts, all_labels)
```

### 2. è¨˜æ†¶é«”å„ªåŒ–

```python
# å¤§è³‡æ–™é›†åˆ†æ‰¹è™•ç†
def process_large_dataset(df, batch_size=1000):
    results = []

    for i in range(0, len(df), batch_size):
        batch_df = df.iloc[i:i+batch_size]

        pipeline = AugmentationPipeline()
        augmented_batch = pipeline.augment_dataframe(
            batch_df, 'text', ['toxicity', 'bullying', 'emotion']
        )

        results.append(augmented_batch)

        # æ¸…ç†è¨˜æ†¶é«”
        del pipeline

    return pd.concat(results, ignore_index=True)
```

### 3. å“è³ªç›£æ§

```python
# è¨­å®šå“è³ªç›£æ§é–¾å€¼
def monitor_augmentation_quality(pipeline, texts, labels):
    augmented_texts, augmented_labels = pipeline.augment(texts, labels)
    stats = pipeline.get_statistics()

    # æª¢æŸ¥é—œéµæŒ‡æ¨™
    if stats['quality_pass_rate'] < 0.8:
        logger.warning("å“è³ªé€šéç‡éä½ï¼Œå»ºè­°èª¿æ•´åƒæ•¸")

    if stats['augmentation_ratio'] < 0.2:
        logger.warning("å¢å¼·æ¯”ä¾‹éä½ï¼Œå¯èƒ½éœ€è¦æ”¾å¯¬é™åˆ¶")

    return augmented_texts, augmented_labels
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **è¨˜æ†¶é«”ä¸è¶³**
   ```python
   # æ¸›å°‘æ‰¹æ¬¡å¤§å°å’Œå·¥ä½œç¨‹åºæ•¸
   config = PipelineConfig(
       batch_size=16,  # å¾ 32 æ¸›å°‘åˆ° 16
       num_workers=2,  # å¾ 4 æ¸›å°‘åˆ° 2
       use_multiprocessing=False  # é—œé–‰å¤šç¨‹åº
   )
   ```

2. **æ¨¡å‹è¼‰å…¥å¤±æ•—**
   ```python
   # ä½¿ç”¨è¼ƒè¼•é‡çš„ç­–ç•¥
   pipeline = create_augmentation_pipeline(
       'medium',
       strategies=['synonym', 'eda']  # é¿å…é‡æ¨¡å‹
   )
   ```

3. **å“è³ªé©—è­‰éåš´**
   ```python
   # èª¿æ•´é©—è­‰é–¾å€¼
   validation_config = LabelConsistencyConfig(
       min_toxicity_confidence=0.5,  # é™ä½ä¿¡å¿ƒé–€æª»
       min_semantic_similarity=0.2   # é™ä½ç›¸ä¼¼åº¦è¦æ±‚
   )
   ```

### é™¤éŒ¯æ¨¡å¼

```python
import logging

# å•Ÿç”¨è©³ç´°æ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)

# ä½¿ç”¨å°æ¨£æœ¬æ¸¬è©¦
test_texts = texts[:10]
test_labels = labels[:10]

pipeline = AugmentationPipeline()
augmented = pipeline.augment(test_texts, test_labels, verbose=True)
```

## ğŸ“ åƒè€ƒè³‡æ–™

- [NTUSD æƒ…æ„Ÿè©å…¸](http://nlp.csie.ntu.edu.tw/resource/sentiment.html)
- [MacBERT é è¨“ç·´æ¨¡å‹](https://github.com/ymcui/MacBERT)
- [Easy Data Augmentation è«–æ–‡](https://arxiv.org/abs/1901.11196)
- [éœ¸å‡Œåµæ¸¬è³‡æ–™é›† COLD](https://github.com/hate-alert/COLD)

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿æäº¤ issue å’Œ pull request ä¾†æ”¹å–„ç³»çµ±ï¼š

1. Fork å°ˆæ¡ˆ
2. å‰µå»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/new-augmentation`)
3. æäº¤æ›´æ”¹ (`git commit -am 'Add new augmentation strategy'`)
4. æ¨é€åˆ†æ”¯ (`git push origin feature/new-augmentation`)
5. å‰µå»º Pull Request

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ã€‚è©³è¦‹ LICENSE æ–‡ä»¶ã€‚