#!/usr/bin/env python3
"""
CyberPuppy è¨“ç·´ç›£æ§å™¨æ•´åˆç¯„ä¾‹
å±•ç¤ºå¦‚ä½•åœ¨å¯¦éš›è¨“ç·´è…³æœ¬ä¸­æ•´åˆç›£æ§å™¨
"""

import sys
import time
import torch
import torch.nn as nn
from pathlib import Path

# æ·»åŠ  src è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from cyberpuppy.training.monitor import TrainingMonitor


class SimpleModel(nn.Module):
    """ç°¡å–®çš„æ¨¡å‹ç¯„ä¾‹"""
    def __init__(self, input_size=100, hidden_size=50, num_classes=3):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)


def training_with_monitor():
    """å±•ç¤ºå¦‚ä½•åœ¨è¨“ç·´å¾ªç’°ä¸­ä½¿ç”¨ç›£æ§å™¨"""

    # è¨“ç·´é…ç½®
    config = {
        'total_epochs': 15,
        'steps_per_epoch': 50,
        'model_name': 'macbert_aggressive',
        'batch_size': 32,
        'learning_rate': 2e-5,
        'early_stopping_patience': 3
    }

    # åˆå§‹åŒ–æ¨¡å‹å’Œå„ªåŒ–å™¨ï¼ˆæ¨¡æ“¬ï¼‰
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SimpleModel().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])

    # å‰µå»ºè¨“ç·´ç›£æ§å™¨
    monitor = TrainingMonitor(
        total_epochs=config['total_epochs'],
        steps_per_epoch=config['steps_per_epoch'],
        model_name=config['model_name'],
        use_rich=True,  # è‡ªå‹•åµæ¸¬ Rich å¯ç”¨æ€§
        export_dir='./training_logs'
    )

    print(f"ğŸš€ Starting training on {device}")
    print(f"ğŸ“Š Model: {config['model_name']}")
    print(f"ğŸ”§ Config: {config}")
    print("=" * 60)

    try:
        # é–‹å§‹ç›£æ§
        monitor.start_training()

        # è¨“ç·´è®Šæ•¸
        best_f1 = 0.0
        patience_counter = 0
        global_step = 0

        for epoch in range(config['total_epochs']):
            monitor.start_epoch(epoch)
            monitor.set_early_stopping(config['early_stopping_patience'], patience_counter)

            # æ¨¡æ“¬è¨“ç·´éç¨‹
            epoch_train_losses = []

            for step in range(config['steps_per_epoch']):
                # æ¨¡æ“¬å‰å‘å‚³æ’­å’Œæå¤±è¨ˆç®—
                batch_size = config['batch_size']
                fake_input = torch.randn(batch_size, 100).to(device)
                fake_target = torch.randint(0, 3, (batch_size,)).to(device)

                optimizer.zero_grad()

                # å‰å‘å‚³æ’­
                outputs = model(fake_input)
                loss = nn.CrossEntropyLoss()(outputs, fake_target)

                # åå‘å‚³æ’­
                loss.backward()
                optimizer.step()

                train_loss = loss.item()
                epoch_train_losses.append(train_loss)

                # æ¯10æ­¥è¨ˆç®—ä¸€æ¬¡é©—è­‰æŒ‡æ¨™ï¼ˆæ¨¡æ“¬ï¼‰
                if step % 10 == 0 or step == config['steps_per_epoch'] - 1:
                    # æ¨¡æ“¬é©—è­‰æŒ‡æ¨™ï¼ˆéš¨æ™‚é–“æ”¹å–„ï¼‰
                    progress = (epoch * config['steps_per_epoch'] + step) / (config['total_epochs'] * config['steps_per_epoch'])

                    val_loss = train_loss + 0.1 + torch.rand(1).item() * 0.1
                    val_f1 = min(0.95, 0.5 + progress * 0.4 + torch.rand(1).item() * 0.1)
                    val_accuracy = val_f1 + torch.rand(1).item() * 0.05

                    # æ›´æ–°å­¸ç¿’ç‡ï¼ˆæ¨¡æ“¬ schedulerï¼‰
                    current_lr = config['learning_rate'] * (0.95 ** epoch)
                    for param_group in optimizer.param_groups:
                        param_group['lr'] = current_lr

                    # æ›´æ–°ç›£æ§å™¨
                    monitor.update_step(
                        step=step,
                        train_loss=train_loss,
                        val_loss=val_loss,
                        val_f1=val_f1,
                        val_accuracy=val_accuracy,
                        learning_rate=current_lr
                    )

                    # æª¢æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                    if val_f1 > best_f1:
                        best_f1 = val_f1
                        patience_counter = 0
                        # åœ¨å¯¦éš›æƒ…æ³ä¸‹ï¼Œé€™è£¡æœƒä¿å­˜æœ€ä½³æ¨¡å‹
                        print(f"ğŸ’¾ New best model saved! F1: {best_f1:.4f}")

                global_step += 1

                # æ¨¡æ“¬è¨“ç·´æ™‚é–“
                time.sleep(0.05)

            # çµæŸ epoch
            monitor.end_epoch(epoch, export_metrics=True)

            # Early stopping æª¢æŸ¥
            epoch_val_f1 = val_f1  # ä½¿ç”¨æœ€å¾Œçš„é©—è­‰ F1
            if epoch_val_f1 <= best_f1:
                patience_counter += 1
                if patience_counter >= config['early_stopping_patience']:
                    print(f"\nğŸ›‘ Early stopping triggered at epoch {epoch + 1}")
                    print(f"   Best F1: {best_f1:.4f}")
                    break
            else:
                patience_counter = 0

        print(f"\nâœ… Training completed!")
        print(f"ğŸ“ˆ Final best F1: {best_f1:.4f}")

    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  Training interrupted by user")
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
    finally:
        # åœæ­¢ç›£æ§
        monitor.stop_monitoring()

        # åŒ¯å‡ºæœ€çµ‚æŒ‡æ¨™
        final_metrics_path = './training_logs/final_training_metrics.json'
        monitor.export_metrics(final_metrics_path)
        print(f"ğŸ“Š Final metrics exported to: {final_metrics_path}")


def demonstrate_gpu_monitoring():
    """å±•ç¤º GPU è¨˜æ†¶é«”ç›£æ§åŠŸèƒ½"""
    from cyberpuppy.training.monitor import GPUMonitor

    print("\nğŸ–¥ï¸  GPU Memory Monitoring Demo")
    print("=" * 40)

    gpu_monitor = GPUMonitor()

    if gpu_monitor.available:
        print(f"ğŸ¯ GPU Available: {gpu_monitor.device_count} device(s)")

        for i in range(gpu_monitor.device_count):
            device_name = gpu_monitor.get_device_name(i)
            stats = gpu_monitor.get_memory_stats(i)

            print(f"\nğŸ“± Device {i}: {device_name}")
            print(f"   Memory: {stats['allocated_gb']:.2f}GB / {stats['total_gb']:.2f}GB")
            print(f"   Utilization: {stats['utilization']:.1f}%")
            print(f"   Reserved: {stats['reserved_gb']:.2f}GB")

        # åˆ†é…ä¸€äº›è¨˜æ†¶é«”ä¾†å±•ç¤ºç›£æ§
        print(f"\nğŸ§ª Allocating tensors to demonstrate monitoring...")
        tensors = []
        for i in range(5):
            tensor = torch.randn(1000, 1000).cuda()
            tensors.append(tensor)

            stats = gpu_monitor.get_memory_stats()
            print(f"   Step {i+1}: {stats['allocated_gb']:.2f}GB allocated")
            time.sleep(0.5)

        # æ¸…ç†è¨˜æ†¶é«”
        del tensors
        torch.cuda.empty_cache()

        final_stats = gpu_monitor.get_memory_stats()
        print(f"   After cleanup: {final_stats['allocated_gb']:.2f}GB allocated")
    else:
        print("âŒ No GPU available - using CPU")


if __name__ == "__main__":
    print("CyberPuppy Training Monitor Integration Example")
    print("=" * 50)

    # å±•ç¤º GPU ç›£æ§
    demonstrate_gpu_monitoring()

    print("\n" + "=" * 50)
    input("Press Enter to start training demo...")

    # åŸ·è¡Œè¨“ç·´ç¤ºç¯„
    training_with_monitor()

    print("\nğŸ‰ Demo completed!")