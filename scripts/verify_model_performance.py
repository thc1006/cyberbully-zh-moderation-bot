"""
é©—è­‰æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„å¯¦éš›æ€§èƒ½
æ¸¬è©¦çœŸå¯¦æ¨è«–çµæœä¸¦èˆ‡è²ç¨±çš„æ€§èƒ½æ¯”è¼ƒ
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Any

import numpy as np
import torch
from sklearn.metrics import classification_report, f1_score
from transformers import AutoModelForSequenceClassification, AutoTokenizer

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


def load_test_data(data_path: str) -> tuple:
    """è¼‰å…¥æ¸¬è©¦è³‡æ–™"""
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    texts = [item['text'] for item in data]

    # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡æ¨™ç±¤
    # éœ¸å‡Œæ¨¡å‹: none=0, harassment=1, threat=2
    # æ¯’æ€§æ¨¡å‹: none=0, toxic=1, severe=2
    label_map = {"none": 0, "harassment": 1, "threat": 2, "toxic": 1, "severe": 2}

    labels = []
    for item in data:
        label_value = item['label'].get('bullying', item['label'].get('toxicity', 'none'))
        labels.append(label_map.get(label_value, 0))

    return texts, labels


def verify_model(model_path: str, test_data_path: str, device: str = 'cpu') -> Dict[str, Any]:
    """é©—è­‰å–®å€‹æ¨¡å‹çš„æ€§èƒ½"""
    logger.info(f"\n{'='*80}")
    logger.info(f"é©—è­‰æ¨¡å‹: {model_path}")
    logger.info(f"{'='*80}")

    model_path = Path(model_path)

    # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    has_safetensors = (model_path / "model.safetensors").exists()
    has_bin = (model_path / "pytorch_model.bin").exists()
    has_config = (model_path / "config.json").exists()

    logger.info(f"æª”æ¡ˆæª¢æŸ¥:")
    logger.info(f"  - config.json: {'âœ…' if has_config else 'âŒ'}")
    logger.info(f"  - model.safetensors: {'âœ…' if has_safetensors else 'âŒ'}")
    logger.info(f"  - pytorch_model.bin: {'âœ…' if has_bin else 'âŒ'}")

    if not has_config or (not has_safetensors and not has_bin):
        logger.error("âŒ ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æª”æ¡ˆ")
        return {"error": "Missing model files", "model_path": str(model_path)}

    try:
        # è¼‰å…¥æ¨¡å‹å’Œ tokenizer
        logger.info("è¼‰å…¥æ¨¡å‹...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        model.to(device)
        model.eval()

        # æª¢æŸ¥æ¨¡å‹é…ç½®
        num_labels = model.config.num_labels
        logger.info(f"æ¨¡å‹é…ç½®: {num_labels} å€‹é¡åˆ¥")

        # è¼‰å…¥æ¸¬è©¦è³‡æ–™
        logger.info("è¼‰å…¥æ¸¬è©¦è³‡æ–™...")
        texts, true_labels = load_test_data(test_data_path)
        logger.info(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(texts)}")

        # æ‰¹æ¬¡æ¨è«–
        logger.info("é€²è¡Œæ¨è«–...")
        predictions = []
        batch_size = 32

        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]

                inputs = tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )
                inputs = {k: v.to(device) for k, v in inputs.items()}

                outputs = model(**inputs)
                logits = outputs.logits
                preds = torch.argmax(logits, dim=-1).cpu().numpy()
                predictions.extend(preds)

        # è¨ˆç®—æŒ‡æ¨™
        logger.info("è¨ˆç®—è©•ä¼°æŒ‡æ¨™...")

        # ç¢ºä¿æ¨™ç±¤ç¯„åœæ­£ç¢º
        true_labels = np.array(true_labels)
        predictions = np.array(predictions)

        # è™•ç†æ¨™ç±¤æ˜ å°„ï¼ˆå¦‚æœæ¨¡å‹æ˜¯3é¡ä½†æ¨™ç±¤åªæœ‰0/1ï¼‰
        if num_labels == 3:
            # å°æ–¼æ¯’æ€§æ¨¡å‹: none=0, toxic=1, severe=2
            # ä½†æ¸¬è©¦è³‡æ–™å¯èƒ½æ˜¯äºŒå…ƒçš„ï¼Œéœ€è¦è™•ç†
            pass

        # è¨ˆç®— F1 åˆ†æ•¸
        # æ˜ç¢ºæŒ‡å®š labels åƒæ•¸ä»¥è™•ç†é¡åˆ¥ä¸åŒ¹é…çš„æƒ…æ³
        all_labels = list(range(num_labels))

        f1_macro = f1_score(true_labels, predictions, labels=all_labels, average='macro', zero_division=0)
        f1_weighted = f1_score(true_labels, predictions, labels=all_labels, average='weighted', zero_division=0)

        # è©³ç´°åˆ†é¡å ±å‘Š
        target_names = [f"Class_{i}" for i in range(num_labels)]
        report = classification_report(
            true_labels,
            predictions,
            labels=all_labels,
            target_names=target_names,
            output_dict=True,
            zero_division=0
        )

        logger.info(f"\nğŸ“Š è©•ä¼°çµæœ:")
        logger.info(f"  - Macro F1: {f1_macro:.4f}")
        logger.info(f"  - Weighted F1: {f1_weighted:.4f}")
        logger.info(f"  - Accuracy: {report['accuracy']:.4f}")

        logger.info(f"\nå„é¡åˆ¥ F1 åˆ†æ•¸:")
        for i in range(num_labels):
            class_name = target_names[i]
            class_f1 = report[class_name]['f1-score']
            class_support = report[class_name]['support']
            logger.info(f"  - {class_name}: {class_f1:.4f} (n={int(class_support)})")

        return {
            "model_path": str(model_path),
            "num_labels": num_labels,
            "f1_macro": f1_macro,
            "f1_weighted": f1_weighted,
            "accuracy": report['accuracy'],
            "classification_report": report,
            "test_samples": len(texts),
            "success": True
        }

    except Exception as e:
        logger.error(f"âŒ é©—è­‰å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e), "model_path": str(model_path), "success": False}


def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸ” é–‹å§‹é©—è­‰æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„æ€§èƒ½")
    logger.info("="*80)

    # æ¸¬è©¦è³‡æ–™è·¯å¾‘
    test_data_path = "data/processed/training_dataset/test.json"

    # æª¢æŸ¥æ¸¬è©¦è³‡æ–™æ˜¯å¦å­˜åœ¨
    if not Path(test_data_path).exists():
        logger.error(f"âŒ æ¸¬è©¦è³‡æ–™ä¸å­˜åœ¨: {test_data_path}")
        return

    # è¨­å®šè£ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"ä½¿ç”¨è£ç½®: {device}")

    # è¦æ¸¬è©¦çš„æ¨¡å‹åˆ—è¡¨
    models_to_test = [
        "models/working_toxicity_model",
        "models/bullying_a100_best",  # ç”Ÿç”¢ç´šæ¨¡å‹ (F1=0.826)
    ]

    results = []

    for model_path in models_to_test:
        if Path(model_path).exists():
            result = verify_model(model_path, test_data_path, device)
            results.append(result)
        else:
            logger.warning(f"âš ï¸ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: {model_path}")

    # ç”Ÿæˆç¸½çµå ±å‘Š
    logger.info(f"\n{'='*80}")
    logger.info("ğŸ“Š é©—è­‰ç¸½çµ")
    logger.info(f"{'='*80}")

    for result in results:
        if result.get('success'):
            logger.info(f"\nâœ… {result['model_path']}")
            logger.info(f"  - Macro F1: {result['f1_macro']:.4f}")
            logger.info(f"  - Weighted F1: {result['f1_weighted']:.4f}")
            logger.info(f"  - Accuracy: {result['accuracy']:.4f}")
            logger.info(f"  - é¡åˆ¥æ•¸: {result['num_labels']}")
        else:
            logger.info(f"\nâŒ {result['model_path']}")
            logger.info(f"  - éŒ¯èª¤: {result.get('error', 'Unknown error')}")

    # ä¿å­˜çµæœ
    output_file = "models/performance_verification_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    logger.info(f"\nâœ… çµæœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()