#!/usr/bin/env python3
"""
æ™ºèƒ½æ¨¡å‹æ¨é€è…³æœ¬ - Git LFS å„ªåŒ–ç‰ˆ
è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹ä¸¦æ¨é€åˆ° GitHubï¼Œå„ªåŒ– LFS ç”¨é‡
"""

import os
import json
import shutil
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Optional


class SmartModelPusher:
    """æ™ºèƒ½æ¨¡å‹æ¨é€å™¨"""

    def __init__(
        self,
        repo_root: str = ".",
        target_f1: float = 0.75,
        max_lfs_usage_gb: float = 2.0
    ):
        self.repo_root = Path(repo_root)
        self.target_f1 = target_f1
        self.max_lfs_usage_gb = max_lfs_usage_gb

    def find_trained_models(self, experiments_dir: str = "models/experiments") -> List[Dict]:
        """å°‹æ‰¾æ‰€æœ‰è¨“ç·´å®Œæˆçš„æ¨¡å‹"""
        models = []
        exp_path = self.repo_root / experiments_dir

        if not exp_path.exists():
            print(f"âš ï¸ å¯¦é©—ç›®éŒ„ä¸å­˜åœ¨: {exp_path}")
            return models

        for model_dir in exp_path.iterdir():
            if not model_dir.is_dir():
                continue

            # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹
            best_model_path = model_dir / "best_model"
            if not best_model_path.exists():
                continue

            # è®€å–è©•ä¼°çµæœ
            eval_results = best_model_path / "eval_results.json"
            if eval_results.exists():
                with open(eval_results, 'r', encoding='utf-8') as f:
                    metrics = json.load(f)

                bullying_f1 = metrics.get('bullying_f1', 0.0)

                # è¨ˆç®—æ¨¡å‹å¤§å°
                model_size_mb = self._get_model_size(best_model_path)

                models.append({
                    'name': model_dir.name,
                    'path': str(best_model_path),
                    'f1': bullying_f1,
                    'size_mb': model_size_mb,
                    'metrics': metrics
                })

        return sorted(models, key=lambda x: x['f1'], reverse=True)

    def _get_model_size(self, model_path: Path) -> float:
        """è¨ˆç®—æ¨¡å‹å¤§å° (MB)"""
        total_size = 0
        for file in model_path.rglob('*'):
            if file.is_file():
                total_size += file.stat().st_size
        return total_size / (1024 * 1024)

    def select_models_to_push(
        self,
        models: List[Dict],
        strategy: str = "best_only"
    ) -> List[Dict]:
        """
        é¸æ“‡è¦æ¨é€çš„æ¨¡å‹

        Strategies:
        - best_only: åªæ¨é€æœ€ä½³æ¨¡å‹
        - top_3: æ¨é€å‰ä¸‰å (ç”¨æ–¼é›†æˆ)
        - all_passing: æ¨é€æ‰€æœ‰é”æ¨™æ¨¡å‹
        """
        if not models:
            return []

        if strategy == "best_only":
            return [models[0]] if models[0]['f1'] >= self.target_f1 else []

        elif strategy == "top_3":
            return models[:3]

        elif strategy == "all_passing":
            return [m for m in models if m['f1'] >= self.target_f1]

        else:
            raise ValueError(f"æœªçŸ¥ç­–ç•¥: {strategy}")

    def prepare_deploy_directory(self, models: List[Dict]) -> Path:
        """æº–å‚™éƒ¨ç½²ç›®éŒ„"""
        deploy_dir = self.repo_root / "models" / "bullying_improved"
        deploy_dir.mkdir(parents=True, exist_ok=True)

        if len(models) == 1:
            # å–®ä¸€æ¨¡å‹
            target_dir = deploy_dir / "best_single_model"
            target_dir.mkdir(exist_ok=True)

            # è¤‡è£½æ¨¡å‹æª”æ¡ˆ
            src_path = Path(models[0]['path'])
            for file in src_path.iterdir():
                if file.is_file():
                    shutil.copy2(file, target_dir / file.name)

            # ä¿å­˜éƒ¨ç½²è³‡è¨Š
            self._save_deployment_info(target_dir, models[0])

            print(f"âœ… å–®ä¸€æœ€ä½³æ¨¡å‹å·²æº–å‚™: {models[0]['name']}")
            print(f"ğŸ“Š F1 Score: {models[0]['f1']:.4f}")
            print(f"ğŸ’¾ å¤§å°: {models[0]['size_mb']:.1f} MB")

            return target_dir

        else:
            # å¤šå€‹æ¨¡å‹ï¼ˆé›†æˆç”¨ï¼‰
            ensemble_dir = deploy_dir / "ensemble_models"
            ensemble_dir.mkdir(exist_ok=True)

            for i, model in enumerate(models, 1):
                model_dir = ensemble_dir / f"model_{i}"
                model_dir.mkdir(exist_ok=True)

                src_path = Path(model['path'])
                for file in src_path.iterdir():
                    if file.is_file():
                        shutil.copy2(file, model_dir / file.name)

                self._save_deployment_info(model_dir, model)

            # ä¿å­˜é›†æˆé…ç½®
            ensemble_config = {
                'models': [
                    {'name': m['name'], 'f1': m['f1'], 'weight': 1.0 / len(models)}
                    for m in models
                ],
                'strategy': 'soft_voting',
                'total_size_mb': sum(m['size_mb'] for m in models)
            }

            with open(ensemble_dir / "ensemble_config.json", 'w', encoding='utf-8') as f:
                json.dump(ensemble_config, f, indent=2, ensure_ascii=False)

            print(f"âœ… é›†æˆæ¨¡å‹å·²æº–å‚™: {len(models)} å€‹æ¨¡å‹")
            for i, model in enumerate(models, 1):
                print(f"  {i}. {model['name']}: F1 = {model['f1']:.4f}")
            print(f"ğŸ’¾ ç¸½å¤§å°: {sum(m['size_mb'] for m in models):.1f} MB")

            return ensemble_dir

    def _save_deployment_info(self, target_dir: Path, model: Dict):
        """ä¿å­˜éƒ¨ç½²è³‡è¨Š"""
        import datetime

        deploy_info = {
            'model_name': model['name'],
            'f1_score': model['f1'],
            'size_mb': model['size_mb'],
            'metrics': model['metrics'],
            'deployed_at': datetime.datetime.now().isoformat(),
            'git_lfs_size_mb': model['size_mb']
        }

        with open(target_dir / "deployment_info.json", 'w', encoding='utf-8') as f:
            json.dump(deploy_info, f, indent=2, ensure_ascii=False)

    def configure_git_lfs(self, deploy_dir: Path):
        """é…ç½® Git LFS è¿½è¹¤"""
        print("\nğŸ”§ é…ç½® Git LFS...")

        # åˆå§‹åŒ– Git LFS
        subprocess.run(['git', 'lfs', 'install'], cwd=self.repo_root, check=True)

        # è¿½è¹¤æ¨¡å‹æª”æ¡ˆ
        patterns = [
            "models/bullying_improved/**/*.safetensors",
            "models/bullying_improved/**/*.bin",
            "models/bullying_improved/**/*.pt",
            "models/bullying_improved/**/*.pth"
        ]

        for pattern in patterns:
            subprocess.run(['git', 'lfs', 'track', pattern], cwd=self.repo_root)

        # æ·»åŠ  .gitattributes
        subprocess.run(['git', 'add', '.gitattributes'], cwd=self.repo_root)

        print("âœ… Git LFS é…ç½®å®Œæˆ")

    def check_lfs_usage(self) -> Dict[str, float]:
        """æª¢æŸ¥ Git LFS ç”¨é‡"""
        try:
            # æŸ¥è©¢ç•¶å‰ LFS ç‰©ä»¶å¤§å°
            result = subprocess.run(
                ['git', 'lfs', 'ls-files', '-s'],
                cwd=self.repo_root,
                capture_output=True,
                text=True
            )

            total_size_mb = 0.0
            for line in result.stdout.split('\n'):
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 3:
                        # ç¬¬äºŒå€‹æ¬„ä½æ˜¯å¤§å°
                        size_str = parts[1]
                        if size_str.endswith('MB'):
                            total_size_mb += float(size_str[:-2])
                        elif size_str.endswith('KB'):
                            total_size_mb += float(size_str[:-2]) / 1024
                        elif size_str.endswith('GB'):
                            total_size_mb += float(size_str[:-2]) * 1024

            return {
                'current_mb': total_size_mb,
                'current_gb': total_size_mb / 1024,
                'max_gb': 10.0,
                'remaining_gb': 10.0 - (total_size_mb / 1024)
            }

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•æŸ¥è©¢ LFS ç”¨é‡: {e}")
            return {'current_gb': 0.0, 'remaining_gb': 10.0}

    def push_to_github(
        self,
        deploy_dir: Path,
        model_info: Dict,
        dry_run: bool = False
    ) -> bool:
        """æ¨é€æ¨¡å‹åˆ° GitHub"""
        try:
            # æª¢æŸ¥ LFS ç”¨é‡
            lfs_usage = self.check_lfs_usage()
            print(f"\nğŸ“Š Git LFS ç”¨é‡:")
            print(f"  ç•¶å‰: {lfs_usage['current_gb']:.2f} GB")
            print(f"  é™åˆ¶: {lfs_usage['max_gb']:.2f} GB")
            print(f"  å‰©é¤˜: {lfs_usage['remaining_gb']:.2f} GB")

            # è¨ˆç®—æ–°å¢ç”¨é‡
            new_usage_gb = model_info['size_mb'] / 1024
            total_usage_gb = lfs_usage['current_gb'] + new_usage_gb

            print(f"  æ–°å¢: {new_usage_gb:.2f} GB")
            print(f"  ç¸½è¨ˆ: {total_usage_gb:.2f} GB")

            if total_usage_gb > lfs_usage['max_gb']:
                print(f"âŒ LFS ç”¨é‡è¶…é™ï¼")
                return False

            if dry_run:
                print("\nğŸ” Dry run æ¨¡å¼ - ä¸åŸ·è¡Œå¯¦éš›æ¨é€")
                print(f"å°‡æ¨é€: {deploy_dir}")
                print(f"å¤§å°: {model_info['size_mb']:.1f} MB")
                return True

            # æ·»åŠ æª”æ¡ˆ
            print(f"\nğŸ“¤ æ·»åŠ æª”æ¡ˆåˆ° Git...")
            subprocess.run(['git', 'add', str(deploy_dir)], cwd=self.repo_root, check=True)

            # æäº¤
            commit_msg = f"feat: Add bullying detection model (F1={model_info['f1']:.4f})"
            if model_info.get('ensemble'):
                commit_msg += f" - {len(model_info['models'])} models ensemble"

            subprocess.run(['git', 'commit', '-m', commit_msg], cwd=self.repo_root, check=True)

            # æ¨é€
            print(f"ğŸ“¤ æ¨é€åˆ° GitHub...")
            subprocess.run(['git', 'push', 'origin', 'main'], cwd=self.repo_root, check=True)

            print(f"âœ… æ¨é€æˆåŠŸï¼")
            print(f"ğŸ“Š F1 Score: {model_info['f1']:.4f}")
            print(f"ğŸ’¾ Git LFS æ–°å¢ç”¨é‡: {new_usage_gb:.2f} GB")

            return True

        except subprocess.CalledProcessError as e:
            print(f"âŒ æ¨é€å¤±æ•—: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(description="æ™ºèƒ½æ¨¡å‹æ¨é€è…³æœ¬")
    parser.add_argument(
        '--repo-root',
        type=str,
        default='.',
        help='Repository æ ¹ç›®éŒ„'
    )
    parser.add_argument(
        '--strategy',
        type=str,
        default='best_only',
        choices=['best_only', 'top_3', 'all_passing'],
        help='æ¨é€ç­–ç•¥'
    )
    parser.add_argument(
        '--target-f1',
        type=float,
        default=0.75,
        help='ç›®æ¨™ F1 åˆ†æ•¸'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='é æ¼”æ¨¡å¼ï¼Œä¸å¯¦éš›æ¨é€'
    )
    parser.add_argument(
        '--experiments-dir',
        type=str,
        default='models/experiments',
        help='å¯¦é©—æ¨¡å‹ç›®éŒ„'
    )

    args = parser.parse_args()

    print("ğŸš€ æ™ºèƒ½æ¨¡å‹æ¨é€ç³»çµ±")
    print("=" * 60)

    # åˆå§‹åŒ–æ¨é€å™¨
    pusher = SmartModelPusher(
        repo_root=args.repo_root,
        target_f1=args.target_f1
    )

    # å°‹æ‰¾è¨“ç·´å®Œæˆçš„æ¨¡å‹
    print(f"\nğŸ” æƒæå¯¦é©—æ¨¡å‹: {args.experiments_dir}")
    models = pusher.find_trained_models(args.experiments_dir)

    if not models:
        print("âŒ æ²’æœ‰æ‰¾åˆ°è¨“ç·´å®Œæˆçš„æ¨¡å‹")
        return

    print(f"\næ‰¾åˆ° {len(models)} å€‹æ¨¡å‹:")
    for i, model in enumerate(models, 1):
        status = "âœ…" if model['f1'] >= args.target_f1 else "âš ï¸"
        print(f"{i}. {status} {model['name']}: F1 = {model['f1']:.4f}, Size = {model['size_mb']:.1f} MB")

    # é¸æ“‡è¦æ¨é€çš„æ¨¡å‹
    print(f"\nğŸ“‹ æ¨é€ç­–ç•¥: {args.strategy}")
    selected_models = pusher.select_models_to_push(models, args.strategy)

    if not selected_models:
        print("âŒ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ¨¡å‹")
        print(f"   ç›®æ¨™ F1: {args.target_f1}")
        print(f"   æœ€ä½³ F1: {models[0]['f1']:.4f}")
        print(f"   å·®è·: {args.target_f1 - models[0]['f1']:.4f}")
        return

    print(f"\né¸æ“‡æ¨é€ {len(selected_models)} å€‹æ¨¡å‹:")
    for model in selected_models:
        print(f"  â€¢ {model['name']}: F1 = {model['f1']:.4f}")

    # æº–å‚™éƒ¨ç½²ç›®éŒ„
    print("\nğŸ“¦ æº–å‚™éƒ¨ç½²ç›®éŒ„...")
    deploy_dir = pusher.prepare_deploy_directory(selected_models)

    # é…ç½® Git LFS
    pusher.configure_git_lfs(deploy_dir)

    # æ¨é€åˆ° GitHub
    model_info = {
        'f1': selected_models[0]['f1'],
        'size_mb': sum(m['size_mb'] for m in selected_models),
        'ensemble': len(selected_models) > 1,
        'models': selected_models
    }

    success = pusher.push_to_github(deploy_dir, model_info, dry_run=args.dry_run)

    if success:
        print("\nğŸ‰ å®Œæˆï¼")
    else:
        print("\nâŒ æ¨é€å¤±æ•—")


if __name__ == "__main__":
    main()