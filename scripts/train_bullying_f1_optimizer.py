#!/usr/bin/env python3
"""
å°ˆé–€å„ªåŒ–éœ¸å‡Œåµæ¸¬F1åˆ†æ•¸çš„è¨“ç·´è…³æœ¬
ç›®æ¨™: éœ¸å‡Œåµæ¸¬ F1 â‰¥ 0.75, æ¯’æ€§åµæ¸¬ F1 â‰¥ 0.78

æ ¸å¿ƒåŠŸèƒ½:
1. ä½¿ç”¨æ”¹é€²çš„æ¨¡å‹æ¶æ§‹ (improved_detector.py)
2. ç„¦é»æå¤±å’Œé¡åˆ¥æ¬Šé‡å„ªåŒ–
3. RTX 3050 è¨˜æ†¶é«”å„ªåŒ–
4. å®Œæ•´çš„ç›£æ§å’Œè©•ä¼°ç³»çµ±
5. TensorBoardè¦–è¦ºåŒ–
6. è‡ªå‹•èª¿åƒå»ºè­°ç³»çµ±
"""

import argparse
import json
import logging
import os
import random
import sys
import time
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import torch
import torch.nn.functional as F
from sklearn.metrics import (
    classification_report, confusion_matrix, f1_score,
    precision_recall_fscore_support, balanced_accuracy_score
)
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from transformers import (
    AutoTokenizer, get_cosine_schedule_with_warmup,
    get_cosine_with_hard_restarts_schedule_with_warmup
)
from torch.optim import AdamW
import yaml
import traceback

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.cyberpuppy.models.improved_detector import (
    ImprovedDetector, ImprovedModelConfig
)

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

logger = logging.getLogger(__name__)


def setup_logging(level=logging.INFO, log_file=None):
    """è¨­å®šæ—¥èªŒç³»çµ±"""
    handlers = [logging.StreamHandler(sys.stdout)]

    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))

    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )


class CyberBullyDataset(torch.utils.data.Dataset):
    """éœ¸å‡Œåµæ¸¬è³‡æ–™é›†"""

    def __init__(self, data_path: str, tokenizer, max_length: int = 384,
                 augmentation: Optional[Dict] = None):
        self.data_path = Path(data_path)
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.augmentation = augmentation or {}

        self.data = self._load_data()

    def _load_data(self) -> List[Dict]:
        """è¼‰å…¥è³‡æ–™"""
        if not self.data_path.exists():
            logger.error(f"è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {self.data_path}")
            return []

        with open(self.data_path, 'r', encoding='utf-8') as f:
            if self.data_path.suffix == '.json':
                data = json.load(f)
            elif self.data_path.suffix == '.jsonl':
                data = [json.loads(line) for line in f]
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {self.data_path.suffix}")

        logger.info(f"è¼‰å…¥ {len(data)} ç­†è³‡æ–™å¾ {self.data_path}")
        return data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item.get('text', '')

        # Tokenize
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        # æº–å‚™æ¨™ç±¤ - ä½¿ç”¨çµ±ä¸€æ¨™ç±¤æ ¼å¼
        result = {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
        }

        # æ¨™ç±¤æ˜ å°„å­—å…¸
        label_maps = {
            'toxicity': {'none': 0, 'toxic': 1, 'severe': 2},
            'bullying': {'none': 0, 'harassment': 1, 'threat': 2},
            'role': {'none': 0, 'perpetrator': 1, 'victim': 2, 'bystander': 3},
            'emotion': {'pos': 0, 'neu': 1, 'neg': 2}
        }

        # å¾åµŒå¥—çµæ§‹ç²å–æ¨™ç±¤
        labels = item.get('label', {})

        for task, label_map in label_maps.items():
            if task in labels:
                label_str = labels[task]
                label_id = label_map.get(label_str, 0)  # é»˜èªç‚º 0 (none)
                result[f'{task}_labels'] = torch.tensor(label_id, dtype=torch.long)

        return result


def compute_class_weights(dataset: CyberBullyDataset) -> Dict[str, torch.Tensor]:
    """è¨ˆç®—é¡åˆ¥æ¬Šé‡"""
    from collections import Counter

    weights = {}

    for task in ['toxicity', 'bullying', 'role', 'emotion']:
        labels = []
        for item in dataset:
            if f'{task}_labels' in item:
                labels.append(item[f'{task}_labels'].item())

        if labels:
            counter = Counter(labels)
            total = len(labels)
            num_classes = max(counter.keys()) + 1

            # è¨ˆç®—å¹³è¡¡æ¬Šé‡
            class_weights = []
            for i in range(num_classes):
                count = counter.get(i, 1)  # é¿å…é™¤é›¶
                weight = total / (num_classes * count)
                class_weights.append(weight)

            weights[task] = torch.tensor(class_weights, dtype=torch.float32)
            logger.info(f"{task} é¡åˆ¥æ¬Šé‡: {weights[task].tolist()}")

    return weights


def save_model_artifacts(model, tokenizer, config: Dict, output_dir: Path):
    """ä¿å­˜æ¨¡å‹ç›¸é—œæ–‡ä»¶"""
    artifacts_dir = output_dir / "model_artifacts"
    artifacts_dir.mkdir(exist_ok=True)

    # ä¿å­˜æ¨¡å‹
    model.save_pretrained(artifacts_dir / "model")
    tokenizer.save_pretrained(artifacts_dir / "tokenizer")

    # ä¿å­˜é…ç½®
    with open(artifacts_dir / "training_config.yaml", 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)

    logger.info(f"æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜è‡³: {artifacts_dir}")


class BullyingF1Optimizer:
    """éœ¸å‡ŒF1åˆ†æ•¸å„ªåŒ–è¨“ç·´å™¨"""

    def __init__(self, config_path: str, experiment_name: str):
        self.config = self._load_config(config_path)
        self.experiment_name = experiment_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # è¨­å®šéš¨æ©Ÿç¨®å­
        self._set_seed(self.config.get("experiment", {}).get("seed", 42))

        # åˆå§‹åŒ–çµ„ä»¶
        self.tokenizer = None
        self.model = None
        self.train_loader = None
        self.val_loader = None
        self.test_loader = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = torch.cuda.amp.GradScaler() if self.config["training"]["fp16"] else None

        # è¨“ç·´ç‹€æ…‹
        self.current_epoch = 0
        self.best_bullying_f1 = 0.0
        self.best_metrics = {}
        self.early_stopping_counter = 0
        self.training_history = []

        # è¼¸å‡ºç›®éŒ„
        self.output_dir = Path(self.config["experiment"]["output_dir"]) / experiment_name
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # TensorBoard
        self.tensorboard_writer = SummaryWriter(
            log_dir=self.output_dir / "tensorboard_logs"
        )

        # è¨­å®šæ—¥èªŒæª”æ¡ˆ
        setup_logging(
            level=getattr(logging, self.config.get("experiment", {}).get("log_level", "INFO")),
            log_file=self.output_dir / "training.log"
        )

    def _load_config(self, config_path: str) -> Dict:
        """è¼‰å…¥è¨“ç·´é…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config

    def _set_seed(self, seed: int):
        """è¨­å®šéš¨æ©Ÿç¨®å­"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        if self.config.get("experiment", {}).get("deterministic", False):
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False

    def prepare_data(self):
        """æº–å‚™è¨“ç·´è³‡æ–™"""
        logger.info("æº–å‚™è¨“ç·´è³‡æ–™...")

        # åˆå§‹åŒ–tokenizer
        model_name = self.config["model"]["base_model"]
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # è¼‰å…¥è³‡æ–™é›†
        data_config = self.config["data"]

        # æª¢æŸ¥è³‡æ–™æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        train_path = Path(data_config["train_path"])
        val_path = Path(data_config["val_path"])
        test_path = Path(data_config["test_path"])

        if not train_path.exists():
            logger.error(f"è¨“ç·´è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {train_path}")
            # å˜—è©¦ä½¿ç”¨æ›¿ä»£è·¯å¾‘
            alt_train_path = Path("data/processed/cold/train.json")
            if alt_train_path.exists():
                logger.info(f"ä½¿ç”¨æ›¿ä»£è¨“ç·´è³‡æ–™: {alt_train_path}")
                train_path = alt_train_path
            else:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™æª”æ¡ˆ")

        self.train_dataset = CyberBullyDataset(
            data_path=train_path,
            tokenizer=self.tokenizer,
            max_length=self.config["model"]["max_length"],
            augmentation=data_config.get("augmentation", {})
        )

        if val_path.exists():
            self.val_dataset = CyberBullyDataset(
                data_path=val_path,
                tokenizer=self.tokenizer,
                max_length=self.config["model"]["max_length"]
            )
        else:
            # å¾è¨“ç·´é›†åˆ†å‰²é©—è­‰é›†
            logger.info("é©—è­‰é›†ä¸å­˜åœ¨ï¼Œå¾è¨“ç·´é›†åˆ†å‰²20%ä½œç‚ºé©—è­‰é›†")
            train_size = int(0.8 * len(self.train_dataset))
            val_size = len(self.train_dataset) - train_size
            self.train_dataset, self.val_dataset = torch.utils.data.random_split(
                self.train_dataset, [train_size, val_size]
            )

        if test_path.exists():
            self.test_dataset = CyberBullyDataset(
                data_path=test_path,
                tokenizer=self.tokenizer,
                max_length=self.config["model"]["max_length"]
            )
        else:
            # ä½¿ç”¨é©—è­‰é›†ä½œç‚ºæ¸¬è©¦é›†
            logger.info("æ¸¬è©¦é›†ä¸å­˜åœ¨ï¼Œä½¿ç”¨é©—è­‰é›†é€²è¡Œæœ€çµ‚è©•ä¼°")
            self.test_dataset = self.val_dataset

        # å»ºç«‹DataLoader
        batch_size = self.config["training"]["batch_size"]

        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=data_config.get("num_workers", 0),
            pin_memory=data_config.get("pin_memory", True),
            drop_last=self.config["training"].get("dataloader_drop_last", True)
        )

        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=data_config.get("num_workers", 0),
            pin_memory=data_config.get("pin_memory", True)
        )

        self.test_loader = DataLoader(
            self.test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=data_config.get("num_workers", 0),
            pin_memory=data_config.get("pin_memory", True)
        )

        logger.info(f"è¨“ç·´æ¨£æœ¬: {len(self.train_dataset)}")
        logger.info(f"é©—è­‰æ¨£æœ¬: {len(self.val_dataset)}")
        logger.info(f"æ¸¬è©¦æ¨£æœ¬: {len(self.test_dataset)}")

    def prepare_model(self):
        """æº–å‚™æ”¹é€²çš„æ¨¡å‹"""
        logger.info("åˆå§‹åŒ–æ”¹é€²çš„éœ¸å‡Œåµæ¸¬æ¨¡å‹...")

        # å»ºç«‹æ¨¡å‹é…ç½®
        model_config = ImprovedModelConfig(
            model_name=self.config["model"]["base_model"],
            hidden_size=self.config["model"]["hidden_size"],
            max_length=self.config["model"]["max_length"],
            num_toxicity_classes=self.config["model"]["toxicity_classes"],
            num_bullying_classes=self.config["model"]["bullying_classes"],
            num_role_classes=self.config["model"]["role_classes"],
            num_emotion_classes=self.config["model"]["emotion_classes"],

            # æå¤±å‡½æ•¸é…ç½®
            use_focal_loss=self.config["model"]["use_focal_loss"],
            focal_alpha=self.config["model"]["focal_loss"]["alpha"],
            focal_gamma=self.config["model"]["focal_loss"]["gamma"],
            label_smoothing=self.config["model"]["label_smoothing"]["epsilon"],
            use_class_balanced_loss=self.config["model"]["use_class_weights"]
        )

        # å»ºç«‹æ¨¡å‹
        self.model = ImprovedDetector(model_config)

        # ç§»åˆ°GPU
        self.model.to(self.device)

        # è¨˜éŒ„æ¨¡å‹è³‡è¨Š
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logger.info(f"æ¨¡å‹åƒæ•¸ç¸½é‡: {total_params:,}")
        logger.info(f"å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")

    def prepare_optimizer(self):
        """æº–å‚™å„ªåŒ–å™¨"""
        logger.info("è¨­å®šå„ªåŒ–å™¨...")

        # è¨­å®šä¸åŒçš„æ¬Šé‡è¡°æ¸›
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters()
                          if not any(nd in n for nd in no_decay)],
                "weight_decay": self.config["training"]["weight_decay"],
            },
            {
                "params": [p for n, p in self.model.named_parameters()
                          if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]

        self.optimizer = AdamW(
            optimizer_grouped_parameters,
            lr=float(self.config["training"]["learning_rate"]),
            betas=tuple(self.config["optimization"].get("betas", [0.9, 0.999])),
            eps=float(self.config["optimization"].get("eps", 1e-8))
        )

        # å­¸ç¿’ç‡æ’ç¨‹å™¨
        num_training_steps = len(self.train_loader) * self.config["training"]["num_epochs"]
        num_warmup_steps = int(num_training_steps * self.config["training"]["warmup_ratio"])

        if self.config["training"]["lr_scheduler"] == "cosine":
            self.scheduler = get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps
            )
        elif self.config["training"]["lr_scheduler"] == "cosine_with_restarts":
            self.scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps,
                num_cycles=2
            )

        logger.info(f"ç¸½è¨“ç·´æ­¥æ•¸: {num_training_steps}")
        logger.info(f"é ç†±æ­¥æ•¸: {num_warmup_steps}")

    def train_epoch(self) -> Dict[str, float]:
        """è¨“ç·´ä¸€å€‹epoch"""
        self.model.train()

        total_loss = 0.0
        task_losses = {'toxicity': 0.0, 'bullying': 0.0, 'role': 0.0, 'emotion': 0.0}
        num_batches = 0

        gradient_accumulation_steps = self.config["training"]["gradient_accumulation_steps"]

        progress_bar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch + 1}")

        for batch_idx, batch in enumerate(progress_bar):
            # ç§»å‹•åˆ°GPU
            batch = {k: v.to(self.device) for k, v in batch.items()}

            # åˆ†é›¢è¼¸å…¥å’Œæ¨™ç±¤
            input_ids = batch["input_ids"]
            attention_mask = batch["attention_mask"]
            labels = {
                "toxicity_label": batch["toxicity_labels"],
                "bullying_label": batch["bullying_labels"],
                "role_label": batch["role_labels"],
                "emotion_label": batch["emotion_labels"]
            }

            # å‰å‘å‚³æ’­
            with torch.cuda.amp.autocast(enabled=self.config["training"]["fp16"]):
                outputs = self.model(input_ids, attention_mask)
                loss_dict = self.model.compute_loss(outputs, labels)
                loss = loss_dict["total"] / gradient_accumulation_steps

            # åå‘å‚³æ’­
            if self.scaler is not None:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            total_loss += loss.item() * gradient_accumulation_steps

            # è¨˜éŒ„ä»»å‹™æå¤±
            for task in task_losses.keys():
                if task in loss_dict:
                    task_losses[task] += loss_dict[task].item()

            # æ¢¯åº¦æ›´æ–°
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                if self.scaler is not None:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config["training"]["max_grad_norm"]
                    )
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config["training"]["max_grad_norm"]
                    )
                    self.optimizer.step()

                self.scheduler.step()
                self.optimizer.zero_grad()

            num_batches += 1

            # æ›´æ–°é€²åº¦æ¢
            current_lr = self.scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',
                'lr': f'{current_lr:.2e}'
            })

            # è¨˜éŒ„åˆ°TensorBoard
            global_step = self.current_epoch * len(self.train_loader) + batch_idx
            if batch_idx % self.config["experiment"]["log_every_n_steps"] == 0:
                self.tensorboard_writer.add_scalar(
                    'Training/BatchLoss',
                    loss.item() * gradient_accumulation_steps,
                    global_step
                )
                self.tensorboard_writer.add_scalar(
                    'Training/LearningRate',
                    current_lr,
                    global_step
                )

        # è¨ˆç®—å¹³å‡æå¤±
        avg_loss = total_loss / num_batches
        avg_task_losses = {k: v / num_batches for k, v in task_losses.items()}

        results = {'train_loss': avg_loss}
        results.update({f'train_{k}_loss': v for k, v in avg_task_losses.items()})

        return results

    def evaluate(self, data_loader: DataLoader, prefix: str = "val") -> Dict[str, float]:
        """è©•ä¼°æ¨¡å‹"""
        self.model.eval()

        all_predictions = {
            "toxicity": [], "bullying": [], "role": [], "emotion": []
        }
        all_labels = {
            "toxicity": [], "bullying": [], "role": [], "emotion": []
        }
        total_loss = 0.0
        task_losses = {'toxicity': 0.0, 'bullying': 0.0, 'role': 0.0, 'emotion': 0.0}
        num_batches = 0

        with torch.no_grad():
            for batch in tqdm(data_loader, desc=f"Evaluating {prefix}"):
                batch = {k: v.to(self.device) for k, v in batch.items()}

                with torch.cuda.amp.autocast(enabled=self.config["training"]["fp16"]):
                    outputs = self.model(**batch)

                total_loss += outputs["loss"].item()

                # è¨˜éŒ„ä»»å‹™æå¤±
                for task in task_losses.keys():
                    if f"{task}_loss" in outputs:
                        task_losses[task] += outputs[f"{task}_loss"].item()

                # æ”¶é›†é æ¸¬å’Œæ¨™ç±¤
                for task in ["toxicity", "bullying", "role", "emotion"]:
                    if f"{task}_logits" in outputs and f"{task}_labels" in batch:
                        logits = outputs[f"{task}_logits"]
                        preds = torch.argmax(logits, dim=-1)
                        labels = batch[f"{task}_labels"]

                        all_predictions[task].extend(preds.cpu().numpy())
                        all_labels[task].extend(labels.cpu().numpy())

                num_batches += 1

        # è¨ˆç®—æŒ‡æ¨™
        metrics = {f"{prefix}_loss": total_loss / num_batches}

        # æ·»åŠ ä»»å‹™æå¤±
        for task, loss in task_losses.items():
            if loss > 0:
                metrics[f"{prefix}_{task}_loss"] = loss / num_batches

        # è¨ˆç®—æ¯å€‹ä»»å‹™çš„F1åˆ†æ•¸
        for task in ["toxicity", "bullying", "role", "emotion"]:
            if all_predictions[task] and all_labels[task]:
                preds = np.array(all_predictions[task])
                labels = np.array(all_labels[task])

                # F1åˆ†æ•¸
                f1_macro = f1_score(labels, preds, average='macro', zero_division=0)
                f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)

                metrics[f"{task}_f1"] = f1_macro
                metrics[f"{task}_f1_weighted"] = f1_weighted

                # ç²¾ç¢ºç‡å’Œå¬å›ç‡
                precision, recall, _, _ = precision_recall_fscore_support(
                    labels, preds, average='macro', zero_division=0
                )
                metrics[f"{task}_precision"] = precision
                metrics[f"{task}_recall"] = recall

                # å¹³è¡¡æº–ç¢ºç‡
                balanced_acc = balanced_accuracy_score(labels, preds)
                metrics[f"{task}_balanced_accuracy"] = balanced_acc

        # è¨ˆç®—ç¸½é«”æŒ‡æ¨™
        valid_f1s = []
        for task in ["toxicity", "bullying", "role", "emotion"]:
            if f"{task}_f1" in metrics and metrics[f"{task}_f1"] > 0:
                valid_f1s.append(metrics[f"{task}_f1"])

        if valid_f1s:
            metrics["overall_macro_f1"] = np.mean(valid_f1s)

        return metrics

    def save_checkpoint(self, metrics: Dict[str, float], is_best: bool = False):
        """ä¿å­˜æª¢æŸ¥é»"""
        checkpoint_dir = self.output_dir / "checkpoints"
        checkpoint_dir.mkdir(exist_ok=True)

        checkpoint = {
            "epoch": self.current_epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict(),
            "metrics": metrics,
            "best_bullying_f1": self.best_bullying_f1,
            "config": self.config,
            "training_history": self.training_history
        }

        if self.scaler is not None:
            checkpoint["scaler_state_dict"] = self.scaler.state_dict()

        # ä¿å­˜æœ€æ–°æª¢æŸ¥é»
        torch.save(checkpoint, checkpoint_dir / "last.ckpt")

        # ä¿å­˜æœ€ä½³æª¢æŸ¥é»
        if is_best:
            torch.save(checkpoint, checkpoint_dir / "best.ckpt")
            logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éœ¸å‡ŒF1: {metrics.get('bullying_f1', 0):.4f})")

    def analyze_and_suggest_improvements(self, metrics: Dict[str, float]) -> List[str]:
        """åˆ†æç•¶å‰è¡¨ç¾ä¸¦æä¾›æ”¹é€²å»ºè­°"""
        suggestions = []

        bullying_f1 = metrics.get('bullying_f1', 0)
        toxicity_f1 = metrics.get('toxicity_f1', 0)

        if bullying_f1 < 0.75:
            gap = 0.75 - bullying_f1
            suggestions.append(f"éœ¸å‡ŒF1è·é›¢ç›®æ¨™é‚„æœ‰ {gap:.3f}")

            if bullying_f1 < 0.6:
                suggestions.append("å»ºè­°å¢åŠ éœ¸å‡Œé¡åˆ¥çš„è¨“ç·´æ¨£æœ¬æˆ–èª¿æ•´é¡åˆ¥æ¬Šé‡")
                suggestions.append("è€ƒæ…®ä½¿ç”¨æ›´å¼·çš„è³‡æ–™å¢å¼·æŠ€è¡“")

            if gap > 0.1:
                suggestions.append("å»ºè­°èª¿é«˜éœ¸å‡Œä»»å‹™æ¬Šé‡ (task_weights.bullying)")
                suggestions.append("è€ƒæ…®èª¿æ•´ç„¦é»æå¤±åƒæ•¸ (focal_loss.gamma)")

        if toxicity_f1 < 0.78:
            suggestions.append(f"æ¯’æ€§F1éœ€è¦æå‡ {0.78 - toxicity_f1:.3f}")

        # æª¢æŸ¥éæ“¬åˆ
        train_loss = metrics.get('train_loss', 0)
        val_loss = metrics.get('val_loss', 0)
        if val_loss > train_loss * 1.2:
            suggestions.append("å¯èƒ½å­˜åœ¨éæ“¬åˆï¼Œå»ºè­°å¢åŠ dropoutæˆ–æ­£è¦åŒ–")

        return suggestions

    def train(self):
        """åŸ·è¡Œå®Œæ•´è¨“ç·´æµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹éœ¸å‡Œåµæ¸¬æ¨¡å‹è¨“ç·´...")

        # æº–å‚™è¨“ç·´
        self.prepare_data()
        self.prepare_model()
        self.prepare_optimizer()

        # æ—©åœé…ç½®
        early_stopping_config = self.config["optimization"]["early_stopping"]
        patience = early_stopping_config["patience"]

        # è¨“ç·´å¾ªç’°
        for epoch in range(self.config["training"]["num_epochs"]):
            self.current_epoch = epoch
            epoch_start_time = time.time()

            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ“ˆ Epoch {epoch + 1}/{self.config['training']['num_epochs']}")
            logger.info(f"{'='*60}")

            # è¨“ç·´
            train_metrics = self.train_epoch()

            # é©—è­‰
            val_metrics = self.evaluate(self.val_loader, "val")

            # åˆä½µæŒ‡æ¨™
            all_metrics = {**train_metrics, **val_metrics}
            all_metrics['epoch_time'] = time.time() - epoch_start_time

            # è¨˜éŒ„è¨“ç·´æ­·å²
            self.training_history.append(all_metrics.copy())

            # TensorBoardè¨˜éŒ„
            for key, value in all_metrics.items():
                if isinstance(value, (int, float)):
                    self.tensorboard_writer.add_scalar(f"Epoch/{key}", value, epoch)

            # è¼¸å‡ºé—œéµæŒ‡æ¨™
            logger.info(f"ğŸ“Š è¨“ç·´æå¤±: {train_metrics['train_loss']:.4f}")
            logger.info(f"ğŸ“Š é©—è­‰æå¤±: {val_metrics['val_loss']:.4f}")

            if "bullying_f1" in val_metrics:
                logger.info(f"ğŸ¯ éœ¸å‡ŒF1: {val_metrics['bullying_f1']:.4f} (ç›®æ¨™: â‰¥0.75)")
            if "toxicity_f1" in val_metrics:
                logger.info(f"ğŸ¯ æ¯’æ€§F1: {val_metrics['toxicity_f1']:.4f} (ç›®æ¨™: â‰¥0.78)")
            if "overall_macro_f1" in val_metrics:
                logger.info(f"ğŸ¯ ç¸½é«”F1: {val_metrics['overall_macro_f1']:.4f}")

            # æª¢æŸ¥æœ€ä½³æ¨¡å‹
            current_bullying_f1 = val_metrics.get("bullying_f1", 0)
            is_best = current_bullying_f1 > self.best_bullying_f1

            if is_best:
                self.best_bullying_f1 = current_bullying_f1
                self.best_metrics = val_metrics.copy()
                self.early_stopping_counter = 0
                logger.info("ğŸ† æ–°çš„æœ€ä½³éœ¸å‡ŒF1åˆ†æ•¸!")
            else:
                self.early_stopping_counter += 1

            # åˆ†æå’Œå»ºè­°
            suggestions = self.analyze_and_suggest_improvements(val_metrics)
            if suggestions:
                logger.info("ğŸ’¡ æ”¹é€²å»ºè­°:")
                for suggestion in suggestions:
                    logger.info(f"   â€¢ {suggestion}")

            # ä¿å­˜æª¢æŸ¥é»
            self.save_checkpoint(all_metrics, is_best)

            # æ—©åœæª¢æŸ¥
            if self.early_stopping_counter >= patience:
                logger.info(f"â¹ï¸  Early stopping after {patience} epochs without improvement")
                break

        # æœ€çµ‚è©•ä¼°
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ è¨“ç·´å®Œæˆ! é€²è¡Œæœ€çµ‚è©•ä¼°...")
        logger.info("="*60)

        # è¼‰å…¥æœ€ä½³æ¨¡å‹
        best_checkpoint = torch.load(self.output_dir / "checkpoints" / "best.ckpt")
        self.model.load_state_dict(best_checkpoint["model_state_dict"])

        # æ¸¬è©¦é›†è©•ä¼°
        test_metrics = self.evaluate(self.test_loader, "test")

        # æº–å‚™æœ€çµ‚çµæœ
        results = {
            "experiment_name": self.experiment_name,
            "best_epoch": best_checkpoint["epoch"],
            "training_time_epochs": self.current_epoch + 1,
            "best_val_metrics": self.best_metrics,
            "test_metrics": test_metrics,
            "target_achieved": {
                "bullying_f1_075": test_metrics.get("bullying_f1", 0) >= 0.75,
                "toxicity_f1_078": test_metrics.get("toxicity_f1", 0) >= 0.78,
                "overall_macro_f1_076": test_metrics.get("overall_macro_f1", 0) >= 0.76
            },
            "config": self.config,
            "training_history": self.training_history
        }

        # ä¿å­˜çµæœ
        with open(self.output_dir / "final_results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # è¼¸å‡ºæœ€çµ‚ç¸½çµ
        logger.info(f"\nğŸ“‹ æœ€çµ‚çµæœæ‘˜è¦:")
        logger.info(f"ğŸ¯ éœ¸å‡Œåµæ¸¬ F1: {test_metrics.get('bullying_f1', 0):.4f} (ç›®æ¨™: â‰¥0.75)")
        logger.info(f"ğŸ¯ æ¯’æ€§åµæ¸¬ F1: {test_metrics.get('toxicity_f1', 0):.4f} (ç›®æ¨™: â‰¥0.78)")
        logger.info(f"ğŸ¯ ç¸½é«”Macro F1: {test_metrics.get('overall_macro_f1', 0):.4f} (ç›®æ¨™: â‰¥0.76)")

        # ç›®æ¨™é”æˆç‹€æ³
        targets = results["target_achieved"]
        achievements = []
        if targets["bullying_f1_075"]:
            achievements.append("âœ… éœ¸å‡Œåµæ¸¬F1ç›®æ¨™é”æˆ!")
        else:
            achievements.append("âŒ éœ¸å‡Œåµæ¸¬F1ç›®æ¨™æœªé”æˆ")

        if targets["toxicity_f1_078"]:
            achievements.append("âœ… æ¯’æ€§åµæ¸¬F1ç›®æ¨™é”æˆ!")
        else:
            achievements.append("âŒ æ¯’æ€§åµæ¸¬F1ç›®æ¨™æœªé”æˆ")

        if targets["overall_macro_f1_076"]:
            achievements.append("âœ… ç¸½é«”Macro F1ç›®æ¨™é”æˆ!")
        else:
            achievements.append("âŒ ç¸½é«”Macro F1ç›®æ¨™æœªé”æˆ")

        for achievement in achievements:
            logger.info(achievement)

        # å¦‚æœæœªé”æˆç›®æ¨™ï¼Œæä¾›é€²ä¸€æ­¥å»ºè­°
        if not all(targets.values()):
            logger.info("\nğŸ’¡ é€²ä¸€æ­¥å„ªåŒ–å»ºè­°:")
            final_suggestions = self.analyze_and_suggest_improvements(test_metrics)
            for suggestion in final_suggestions:
                logger.info(f"   â€¢ {suggestion}")

            logger.info("   â€¢ è€ƒæ…®å¢åŠ è¨“ç·´epochs")
            logger.info("   â€¢ å˜—è©¦ä¸åŒçš„å­¸ç¿’ç‡")
            logger.info("   â€¢ ä½¿ç”¨æ›´å¤šçš„è³‡æ–™å¢å¼·æŠ€è¡“")
            logger.info("   â€¢ èª¿æ•´æ¨¡å‹æ¶æ§‹åƒæ•¸")

        # ä¿å­˜æ¨¡å‹å·¥ä»¶
        save_model_artifacts(
            model=self.model,
            tokenizer=self.tokenizer,
            config=self.config,
            output_dir=self.output_dir
        )

        self.tensorboard_writer.close()

        logger.info(f"\nğŸ“ æ‰€æœ‰è¼¸å‡ºå·²ä¿å­˜è‡³: {self.output_dir}")
        logger.info(f"ğŸ“Š TensorBoard: tensorboard --logdir {self.output_dir}/tensorboard_logs")

        return results


def main():
    parser = argparse.ArgumentParser(description="å°ˆé–€å„ªåŒ–éœ¸å‡Œåµæ¸¬F1åˆ†æ•¸çš„è¨“ç·´è…³æœ¬")
    parser.add_argument(
        "--config",
        type=str,
        default="configs/training/bullying_f1_optimization.yaml",
        help="è¨“ç·´é…ç½®æª”æ¡ˆè·¯å¾‘"
    )
    parser.add_argument(
        "--experiment-name",
        type=str,
        default=None,
        help="å¯¦é©—åç¨±"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="è¼¸å‡ºç›®éŒ„ (è¦†è“‹é…ç½®æª”æ¡ˆè¨­å®š)"
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        default=None,
        help="è³‡æ–™ç›®éŒ„ (è¦†è“‹é…ç½®æª”æ¡ˆè¨­å®š)"
    )

    args = parser.parse_args()

    # ç”Ÿæˆå¯¦é©—åç¨±
    if args.experiment_name is None:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        args.experiment_name = f"bullying_f1_optimization_{timestamp}"

    # åˆå§‹æ—¥èªŒ
    setup_logging(level=logging.INFO)

    # æª¢æŸ¥GPU
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        logger.info(f"ğŸš€ ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")

        # RTX 3050ç‰¹æ®Šæç¤º
        if "3050" in gpu_name:
            logger.info("ğŸ’¡ åµæ¸¬åˆ°RTX 3050ï¼Œå·²è‡ªå‹•å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨")
    else:
        logger.warning("âš ï¸  æœªåµæ¸¬åˆ°GPUï¼Œå°‡ä½¿ç”¨CPUè¨“ç·´")

    # å»ºç«‹è¨“ç·´å™¨
    try:
        trainer = BullyingF1Optimizer(
            config_path=args.config,
            experiment_name=args.experiment_name
        )

        # è¦†è“‹é…ç½®
        if args.output_dir:
            trainer.output_dir = Path(args.output_dir) / args.experiment_name
            trainer.output_dir.mkdir(parents=True, exist_ok=True)

        if args.data_dir:
            # æ›´æ–°è³‡æ–™è·¯å¾‘
            trainer.config["data"]["train_path"] = f"{args.data_dir}/train.json"
            trainer.config["data"]["val_path"] = f"{args.data_dir}/val.json"
            trainer.config["data"]["test_path"] = f"{args.data_dir}/test.json"

        # é–‹å§‹è¨“ç·´
        logger.info(f"ğŸ¯ ç›®æ¨™: éœ¸å‡ŒF1â‰¥0.75, æ¯’æ€§F1â‰¥0.78, ç¸½é«”F1â‰¥0.76")
        logger.info(f"ğŸ“ å¯¦é©—åç¨±: {args.experiment_name}")

        results = trainer.train()

        # æœ€çµ‚è¼¸å‡º
        success = all(results["target_achieved"].values())
        if success:
            logger.info("ğŸ‰ æ‰€æœ‰ç›®æ¨™å·²é”æˆ!")
            return 0
        else:
            logger.info("ğŸ“ˆ éƒ¨åˆ†ç›®æ¨™æœªé”æˆï¼Œè«‹æŸ¥çœ‹æ”¹é€²å»ºè­°")
            return 1

    except Exception as e:
        logger.error(f"âŒ è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logger.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        return 1


if __name__ == "__main__":
    sys.exit(main())