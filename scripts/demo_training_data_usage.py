#!/usr/bin/env python3
"""
è¨“ç·´è³‡æ–™ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•è¼‰å…¥å’Œä½¿ç”¨æº–å‚™å¥½çš„è¨“ç·´è³‡æ–™
"""

import sys
import os
from pathlib import Path
import torch

# æ·»åŠ srcè·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from cyberpuppy.data.loader import create_data_loader, TrainingDataset


def demo_basic_data_loading():
    """ç¤ºä¾‹ï¼šåŸºæœ¬è³‡æ–™è¼‰å…¥"""
    print("=" * 60)
    print("1. åŸºæœ¬è³‡æ–™è¼‰å…¥ç¤ºä¾‹")
    print("=" * 60)

    # æª¢æŸ¥è³‡æ–™æ˜¯å¦å­˜åœ¨
    data_path = "./data/processed/training_dataset"
    if not Path(data_path).exists():
        print(f"âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {data_path}")
        print("è«‹å…ˆé‹è¡Œ python scripts/prepare_complete_training_data.py")
        return

    try:
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        data_loader = create_data_loader(
            data_path=data_path,
            batch_size=4,
            include_features=False,
            num_workers=0
        )

        print(f"âœ… æˆåŠŸå‰µå»ºè³‡æ–™è¼‰å…¥å™¨")
        print(f"å¯ç”¨åˆ†å‰²: {list(data_loader.datasets.keys())}")

        # ç²å–çµ±è¨ˆè³‡è¨Š
        stats = data_loader.get_statistics()
        print(f"ç¸½æ¨£æœ¬æ•¸: {stats['total_samples']}")

        for split, split_stats in stats['splits'].items():
            print(f"{split}: {split_stats['size']} æ¨£æœ¬")

        # æ¸¬è©¦è¨“ç·´é›†è¼‰å…¥
        train_loader = data_loader.get_dataloader('train')
        if train_loader:
            print("\nğŸ“Š è¨“ç·´é›†ç¬¬ä¸€å€‹æ‰¹æ¬¡:")
            for batch in train_loader:
                print(f"  æ‰¹æ¬¡å¤§å°: {len(batch['texts'])}")
                print(f"  æ¨™ç±¤é¡å‹: {list(batch['labels'].keys())}")
                print(f"  æ–‡å­—ç¯„ä¾‹: {batch['texts'][0][:50]}...")

                # é¡¯ç¤ºæ¨™ç±¤åˆ†ä½ˆ
                for label_type, label_tensor in batch['labels'].items():
                    unique_labels = torch.unique(label_tensor)
                    print(f"  {label_type} æ¨™ç±¤: {unique_labels.tolist()}")
                break

    except Exception as e:
        print(f"âŒ è³‡æ–™è¼‰å…¥å¤±æ•—: {e}")


def demo_feature_extraction():
    """ç¤ºä¾‹ï¼šç‰¹å¾µæå–"""
    print("\n" + "=" * 60)
    print("2. ç‰¹å¾µæå–ç¤ºä¾‹")
    print("=" * 60)

    data_path = "./data/processed/training_dataset"
    if not Path(data_path).exists():
        print("âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éç‰¹å¾µæå–ç¤ºä¾‹")
        return

    try:
        # è¼‰å…¥å¸¶ç‰¹å¾µçš„è³‡æ–™
        data_loader = create_data_loader(
            data_path=data_path,
            batch_size=2,
            include_features=True,  # å•Ÿç”¨ç‰¹å¾µæå–
            num_workers=0
        )

        train_loader = data_loader.get_dataloader('train')
        if train_loader:
            print("âœ… æˆåŠŸå‰µå»ºå¸¶ç‰¹å¾µçš„è³‡æ–™è¼‰å…¥å™¨")

            for batch in train_loader:
                print(f"æ‰¹æ¬¡å¤§å°: {len(batch['texts'])}")

                if 'features' in batch:
                    print(f"ç‰¹å¾µå½¢ç‹€: {batch['features'].shape}")
                    print(f"ç‰¹å¾µæ•¸é‡: {len(batch['feature_names'])}")
                    print(f"ç‰¹å¾µç¯„ä¾‹: {batch['feature_names'][:5]}")
                    print(f"ç‰¹å¾µå€¼ç¯„ä¾‹: {batch['features'][0][:5].tolist()}")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°ç‰¹å¾µè³‡æ–™")
                break

    except Exception as e:
        print(f"âŒ ç‰¹å¾µæå–å¤±æ•—: {e}")


def demo_class_weights():
    """ç¤ºä¾‹ï¼šé¡åˆ¥æ¬Šé‡è¨ˆç®—"""
    print("\n" + "=" * 60)
    print("3. é¡åˆ¥æ¬Šé‡è¨ˆç®—ç¤ºä¾‹")
    print("=" * 60)

    data_path = "./data/processed/training_dataset"
    if not Path(data_path).exists():
        print("âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éé¡åˆ¥æ¬Šé‡ç¤ºä¾‹")
        return

    try:
        # å‰µå»ºè¨“ç·´è³‡æ–™é›†
        train_dataset = TrainingDataset(
            data_path=data_path,
            split='train'
        )

        print(f"âœ… è¼‰å…¥è¨“ç·´é›†: {len(train_dataset)} æ¨£æœ¬")

        # ç²å–æ¨™ç±¤åˆ†ä½ˆ
        distributions = train_dataset.get_label_distributions()
        print("\nğŸ“Š æ¨™ç±¤åˆ†ä½ˆ:")
        for label_type, dist in distributions.items():
            print(f"  {label_type}: {dist}")

        # è¨ˆç®—é¡åˆ¥æ¬Šé‡
        print("\nâš–ï¸ é¡åˆ¥æ¬Šé‡:")
        for task in ['toxicity', 'bullying', 'emotion']:
            try:
                weights = train_dataset.get_class_weights(task)
                print(f"  {task}: {weights.tolist()}")
            except Exception as e:
                print(f"  {task}: è¨ˆç®—å¤±æ•— ({e})")

    except Exception as e:
        print(f"âŒ é¡åˆ¥æ¬Šé‡è¨ˆç®—å¤±æ•—: {e}")


def demo_sample_access():
    """ç¤ºä¾‹ï¼šå–®å€‹æ¨£æœ¬å­˜å–"""
    print("\n" + "=" * 60)
    print("4. å–®å€‹æ¨£æœ¬å­˜å–ç¤ºä¾‹")
    print("=" * 60)

    data_path = "./data/processed/training_dataset"
    if not Path(data_path).exists():
        print("âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éæ¨£æœ¬å­˜å–ç¤ºä¾‹")
        return

    try:
        # å‰µå»ºè³‡æ–™é›†
        dataset = TrainingDataset(
            data_path=data_path,
            split='train',
            include_features=False
        )

        print(f"âœ… è¼‰å…¥è³‡æ–™é›†: {len(dataset)} æ¨£æœ¬")

        # å­˜å–å‰å¹¾å€‹æ¨£æœ¬
        print("\nğŸ“ æ¨£æœ¬ç¯„ä¾‹:")
        for i in range(min(3, len(dataset))):
            sample = dataset[i]
            print(f"\næ¨£æœ¬ {i+1}:")
            print(f"  æ–‡å­—: {sample['text'][:80]}...")
            print(f"  æ¯’æ€§: {sample['original_labels']['toxicity']}")
            print(f"  éœ¸å‡Œ: {sample['original_labels']['bullying']}")
            print(f"  æƒ…ç·’: {sample['original_labels']['emotion']}")
            print(f"  ç·¨ç¢¼æ¨™ç±¤: {sample['labels']}")

    except Exception as e:
        print(f"âŒ æ¨£æœ¬å­˜å–å¤±æ•—: {e}")


def demo_multi_task_training_setup():
    """ç¤ºä¾‹ï¼šå¤šä»»å‹™è¨“ç·´è¨­ç½®"""
    print("\n" + "=" * 60)
    print("5. å¤šä»»å‹™è¨“ç·´è¨­ç½®ç¤ºä¾‹")
    print("=" * 60)

    data_path = "./data/processed/training_dataset"
    if not Path(data_path).exists():
        print("âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éè¨“ç·´è¨­ç½®ç¤ºä¾‹")
        return

    try:
        # å‰µå»ºå¤šä»»å‹™è³‡æ–™è¼‰å…¥å™¨
        data_loader = create_data_loader(
            data_path=data_path,
            batch_size=8,
            include_features=False,
            num_workers=0
        )

        # ç²å–è¨“ç·´å’Œé©—è­‰è¼‰å…¥å™¨
        train_loader = data_loader.get_dataloader('train')
        dev_loader = data_loader.get_dataloader('dev')

        print(f"âœ… è¨“ç·´è¼‰å…¥å™¨: {len(train_loader)} æ‰¹æ¬¡")
        print(f"âœ… é©—è­‰è¼‰å…¥å™¨: {len(dev_loader)} æ‰¹æ¬¡")

        # æ¨¡æ“¬è¨“ç·´å¾ªç’°
        print("\nğŸš€ æ¨¡æ“¬è¨“ç·´å¾ªç’°:")
        epoch = 1
        for i, batch in enumerate(train_loader):
            if i >= 2:  # åªç¤ºä¾‹å‰å…©å€‹æ‰¹æ¬¡
                break

            texts = batch['texts']
            labels = batch['labels']

            print(f"Epoch {epoch}, Batch {i+1}:")
            print(f"  æ‰¹æ¬¡å¤§å°: {len(texts)}")

            # æ¨¡æ“¬å¤šä»»å‹™æå¤±è¨ˆç®—
            task_info = []
            for task, task_labels in labels.items():
                unique_labels = torch.unique(task_labels)
                task_info.append(f"{task}({len(unique_labels)}é¡)")

            print(f"  ä»»å‹™: {', '.join(task_info)}")

        # ç²å–é¡åˆ¥æ¬Šé‡ç”¨æ–¼æå¤±å‡½æ•¸
        print("\nâš–ï¸ å»ºè­°çš„æå¤±å‡½æ•¸æ¬Šé‡:")
        toxicity_weights = data_loader.get_class_weights('toxicity')
        if toxicity_weights is not None:
            print(f"æ¯’æ€§ä»»å‹™æ¬Šé‡: {toxicity_weights.tolist()}")

    except Exception as e:
        print(f"âŒ è¨“ç·´è¨­ç½®å¤±æ•—: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    print("CyberPuppy è¨“ç·´è³‡æ–™ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)

    # é‹è¡Œæ‰€æœ‰ç¤ºä¾‹
    demo_basic_data_loading()
    demo_feature_extraction()
    demo_class_weights()
    demo_sample_access()
    demo_multi_task_training_setup()

    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹å®Œæˆ!")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. æŸ¥çœ‹ docs/DATA_USAGE_GUIDE.md ç²å–è©³ç´°ä½¿ç”¨èªªæ˜")
    print("   2. é‹è¡Œ python scripts/run_data_tests.py é©—è­‰ç³»çµ±")
    print("   3. é–‹å§‹æ¨¡å‹è¨“ç·´é–‹ç™¼")


if __name__ == "__main__":
    main()