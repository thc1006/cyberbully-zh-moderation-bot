#!/usr/bin/env python3
"""
è³‡æ–™é›†ä¿®å¾©è…³æœ¬
è§£æ±º ChnSentiCorpã€DMSC v2ã€NTUSD çš„ä¸‹è¼‰å•é¡Œ
"""

import os
import sys
import json
import zipfile
import logging
from pathlib import Path
from typing import Dict, Optional
import subprocess

import requests
from tqdm import tqdm

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DatasetFixer:
    """è³‡æ–™é›†ä¿®å¾©å™¨"""

    def __init__(self, base_dir: str = "./data/raw"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'CyberPuppy-Dataset-Fixer/1.0'
        })

    def fix_chnsenticorp(self) -> bool:
        """
        ä¿®å¾© ChnSentiCorp è³‡æ–™é›†
        ä½¿ç”¨ç›´æ¥ HTTP ä¸‹è¼‰è€Œé Hugging Face datasets åº«
        """
        logger.info("="*60)
        logger.info("ä¿®å¾© ChnSentiCorp è³‡æ–™é›†")
        logger.info("="*60)

        dest_dir = self.base_dir / "chnsenticorp"
        dest_dir.mkdir(parents=True, exist_ok=True)

        # æ–¹æ¡ˆ 1: å¾ Hugging Face Hub ç›´æ¥ä¸‹è¼‰ parquet æª”æ¡ˆ
        hf_urls = {
            "train": "https://huggingface.co/datasets/seamew/ChnSentiCorp/resolve/main/train.parquet",
            "validation": "https://huggingface.co/datasets/seamew/ChnSentiCorp/resolve/main/validation.parquet",
            "test": "https://huggingface.co/datasets/seamew/ChnSentiCorp/resolve/main/test.parquet"
        }

        success_count = 0

        for split, url in hf_urls.items():
            output_file = dest_dir / f"{split}.parquet"

            if output_file.exists():
                logger.info(f"âœ“ {split}.parquet å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")
                success_count += 1
                continue

            try:
                logger.info(f"ä¸‹è¼‰ {split} split...")
                response = self.session.get(url, stream=True, timeout=120)
                response.raise_for_status()

                total_size = int(response.headers.get('content-length', 0))

                with open(output_file, 'wb') as f:
                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=split) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))

                logger.info(f"âœ“ {split}.parquet ä¸‹è¼‰æˆåŠŸ")
                success_count += 1

            except Exception as e:
                logger.error(f"âœ— ä¸‹è¼‰ {split} å¤±æ•—: {e}")

        # è½‰æ› parquet åˆ° JSONï¼ˆéœ€è¦ pandas å’Œ pyarrowï¼‰
        try:
            import pandas as pd

            logger.info("\nè½‰æ› parquet åˆ° JSON æ ¼å¼...")

            for split in ["train", "validation", "test"]:
                parquet_file = dest_dir / f"{split}.parquet"
                json_file = dest_dir / f"{split}.json"

                if not parquet_file.exists():
                    continue

                df = pd.read_parquet(parquet_file)
                df.to_json(json_file, orient='records', force_ascii=False, indent=2)
                logger.info(f"âœ“ {split}.json è½‰æ›å®Œæˆ ({len(df)} ç­†è³‡æ–™)")

            # å„²å­˜å…ƒè³‡æ–™
            metadata = {
                "source": "seamew/ChnSentiCorp",
                "download_method": "direct_http",
                "splits": ["train", "validation", "test"],
                "format": "parquet+json",
                "description": "ä¸­æ–‡æƒ…æ„Ÿåˆ†æè³‡æ–™é›†ï¼ˆæ­£è² äºŒå…ƒï¼‰"
            }

            with open(dest_dir / "metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            logger.info(f"\nâœ“ ChnSentiCorp ä¿®å¾©å®Œæˆï¼")
            return success_count == 3

        except ImportError:
            logger.warning("âš  pandas æˆ– pyarrow æœªå®‰è£ï¼Œç„¡æ³•è½‰æ› parquet")
            logger.info("å®‰è£æŒ‡ä»¤: pip install pandas pyarrow")
            return success_count == 3
        except Exception as e:
            logger.error(f"âœ— è½‰æ›éç¨‹å‡ºéŒ¯: {e}")
            return False

    def fix_dmsc(self) -> bool:
        """
        ä¿®å¾© DMSC v2 è³‡æ–™é›†
        è§£å£“ç¸®å·²ä¸‹è¼‰çš„ ZIP æª”æ¡ˆä¸¦é©—è­‰
        """
        logger.info("\n" + "="*60)
        logger.info("ä¿®å¾© DMSC v2 è³‡æ–™é›†")
        logger.info("="*60)

        dest_dir = self.base_dir / "dmsc"
        zip_file = dest_dir / "dmsc_kaggle.zip"

        if not zip_file.exists():
            logger.error(f"âœ— ZIP æª”æ¡ˆä¸å­˜åœ¨: {zip_file}")
            logger.info("è«‹å…ˆä¸‹è¼‰: https://github.com/ownthink/dmsc-v2/releases/download/v2.0/dmsc_v2.zip")
            return False

        try:
            # æª¢æŸ¥ ZIP æª”æ¡ˆå¤§å°
            zip_size_mb = zip_file.stat().st_size / (1024 * 1024)
            logger.info(f"ZIP æª”æ¡ˆå¤§å°: {zip_size_mb:.2f} MB")

            # è§£å£“ç¸®
            logger.info("è§£å£“ç¸®ä¸­...")
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                members = zip_ref.namelist()
                logger.info(f"ZIP å…§åŒ…å« {len(members)} å€‹æª”æ¡ˆ")

                with tqdm(total=len(members), desc="è§£å£“ç¸®") as pbar:
                    for member in members:
                        zip_ref.extract(member, dest_dir)
                        pbar.update(1)

            logger.info("âœ“ è§£å£“ç¸®å®Œæˆ")

            # åˆ—å‡ºè§£å£“ç¸®å¾Œçš„æª”æ¡ˆ
            extracted_files = list(dest_dir.glob("**/*.csv"))
            logger.info(f"\næ‰¾åˆ° {len(extracted_files)} å€‹ CSV æª”æ¡ˆ:")
            for f in extracted_files[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
                size_mb = f.stat().st_size / (1024 * 1024)
                logger.info(f"  - {f.name}: {size_mb:.2f} MB")

            if len(extracted_files) > 5:
                logger.info(f"  ... å’Œå…¶ä»– {len(extracted_files) - 5} å€‹æª”æ¡ˆ")

            # é©—è­‰ä¸»æª”æ¡ˆ
            main_csv = dest_dir / "DMSC.csv"
            if main_csv.exists():
                csv_size_mb = main_csv.stat().st_size / (1024 * 1024)
                logger.info(f"\nâœ“ ä¸»æª”æ¡ˆ DMSC.csv: {csv_size_mb:.2f} MB")

                # è®€å–å‰å¹¾è¡Œé©—è­‰æ ¼å¼
                import pandas as pd
                df_sample = pd.read_csv(main_csv, nrows=5)
                logger.info(f"âœ“ è³‡æ–™æ ¼å¼é©—è­‰é€šé")
                logger.info(f"  æ¬„ä½: {list(df_sample.columns)}")
                logger.info(f"  æ¨£æœ¬æ•¸ï¼ˆä¼°è¨ˆï¼‰: {csv_size_mb * 2500:.0f} ç­†")  # ä¼°è¨ˆ

                return True
            else:
                logger.warning("âš  æœªæ‰¾åˆ° DMSC.csv ä¸»æª”æ¡ˆ")
                return len(extracted_files) > 0

        except Exception as e:
            logger.error(f"âœ— DMSC ä¿®å¾©å¤±æ•—: {e}")
            return False

    def fix_ntusd(self) -> bool:
        """
        ä¿®å¾© NTUSD è³‡æ–™é›†
        é‡æ–°å…‹éš†å®Œæ•´å„²å­˜åº«
        """
        logger.info("\n" + "="*60)
        logger.info("ä¿®å¾© NTUSD è³‡æ–™é›†")
        logger.info("="*60)

        dest_dir = self.base_dir / "ntusd"
        repo_url = "https://github.com/candlewill/NTUSD.git"

        # æª¢æŸ¥ç¾æœ‰æª”æ¡ˆå¤§å°
        existing_files = {}
        for filename in ["positive.txt", "negative.txt"]:
            filepath = dest_dir / filename
            if filepath.exists():
                size = filepath.stat().st_size
                existing_files[filename] = size
                logger.info(f"ç¾æœ‰æª”æ¡ˆ {filename}: {size} bytes")

        # å¦‚æœæª”æ¡ˆå¤ªå°ï¼ˆ< 100 bytesï¼‰ï¼Œéœ€è¦é‡æ–°ä¸‹è¼‰
        needs_update = any(size < 100 for size in existing_files.values()) or len(existing_files) < 2

        if not needs_update:
            logger.info("âœ“ NTUSD æª”æ¡ˆçœ‹èµ·ä¾†æ­£å¸¸ï¼Œè·³éæ›´æ–°")
            return True

        try:
            # åˆªé™¤èˆŠçš„ç›®éŒ„
            if dest_dir.exists():
                logger.info(f"åˆªé™¤èˆŠç›®éŒ„: {dest_dir}")
                import shutil
                shutil.rmtree(dest_dir)

            # é‡æ–°å…‹éš†
            logger.info(f"å…‹éš†å„²å­˜åº«: {repo_url}")
            subprocess.run([
                "git", "clone", "--depth", "1", repo_url, str(dest_dir)
            ], check=True)

            # é©—è­‰æª”æ¡ˆ
            expected_files = {
                "positive.txt": 3000,   # è‡³å°‘ 3KB
                "negative.txt": 3000,   # è‡³å°‘ 3KB
                "ntusd-positive.txt": 1000,
                "ntusd-negative.txt": 1000
            }

            logger.info("\né©—è­‰æª”æ¡ˆ:")
            success = True
            for filename, min_size in expected_files.items():
                filepath = dest_dir / filename
                if filepath.exists():
                    size = filepath.stat().st_size
                    status = "âœ“" if size >= min_size else "âš "
                    logger.info(f"{status} {filename}: {size} bytes")
                    if size < min_size:
                        success = False
                else:
                    logger.warning(f"âœ— {filename}: ä¸å­˜åœ¨")

            if success:
                logger.info("\nâœ“ NTUSD ä¿®å¾©å®Œæˆï¼")
            else:
                logger.warning("\nâš  NTUSD éƒ¨åˆ†æª”æ¡ˆå¯èƒ½ä¸å®Œæ•´")

            return success

        except subprocess.CalledProcessError as e:
            logger.error(f"âœ— Git å…‹éš†å¤±æ•—: {e}")
            return False
        except Exception as e:
            logger.error(f"âœ— NTUSD ä¿®å¾©å¤±æ•—: {e}")
            return False

    def verify_all(self) -> Dict[str, bool]:
        """é©—è­‰æ‰€æœ‰è³‡æ–™é›†"""
        logger.info("\n" + "="*60)
        logger.info("é©—è­‰æ‰€æœ‰è³‡æ–™é›†ç‹€æ…‹")
        logger.info("="*60)

        results = {}

        # ChnSentiCorp
        chnsenti_dir = self.base_dir / "chnsenticorp"
        chnsenti_files = list(chnsenti_dir.glob("*.json")) + list(chnsenti_dir.glob("*.parquet"))
        results["ChnSentiCorp"] = len(chnsenti_files) >= 3
        logger.info(f"ChnSentiCorp: {'âœ“' if results['ChnSentiCorp'] else 'âœ—'} ({len(chnsenti_files)} å€‹æª”æ¡ˆ)")

        # DMSC
        dmsc_dir = self.base_dir / "dmsc"
        dmsc_csv = dmsc_dir / "DMSC.csv"
        results["DMSC"] = dmsc_csv.exists() and dmsc_csv.stat().st_size > 100_000_000  # > 100MB
        size_mb = dmsc_csv.stat().st_size / (1024 * 1024) if dmsc_csv.exists() else 0
        logger.info(f"DMSC v2: {'âœ“' if results['DMSC'] else 'âœ—'} ({size_mb:.2f} MB)")

        # NTUSD
        ntusd_dir = self.base_dir / "ntusd"
        ntusd_files = [ntusd_dir / f for f in ["positive.txt", "negative.txt"]]
        results["NTUSD"] = all(f.exists() and f.stat().st_size > 1000 for f in ntusd_files)
        logger.info(f"NTUSD: {'âœ“' if results['NTUSD'] else 'âœ—'}")

        logger.info("="*60)
        return results

    def run_all_fixes(self):
        """åŸ·è¡Œæ‰€æœ‰ä¿®å¾©"""
        results = {}

        logger.info("\nğŸ”§ é–‹å§‹ä¿®å¾©æ‰€æœ‰è³‡æ–™é›†...\n")

        # 1. ä¿®å¾© ChnSentiCorp
        results["ChnSentiCorp"] = self.fix_chnsenticorp()

        # 2. ä¿®å¾© DMSC
        results["DMSC"] = self.fix_dmsc()

        # 3. ä¿®å¾© NTUSD
        results["NTUSD"] = self.fix_ntusd()

        # 4. æœ€çµ‚é©—è­‰
        verification = self.verify_all()

        # ç¸½çµ
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ä¿®å¾©ç¸½çµ")
        logger.info("="*60)

        all_success = all(results.values())

        for dataset, success in results.items():
            status = "âœ“ æˆåŠŸ" if success else "âœ— å¤±æ•—"
            verify = "âœ“" if verification.get(dataset, False) else "âœ—"
            logger.info(f"{dataset}: {status} (é©—è­‰: {verify})")

        logger.info("="*60)

        if all_success:
            logger.info("\nğŸ‰ æ‰€æœ‰è³‡æ–™é›†ä¿®å¾©å®Œæˆï¼")
        else:
            logger.warning("\nâš  éƒ¨åˆ†è³‡æ–™é›†ä¿®å¾©å¤±æ•—ï¼Œè«‹æŸ¥çœ‹ä¸Šæ–¹æ—¥èªŒ")

        return results


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ä¿®å¾©è³‡æ–™é›†ä¸‹è¼‰å•é¡Œ")
    parser.add_argument(
        "--dataset",
        choices=["chnsenticorp", "dmsc", "ntusd", "all"],
        default="all",
        help="è¦ä¿®å¾©çš„è³‡æ–™é›†"
    )
    parser.add_argument(
        "--output-dir",
        default="./data/raw",
        help="è³‡æ–™é›†ç›®éŒ„"
    )
    parser.add_argument(
        "--verify-only",
        action="store_true",
        help="åªé©—è­‰ï¼Œä¸åŸ·è¡Œä¿®å¾©"
    )

    args = parser.parse_args()

    fixer = DatasetFixer(base_dir=args.output_dir)

    if args.verify_only:
        fixer.verify_all()
    elif args.dataset == "all":
        fixer.run_all_fixes()
    elif args.dataset == "chnsenticorp":
        fixer.fix_chnsenticorp()
        fixer.verify_all()
    elif args.dataset == "dmsc":
        fixer.fix_dmsc()
        fixer.verify_all()
    elif args.dataset == "ntusd":
        fixer.fix_ntusd()
        fixer.verify_all()


if __name__ == "__main__":
    main()