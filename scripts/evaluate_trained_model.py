#!/usr/bin/env python3
"""
é‡å°è¨“ç·´å®Œæˆçš„ CyberPuppy æ¨¡å‹çš„å°ˆç”¨è©•ä¼°è…³æœ¬
"""

import json
import os
import sys
import numpy as np
import pandas as pd
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support,
    confusion_matrix, classification_report,
    roc_auc_score, average_precision_score
)
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class CyberPuppyModelEvaluator:
    """CyberPuppy æ¨¡å‹è©•ä¼°å™¨"""

    def __init__(self, model_path: str, output_dir: str):
        self.model_path = model_path
        self.output_dir = output_dir
        self.results = {}

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(output_dir, exist_ok=True)

        print(f"åˆå§‹åŒ–è©•ä¼°å™¨...")
        print(f"æ¨¡å‹è·¯å¾‘: {model_path}")
        print(f"è¼¸å‡ºç›®éŒ„: {output_dir}")

    def load_model(self):
        """è¼‰å…¥æ¨¡å‹"""
        print("è¼‰å…¥æ¨¡å‹...")

        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification

            # è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)

            # è¨­ç½®è©•ä¼°æ¨¡å¼
            self.model.eval()

            # æª¢æŸ¥æ˜¯å¦æœ‰ GPU
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model.to(self.device)

            print(f"æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨è¨­å‚™: {self.device}")
            print(f"æ¨¡å‹é…ç½®: {self.model.config}")

        except Exception as e:
            print(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise

    def load_test_data(self, data_path: str) -> Dict[str, List]:
        """è¼‰å…¥æ¸¬è©¦æ•¸æ“š"""
        print(f"è¼‰å…¥æ¸¬è©¦æ•¸æ“š: {data_path}")

        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        texts = []
        labels = []

        # è½‰æ›æ•¸æ“šæ ¼å¼
        for item in data:
            texts.append(item['text'])

            # æå– toxicity æ¨™ç±¤
            toxicity_label = item['label'].get('toxicity', 'none')

            # å°‡æ¨™ç±¤è½‰æ›ç‚ºæ•¸å­— (åªæœ‰2å€‹é¡åˆ¥: none, toxic)
            if toxicity_label == 'none':
                labels.append(0)
            elif toxicity_label == 'toxic':
                labels.append(1)
            else:
                labels.append(0)  # é»˜èªç‚º none

        print(f"è¼‰å…¥ {len(texts)} å€‹æ¸¬è©¦æ¨£æœ¬")
        print(f"æ¨™ç±¤åˆ†å¸ƒ: {Counter(labels)}")

        return {
            'texts': texts,
            'labels': labels,
            'raw_data': data
        }

    def predict_batch(self, texts: List[str], batch_size: int = 32) -> Tuple[List[int], List[float], List[List[float]]]:
        """æ‰¹é‡é æ¸¬"""
        print(f"é–‹å§‹æ‰¹é‡é æ¸¬ {len(texts)} å€‹æ¨£æœ¬...")

        all_predictions = []
        all_confidences = []
        all_probabilities = []

        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]

                # åˆ†è©
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors='pt'
                )

                # ç§»åˆ°è¨­å‚™
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

                # é æ¸¬
                outputs = self.model(**inputs)
                logits = outputs.logits

                # è¨ˆç®—æ¦‚ç‡
                probs = torch.softmax(logits, dim=-1)

                # ç²å–é æ¸¬çµæœ
                predictions = torch.argmax(probs, dim=-1)
                confidences = torch.max(probs, dim=-1)[0]

                # è½‰æ›ç‚ºåˆ—è¡¨
                all_predictions.extend(predictions.cpu().numpy().tolist())
                all_confidences.extend(confidences.cpu().numpy().tolist())
                all_probabilities.extend(probs.cpu().numpy().tolist())

                if (i // batch_size + 1) % 10 == 0:
                    print(f"å·²è™•ç† {i + len(batch_texts)} / {len(texts)} æ¨£æœ¬")

        print("æ‰¹é‡é æ¸¬å®Œæˆ")
        return all_predictions, all_confidences, all_probabilities

    def calculate_metrics(self, y_true: List[int], y_pred: List[int], y_proba: List[List[float]]) -> Dict[str, Any]:
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        print("è¨ˆç®—è©•ä¼°æŒ‡æ¨™...")

        # åŸºç¤æŒ‡æ¨™
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='weighted')
        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')

        # æ¯é¡åˆ¥æŒ‡æ¨™
        per_class_precision, per_class_recall, per_class_f1, per_class_support = precision_recall_fscore_support(
            y_true, y_pred, average=None
        )

        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(y_true, y_pred)

        # åˆ†é¡å ±å‘Š (åªæœ‰2å€‹é¡åˆ¥)
        class_names = ['none', 'toxic']
        clf_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)

        # è¨ˆç®—æ¯é¡åˆ¥çš„ AUCï¼ˆå¦‚æœæœ‰è¶³å¤ çš„é¡åˆ¥ï¼‰
        auc_scores = {}
        if len(set(y_true)) > 1 and len(y_proba[0]) > 1:
            try:
                y_proba_array = np.array(y_proba)
                for i, class_name in enumerate(class_names):
                    if i < y_proba_array.shape[1]:
                        # äºŒåˆ†é¡ AUC
                        y_true_binary = [1 if label == i else 0 for label in y_true]
                        if len(set(y_true_binary)) > 1:  # ç¢ºä¿æœ‰å…©å€‹é¡åˆ¥
                            auc_scores[class_name] = roc_auc_score(y_true_binary, y_proba_array[:, i])
            except Exception as e:
                print(f"AUC è¨ˆç®—å¤±æ•—: {e}")

        metrics = {
            'accuracy': accuracy,
            'weighted_precision': precision,
            'weighted_recall': recall,
            'weighted_f1': f1,
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'per_class_metrics': {
                class_names[i]: {
                    'precision': per_class_precision[i] if i < len(per_class_precision) else 0,
                    'recall': per_class_recall[i] if i < len(per_class_recall) else 0,
                    'f1': per_class_f1[i] if i < len(per_class_f1) else 0,
                    'support': int(per_class_support[i]) if i < len(per_class_support) else 0
                }
                for i in range(len(class_names))
            },
            'confusion_matrix': cm.tolist(),
            'classification_report': clf_report,
            'auc_scores': auc_scores,
            'class_names': class_names
        }

        print("æŒ‡æ¨™è¨ˆç®—å®Œæˆ")
        return metrics

    def analyze_errors(self, texts: List[str], y_true: List[int], y_pred: List[int],
                      y_proba: List[List[float]]) -> Dict[str, Any]:
        """éŒ¯èª¤åˆ†æ"""
        print("é€²è¡ŒéŒ¯èª¤åˆ†æ...")

        class_names = ['none', 'toxic']

        # æ‰¾å‡ºéŒ¯èª¤æ¡ˆä¾‹
        errors = []
        correct_predictions = []

        for i, (text, true_label, pred_label, proba) in enumerate(zip(texts, y_true, y_pred, y_proba)):
            confidence = max(proba)

            if true_label != pred_label:
                errors.append({
                    'index': i,
                    'text': text,
                    'true_label': class_names[true_label],
                    'pred_label': class_names[pred_label],
                    'confidence': confidence,
                    'probabilities': proba,
                    'error_type': f"{class_names[true_label]} -> {class_names[pred_label]}"
                })
            else:
                correct_predictions.append({
                    'index': i,
                    'text': text,
                    'label': class_names[true_label],
                    'confidence': confidence
                })

        # æŒ‰ç½®ä¿¡åº¦æ’åºéŒ¯èª¤æ¡ˆä¾‹
        errors.sort(key=lambda x: x['confidence'], reverse=True)

        # åˆ†æéŒ¯èª¤æ¨¡å¼
        error_patterns = Counter([error['error_type'] for error in errors])

        # ä½ç½®ä¿¡åº¦æ­£ç¢ºé æ¸¬
        low_confidence_correct = [pred for pred in correct_predictions if pred['confidence'] < 0.7]
        low_confidence_correct.sort(key=lambda x: x['confidence'])

        # é«˜ç½®ä¿¡åº¦éŒ¯èª¤é æ¸¬
        high_confidence_errors = [error for error in errors if error['confidence'] > 0.7]

        error_analysis = {
            'total_errors': len(errors),
            'total_correct': len(correct_predictions),
            'error_rate': len(errors) / (len(errors) + len(correct_predictions)),
            'error_patterns': dict(error_patterns),
            'top_errors': errors[:20],  # å‰20å€‹éŒ¯èª¤æ¡ˆä¾‹
            'high_confidence_errors': high_confidence_errors[:10],
            'low_confidence_correct': low_confidence_correct[:10],
            'confidence_distribution': {
                'all_predictions': [max(proba) for proba in y_proba],
                'correct_predictions': [pred['confidence'] for pred in correct_predictions],
                'error_predictions': [error['confidence'] for error in errors]
            }
        }

        print(f"éŒ¯èª¤åˆ†æå®Œæˆ: {len(errors)} å€‹éŒ¯èª¤æ¡ˆä¾‹")
        return error_analysis

    def create_visualizations(self, metrics: Dict[str, Any], error_analysis: Dict[str, Any]):
        """å‰µå»ºè¦–è¦ºåŒ–åœ–è¡¨"""
        print("å‰µå»ºè¦–è¦ºåŒ–åœ–è¡¨...")

        # è¨­ç½®åœ–è¡¨é¢¨æ ¼
        plt.style.use('default')

        # 1. æ··æ·†çŸ©é™£
        plt.figure(figsize=(8, 6))
        cm = np.array(metrics['confusion_matrix'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=metrics['class_names'],
                   yticklabels=metrics['class_names'])
        plt.title('æ··æ·†çŸ©é™£')
        plt.xlabel('é æ¸¬æ¨™ç±¤')
        plt.ylabel('çœŸå¯¦æ¨™ç±¤')
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
        plt.close()

        # 2. æ¯é¡åˆ¥æŒ‡æ¨™å°æ¯”
        plt.figure(figsize=(12, 6))
        classes = metrics['class_names']
        x = np.arange(len(classes))
        width = 0.25

        precision_scores = [metrics['per_class_metrics'][c]['precision'] for c in classes]
        recall_scores = [metrics['per_class_metrics'][c]['recall'] for c in classes]
        f1_scores = [metrics['per_class_metrics'][c]['f1'] for c in classes]

        plt.bar(x - width, precision_scores, width, label='Precision', alpha=0.8)
        plt.bar(x, recall_scores, width, label='Recall', alpha=0.8)
        plt.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8)

        plt.xlabel('é¡åˆ¥')
        plt.ylabel('åˆ†æ•¸')
        plt.title('æ¯é¡åˆ¥æŒ‡æ¨™å°æ¯”')
        plt.xticks(x, classes)
        plt.legend()
        plt.ylim(0, 1.1)

        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, (p, r, f) in enumerate(zip(precision_scores, recall_scores, f1_scores)):
            plt.text(i - width, p + 0.01, f'{p:.3f}', ha='center', va='bottom', fontsize=8)
            plt.text(i, r + 0.01, f'{r:.3f}', ha='center', va='bottom', fontsize=8)
            plt.text(i + width, f + 0.01, f'{f:.3f}', ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'per_class_metrics.png'), dpi=300, bbox_inches='tight')
        plt.close()

        # 3. ç½®ä¿¡åº¦åˆ†å¸ƒ
        plt.figure(figsize=(12, 6))

        plt.subplot(1, 2, 1)
        correct_conf = error_analysis['confidence_distribution']['correct_predictions']
        error_conf = error_analysis['confidence_distribution']['error_predictions']

        plt.hist(correct_conf, alpha=0.7, label='æ­£ç¢ºé æ¸¬', bins=20, color='green')
        plt.hist(error_conf, alpha=0.7, label='éŒ¯èª¤é æ¸¬', bins=20, color='red')
        plt.xlabel('ç½®ä¿¡åº¦')
        plt.ylabel('é »ç‡')
        plt.title('é æ¸¬ç½®ä¿¡åº¦åˆ†å¸ƒ')
        plt.legend()

        plt.subplot(1, 2, 2)
        error_patterns = error_analysis['error_patterns']
        if error_patterns:
            patterns = list(error_patterns.keys())
            counts = list(error_patterns.values())

            plt.bar(range(len(patterns)), counts)
            plt.xlabel('éŒ¯èª¤é¡å‹')
            plt.ylabel('éŒ¯èª¤æ•¸é‡')
            plt.title('éŒ¯èª¤æ¨¡å¼åˆ†å¸ƒ')
            plt.xticks(range(len(patterns)), patterns, rotation=45)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'error_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()

        print("è¦–è¦ºåŒ–åœ–è¡¨å‰µå»ºå®Œæˆ")

    def generate_html_report(self, metrics: Dict[str, Any], error_analysis: Dict[str, Any],
                           evaluation_time: str) -> str:
        """ç”Ÿæˆ HTML å ±å‘Š"""
        print("ç”Ÿæˆ HTML å ±å‘Š...")

        # æª¢æŸ¥é—œéµæŒ‡æ¨™æ˜¯å¦é”æ¨™
        bullying_f1 = metrics['per_class_metrics']['toxic']['f1']
        overall_f1 = metrics['weighted_f1']
        accuracy = metrics['accuracy']

        # é”æ¨™ç‹€æ…‹
        f1_target_met = bullying_f1 >= 0.75
        precision_target_met = metrics['per_class_metrics']['toxic']['precision'] >= 0.70
        recall_target_met = metrics['per_class_metrics']['toxic']['recall'] >= 0.70

        html_content = f"""
<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CyberPuppy æ¨¡å‹è©•ä¼°å ±å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 40px; line-height: 1.6; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; }}
        .metric-card {{ background: white; border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin: 15px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
        .status-badge {{ padding: 5px 10px; border-radius: 20px; color: white; font-weight: bold; }}
        .status-pass {{ background-color: #28a745; }}
        .status-fail {{ background-color: #dc3545; }}
        .status-warning {{ background-color: #ffc107; color: #212529; }}
        table {{ width: 100%; border-collapse: collapse; margin: 15px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f8f9fa; font-weight: bold; }}
        .error-sample {{ background: #f8f9fa; border-left: 4px solid #dc3545; padding: 15px; margin: 10px 0; border-radius: 4px; }}
        .improvement-section {{ background: #e3f2fd; border-left: 4px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 4px; }}
        .chart-container {{ text-align: center; margin: 20px 0; }}
        .chart-container img {{ max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px; }}
        .summary-stats {{ display: flex; justify-content: space-around; flex-wrap: wrap; margin: 20px 0; }}
        .stat-item {{ text-align: center; padding: 15px; }}
        .stat-value {{ font-size: 2em; font-weight: bold; color: #667eea; }}
        .stat-label {{ color: #666; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¤– CyberPuppy éœ¸å‡Œåµæ¸¬æ¨¡å‹è©•ä¼°å ±å‘Š</h1>
        <p>è©•ä¼°æ™‚é–“: {evaluation_time}</p>
        <p>æ¨¡å‹è·¯å¾‘: {self.model_path}</p>
    </div>

    <div class="metric-card">
        <h2>ğŸ“Š è©•ä¼°ç¸½çµ</h2>
        <div class="summary-stats">
            <div class="stat-item">
                <div class="stat-value">{accuracy:.3f}</div>
                <div class="stat-label">æ•´é«”æº–ç¢ºç‡</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{overall_f1:.3f}</div>
                <div class="stat-label">æ•´é«” F1 åˆ†æ•¸</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{bullying_f1:.3f}</div>
                <div class="stat-label">éœ¸å‡Œåµæ¸¬ F1</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">{error_analysis['total_errors']}</div>
                <div class="stat-label">éŒ¯èª¤æ¡ˆä¾‹æ•¸</div>
            </div>
        </div>
    </div>

    <div class="metric-card">
        <h2>ğŸ¯ é—œéµæŒ‡æ¨™é”æ¨™ç‹€æ³</h2>
        <div class="metric-grid">
            <div>
                <h3>éœ¸å‡Œåµæ¸¬ F1 åˆ†æ•¸</h3>
                <span class="status-badge {'status-pass' if f1_target_met else 'status-fail'}">
                    {bullying_f1:.3f} {'âœ“ é”æ¨™' if f1_target_met else 'âœ— æœªé”æ¨™'} (ç›®æ¨™: â‰¥0.75)
                </span>
            </div>
            <div>
                <h3>éœ¸å‡Œåµæ¸¬ Precision</h3>
                <span class="status-badge {'status-pass' if precision_target_met else 'status-fail'}">
                    {metrics['per_class_metrics']['toxic']['precision']:.3f} {'âœ“ é”æ¨™' if precision_target_met else 'âœ— æœªé”æ¨™'} (ç›®æ¨™: â‰¥0.70)
                </span>
            </div>
            <div>
                <h3>éœ¸å‡Œåµæ¸¬ Recall</h3>
                <span class="status-badge {'status-pass' if recall_target_met else 'status-fail'}">
                    {metrics['per_class_metrics']['toxic']['recall']:.3f} {'âœ“ é”æ¨™' if recall_target_met else 'âœ— æœªé”æ¨™'} (ç›®æ¨™: â‰¥0.70)
                </span>
            </div>
        </div>
    </div>

    <div class="metric-card">
        <h2>ğŸ“ˆ è©³ç´°æŒ‡æ¨™</h2>
        <table>
            <tr>
                <th>é¡åˆ¥</th>
                <th>Precision</th>
                <th>Recall</th>
                <th>F1-Score</th>
                <th>Support</th>
            </tr>
        """

        for class_name in metrics['class_names']:
            class_metrics = metrics['per_class_metrics'][class_name]
            html_content += f"""
            <tr>
                <td>{class_name}</td>
                <td>{class_metrics['precision']:.3f}</td>
                <td>{class_metrics['recall']:.3f}</td>
                <td>{class_metrics['f1']:.3f}</td>
                <td>{class_metrics['support']}</td>
            </tr>
            """

        html_content += f"""
        </table>
    </div>

    <div class="metric-card">
        <h2>ğŸ” éŒ¯èª¤åˆ†æ</h2>
        <h3>éŒ¯èª¤æ¨¡å¼åˆ†å¸ƒ</h3>
        <table>
            <tr><th>éŒ¯èª¤é¡å‹</th><th>æ•¸é‡</th><th>ç™¾åˆ†æ¯”</th></tr>
        """

        total_errors = error_analysis['total_errors']
        for pattern, count in error_analysis['error_patterns'].items():
            percentage = (count / total_errors * 100) if total_errors > 0 else 0
            html_content += f"""
            <tr>
                <td>{pattern}</td>
                <td>{count}</td>
                <td>{percentage:.1f}%</td>
            </tr>
            """

        html_content += f"""
        </table>

        <h3>é«˜ç½®ä¿¡åº¦éŒ¯èª¤æ¡ˆä¾‹ (å‰5å€‹)</h3>
        """

        for i, error in enumerate(error_analysis['high_confidence_errors'][:5]):
            html_content += f"""
            <div class="error-sample">
                <strong>æ¡ˆä¾‹ {i+1}</strong> (ç½®ä¿¡åº¦: {error['confidence']:.3f})<br>
                <strong>æ–‡æœ¬:</strong> {error['text'][:200]}{'...' if len(error['text']) > 200 else ''}<br>
                <strong>çœŸå¯¦æ¨™ç±¤:</strong> {error['true_label']} â†’ <strong>é æ¸¬æ¨™ç±¤:</strong> {error['pred_label']}
            </div>
            """

        html_content += """
    </div>

    <div class="chart-container">
        <h2>ğŸ“Š è¦–è¦ºåŒ–åˆ†æ</h2>
        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 20px;">
            <div>
                <h3>æ··æ·†çŸ©é™£</h3>
                <img src="confusion_matrix.png" alt="æ··æ·†çŸ©é™£">
            </div>
            <div>
                <h3>æ¯é¡åˆ¥æŒ‡æ¨™å°æ¯”</h3>
                <img src="per_class_metrics.png" alt="æ¯é¡åˆ¥æŒ‡æ¨™">
            </div>
            <div style="grid-column: span 2;">
                <h3>éŒ¯èª¤åˆ†æ</h3>
                <img src="error_analysis.png" alt="éŒ¯èª¤åˆ†æ">
            </div>
        </div>
    </div>
        """

        # æ”¹é€²å»ºè­°
        improvements = []
        if not f1_target_met:
            improvements.append("éœ¸å‡Œåµæ¸¬ F1 åˆ†æ•¸æœªé”æ¨™æº– (0.75)ï¼Œå»ºè­°å¢åŠ è¨“ç·´æ•¸æ“šæˆ–èª¿æ•´æ¨¡å‹æ¶æ§‹")
        if not precision_target_met:
            improvements.append("éœ¸å‡Œåµæ¸¬ Precision åä½ï¼Œå¯èƒ½å­˜åœ¨è¼ƒå¤šèª¤å ±ï¼Œå»ºè­°æé«˜æ±ºç­–é–¾å€¼æˆ–æ”¹é€²ç‰¹å¾µæå–")
        if not recall_target_met:
            improvements.append("éœ¸å‡Œåµæ¸¬ Recall åä½ï¼Œå¯èƒ½æ¼æª¢è¼ƒå¤šçœŸå¯¦æ¡ˆä¾‹ï¼Œå»ºè­°å¢åŠ è² æ¨£æœ¬æ•¸æ“šæˆ–èª¿æ•´æå¤±å‡½æ•¸")

        if error_analysis['total_errors'] > 0:
            top_error_pattern = max(error_analysis['error_patterns'].items(), key=lambda x: x[1])
            improvements.append(f"ä¸»è¦éŒ¯èª¤æ¨¡å¼ç‚º '{top_error_pattern[0]}'ï¼Œå»ºè­°é‡å°æ­¤é¡æ¡ˆä¾‹é€²è¡Œæ•¸æ“šå¢å¼·")

        if len(error_analysis['high_confidence_errors']) > 5:
            improvements.append("å­˜åœ¨è¼ƒå¤šé«˜ç½®ä¿¡åº¦éŒ¯èª¤é æ¸¬ï¼Œå»ºè­°æª¢æŸ¥æ¨¡å‹éåº¦è‡ªä¿¡å•é¡Œ")

        if improvements:
            html_content += f"""
    <div class="improvement-section">
        <h2>ğŸ’¡ æ”¹é€²å»ºè­°</h2>
        <ul>
        """
            for improvement in improvements:
                html_content += f"<li>{improvement}</li>"

            html_content += """
        </ul>
    </div>
            """

        html_content += """
    <div class="metric-card">
        <h2>ğŸ“‹ è©•ä¼°ç¸½çµ</h2>
        <p>æœ¬æ¬¡è©•ä¼°å®Œæˆäº†å° CyberPuppy éœ¸å‡Œåµæ¸¬æ¨¡å‹çš„å…¨é¢æ¸¬è©¦ï¼ŒåŒ…æ‹¬åŸºç¤æŒ‡æ¨™è¨ˆç®—ã€éŒ¯èª¤åˆ†æå’Œè¦–è¦ºåŒ–å±•ç¤ºã€‚</p>
        <p><strong>å»ºè­°ä¸‹ä¸€æ­¥è¡Œå‹•:</strong></p>
        <ul>
            <li>æ ¹æ“šéŒ¯èª¤åˆ†æçµæœå„ªåŒ–è¨“ç·´æ•¸æ“š</li>
            <li>è€ƒæ…®æ¨¡å‹æ¶æ§‹èª¿æ•´ä»¥æå‡æ€§èƒ½</li>
            <li>å¯¦æ–½æ›´åš´æ ¼çš„æ•¸æ“šæ¸…ç†æµç¨‹</li>
            <li>é€²è¡Œæ›´å¤šæ¨£åŒ–çš„æ¸¬è©¦å ´æ™¯é©—è­‰</li>
        </ul>
    </div>

    <footer style="text-align: center; margin-top: 50px; color: #666;">
        <p>CyberPuppy éœ¸å‡Œåµæ¸¬ç³»çµ± - è®“ç¶²è·¯ç’°å¢ƒæ›´å®‰å…¨</p>
    </footer>
</body>
</html>
        """

        # ä¿å­˜ HTML å ±å‘Š
        html_path = os.path.join(self.output_dir, 'evaluation_report.html')
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"HTML å ±å‘Šå·²ä¿å­˜: {html_path}")
        return html_path

    def save_results(self, metrics: Dict[str, Any], error_analysis: Dict[str, Any]):
        """ä¿å­˜è©•ä¼°çµæœ"""
        print("ä¿å­˜è©•ä¼°çµæœ...")

        # ä¿å­˜ JSON æ ¼å¼çµæœ
        results = {
            'evaluation_time': datetime.now().isoformat(),
            'model_path': self.model_path,
            'metrics': metrics,
            'error_analysis': error_analysis,
            'target_achievement': {
                'bullying_f1_target': 0.75,
                'bullying_f1_actual': metrics['per_class_metrics']['toxic']['f1'],
                'bullying_f1_achieved': metrics['per_class_metrics']['toxic']['f1'] >= 0.75,
                'precision_target': 0.70,
                'precision_actual': metrics['per_class_metrics']['toxic']['precision'],
                'precision_achieved': metrics['per_class_metrics']['toxic']['precision'] >= 0.70,
                'recall_target': 0.70,
                'recall_actual': metrics['per_class_metrics']['toxic']['recall'],
                'recall_achieved': metrics['per_class_metrics']['toxic']['recall'] >= 0.70
            }
        }

        json_path = os.path.join(self.output_dir, 'evaluation_results.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"JSON çµæœå·²ä¿å­˜: {json_path}")

        # ä¿å­˜ç°¡åŒ–çš„æŒ‡æ¨™ç¸½çµ
        summary = {
            'overall_accuracy': metrics['accuracy'],
            'overall_f1': metrics['weighted_f1'],
            'bullying_detection_f1': metrics['per_class_metrics']['toxic']['f1'],
            'bullying_detection_precision': metrics['per_class_metrics']['toxic']['precision'],
            'bullying_detection_recall': metrics['per_class_metrics']['toxic']['recall'],
            'total_errors': error_analysis['total_errors'],
            'error_rate': error_analysis['error_rate'],
            'targets_achieved': results['target_achievement']
        }

        summary_path = os.path.join(self.output_dir, 'summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        return results

    def run_evaluation(self, data_path: str):
        """åŸ·è¡Œå®Œæ•´è©•ä¼°"""
        print("="*60)
        print("é–‹å§‹ CyberPuppy æ¨¡å‹è©•ä¼°")
        print("="*60)

        start_time = datetime.now()

        try:
            # 1. è¼‰å…¥æ¨¡å‹
            self.load_model()

            # 2. è¼‰å…¥æ¸¬è©¦æ•¸æ“š
            test_data = self.load_test_data(data_path)

            # 3. é€²è¡Œé æ¸¬
            predictions, confidences, probabilities = self.predict_batch(test_data['texts'])

            # 4. è¨ˆç®—æŒ‡æ¨™
            metrics = self.calculate_metrics(test_data['labels'], predictions, probabilities)

            # 5. éŒ¯èª¤åˆ†æ
            error_analysis = self.analyze_errors(
                test_data['texts'], test_data['labels'], predictions, probabilities
            )

            # 6. å‰µå»ºè¦–è¦ºåŒ–
            self.create_visualizations(metrics, error_analysis)

            # 7. ç”Ÿæˆå ±å‘Š
            evaluation_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            html_report = self.generate_html_report(metrics, error_analysis, evaluation_time)

            # 8. ä¿å­˜çµæœ
            results = self.save_results(metrics, error_analysis)

            # 9. è¼¸å‡ºç¸½çµ
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            print("="*60)
            print("è©•ä¼°å®Œæˆï¼")
            print(f"è©•ä¼°ç”¨æ™‚: {duration:.1f} ç§’")
            print(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(test_data['texts'])}")
            print(f"æ•´é«”æº–ç¢ºç‡: {metrics['accuracy']:.3f}")
            print(f"æ•´é«” F1 åˆ†æ•¸: {metrics['weighted_f1']:.3f}")
            print(f"éœ¸å‡Œåµæ¸¬ F1: {metrics['per_class_metrics']['toxic']['f1']:.3f}")
            print(f"éŒ¯èª¤æ¡ˆä¾‹æ•¸: {error_analysis['total_errors']}")

            # ç›®æ¨™é”æˆæƒ…æ³
            print("\nğŸ¯ ç›®æ¨™é”æˆæƒ…æ³:")
            targets = results['target_achievement']
            print(f"éœ¸å‡Œåµæ¸¬ F1 â‰¥ 0.75: {'âœ“' if targets['bullying_f1_achieved'] else 'âœ—'} ({targets['bullying_f1_actual']:.3f})")
            print(f"Precision â‰¥ 0.70: {'âœ“' if targets['precision_achieved'] else 'âœ—'} ({targets['precision_actual']:.3f})")
            print(f"Recall â‰¥ 0.70: {'âœ“' if targets['recall_achieved'] else 'âœ—'} ({targets['recall_actual']:.3f})")

            print(f"\nğŸ“ çµæœä¿å­˜åœ¨: {self.output_dir}")
            print(f"ğŸ“Š HTML å ±å‘Š: {html_report}")
            print("="*60)

            return results

        except Exception as e:
            print(f"è©•ä¼°éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

def main():
    """ä¸»å‡½æ•¸"""
    import argparse

    parser = argparse.ArgumentParser(description='CyberPuppy æ¨¡å‹è©•ä¼°å·¥å…·')
    parser.add_argument('--model_path', type=str, required=True, help='æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--data_path', type=str, required=True, help='æ¸¬è©¦æ•¸æ“šè·¯å¾‘')
    parser.add_argument('--output_dir', type=str, default='evaluation_results', help='è¼¸å‡ºç›®éŒ„')

    args = parser.parse_args()

    # å‰µå»ºè©•ä¼°å™¨
    evaluator = CyberPuppyModelEvaluator(args.model_path, args.output_dir)

    # é‹è¡Œè©•ä¼°
    evaluator.run_evaluation(args.data_path)

if __name__ == '__main__':
    main()