#!/usr/bin/env python3
"""
å®Œæ•´è³‡æ–™é›†ä¸‹è¼‰èˆ‡ä¿®å¾©è…³æœ¬
æ”¯æ´ ChnSentiCorp (.arrow)ã€DMSC v2 (ZIP)ã€NTUSD (Git) ç­‰è³‡æ–™é›†çš„æ­£ç¢ºä¸‹è¼‰èˆ‡è™•ç†
åŒ…å«é€²åº¦æ¢ã€éŒ¯èª¤è™•ç†ã€æ–·é»çºŒå‚³ã€è©³ç´°æ—¥èªŒç­‰åŠŸèƒ½
"""

import os
import sys
import json
import zipfile
import logging
import hashlib
import time
from pathlib import Path
from typing import Dict, Optional, List, Tuple
import subprocess
import argparse
from datetime import datetime

try:
    import requests
    from tqdm import tqdm
    import pandas as pd
    from datasets import load_dataset, Dataset
    from huggingface_hub import hf_hub_download, list_repo_files
except ImportError as e:
    print(f"ç¼ºå°‘å¿…è¦çš„ä¾è³´å¥—ä»¶: {e}")
    print("è«‹åŸ·è¡Œ: pip install requests tqdm pandas datasets huggingface_hub pyarrow")
    sys.exit(1)

# è¨­å®šæ—¥èªŒ
class SafeStreamHandler(logging.StreamHandler):
    """å®‰å…¨çš„ä¸²æµè™•ç†å™¨ï¼Œé¿å… Unicode ç·¨ç¢¼å•é¡Œ"""
    def emit(self, record):
        try:
            msg = self.format(record)
            # ç§»é™¤ emoji å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåœ¨ Windows å‘½ä»¤åˆ—ä¸­ä½¿ç”¨ç´”æ–‡å­—
            msg = (msg.replace('ğŸ”', '[INFO]')
                     .replace('ğŸ“Š', '[REPORT]')
                     .replace('âœ…', '[OK]')
                     .replace('âŒ', '[FAIL]')
                     .replace('âš ï¸', '[WARN]')
                     .replace('ğŸš«', '[ERROR]')
                     .replace('ğŸ“ˆ', '[SUMMARY]')
                     .replace('ğŸ“', '[SAVE]')
                     .replace('ğŸ“„', '[FILE]')
                     .replace('ğŸ’¾', '[DATA]')
                     .replace('ğŸš€', '[START]')
                     .replace('ğŸ”„', '[PROCESS]')
                     .replace('â±ï¸', '[TIME]')
                     .replace('ğŸ‰', '[SUCCESS]'))
            self.stream.write(msg + self.terminator)
            self.flush()
        except UnicodeEncodeError:
            # å¦‚æœä»ç„¶æœ‰ç·¨ç¢¼å•é¡Œï¼Œä½¿ç”¨ ASCII å®‰å…¨ç‰ˆæœ¬
            safe_msg = record.getMessage().encode('ascii', 'ignore').decode('ascii')
            self.stream.write(f"{record.levelname}: {safe_msg}{self.terminator}")
            self.flush()
        except Exception:
            self.handleError(record)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dataset_setup.log', encoding='utf-8'),
        SafeStreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class DatasetDownloader:
    """å®Œæ•´è³‡æ–™é›†ä¸‹è¼‰å™¨"""

    def __init__(self, base_dir: str = "./data/raw", force_download: bool = False):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.force_download = force_download
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'CyberPuppy-Complete-Dataset-Setup/2.0'
        })

        # è³‡æ–™é›†é…ç½®
        self.datasets_config = {
            'cold': {
                'name': 'COLD Dataset',
                'description': 'ä¸­æ–‡æ”»æ“Šæ€§èªè¨€æª¢æ¸¬è³‡æ–™é›†',
                'method': 'git_clone',
                'url': 'https://github.com/thu-coai/COLDataset.git',
                'expected_files': ['COLDataset/train.csv', 'COLDataset/dev.csv', 'COLDataset/test.csv'],
                'size_estimate': '10MB'
            },
            'chnsenticorp': {
                'name': 'ChnSentiCorp',
                'description': 'ä¸­æ–‡æƒ…æ„Ÿåˆ†æè³‡æ–™é›†',
                'method': 'huggingface_arrow',
                'dataset_id': 'seamew/ChnSentiCorp',
                'expected_files': ['train.arrow', 'validation.arrow', 'test.arrow', 'ChnSentiCorp_htl_all.csv'],
                'size_estimate': '5MB'
            },
            'dmsc': {
                'name': 'DMSC v2',
                'description': 'å¤§çœ¾é»è©•æƒ…æ„Ÿåˆ†æè³‡æ–™é›†',
                'method': 'zip_download',
                'url': 'https://github.com/ownthink/dmsc-v2/releases/download/v2.0/dmsc_v2.zip',
                'expected_files': ['DMSC.csv'],
                'size_estimate': '144MB'
            },
            'ntusd': {
                'name': 'NTUSD',
                'description': 'å°å¤§ä¸­æ–‡æƒ…æ„Ÿè©å…¸',
                'method': 'git_clone',
                'url': 'https://github.com/ntunlplab/NTUSD.git',
                'expected_files': ['data/æ­£é¢è©ç„¡é‡è¤‡_9365è©.txt', 'data/è² é¢è©ç„¡é‡è¤‡_11230è©.txt'],
                'size_estimate': '1MB'
            }
        }

    def _calculate_file_hash(self, file_path: Path) -> str:
        """è¨ˆç®—æª”æ¡ˆ SHA256 hash"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256_hash.update(chunk)
        return sha256_hash.hexdigest()

    def _download_with_progress(self, url: str, output_path: Path,
                               chunk_size: int = 8192) -> bool:
        """å¸¶é€²åº¦æ¢çš„æª”æ¡ˆä¸‹è¼‰ï¼Œæ”¯æ´æ–·é»çºŒå‚³"""
        headers = {}
        initial_pos = 0

        # æª¢æŸ¥æ˜¯å¦æ”¯æ´æ–·é»çºŒå‚³
        if output_path.exists() and not self.force_download:
            initial_pos = output_path.stat().st_size
            headers['Range'] = f'bytes={initial_pos}-'
            logger.info(f"æª¢æ¸¬åˆ°éƒ¨åˆ†ä¸‹è¼‰æª”æ¡ˆï¼Œå¾ {initial_pos} bytes ç¹¼çºŒ")

        try:
            response = self.session.get(url, headers=headers, stream=True, timeout=30)

            # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
            if response.status_code not in [200, 206]:
                if response.status_code == 416:  # Range Not Satisfiable
                    logger.info("æª”æ¡ˆå·²å®Œå…¨ä¸‹è¼‰")
                    return True
                response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))
            if initial_pos > 0:
                total_size += initial_pos

            mode = 'ab' if initial_pos > 0 else 'wb'

            with open(output_path, mode) as f:
                with tqdm(
                    total=total_size,
                    initial=initial_pos,
                    unit='B',
                    unit_scale=True,
                    desc=output_path.name
                ) as pbar:
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))

            logger.info(f"âœ“ ä¸‹è¼‰å®Œæˆ: {output_path}")
            return True

        except Exception as e:
            logger.error(f"âœ— ä¸‹è¼‰å¤±æ•—: {e}")
            if output_path.exists():
                output_path.unlink()  # åˆªé™¤ä¸å®Œæ•´çš„æª”æ¡ˆ
            return False

    def download_huggingface_arrow(self, dataset_id: str, dest_dir: Path) -> bool:
        """ä¸‹è¼‰ Hugging Face è³‡æ–™é›†çš„ .arrow æª”æ¡ˆ"""
        logger.info(f"="*60)
        logger.info(f"ä¸‹è¼‰ Hugging Face è³‡æ–™é›†: {dataset_id}")
        logger.info(f"="*60)

        dest_dir.mkdir(parents=True, exist_ok=True)

        try:
            # æ–¹æ³• 1: å˜—è©¦ç›´æ¥å¾ Hub ä¸‹è¼‰ parquet æª”æ¡ˆ
            logger.info("å˜—è©¦å¾ Hugging Face Hub ç›´æ¥ä¸‹è¼‰...")

            success_count = 0
            try:
                # åˆ—å‡ºå„²å­˜åº«ä¸­çš„æ‰€æœ‰æª”æ¡ˆ
                repo_files = list_repo_files(dataset_id, repo_type="dataset")

                # å„ªå…ˆä¸‹è¼‰ arrow æª”æ¡ˆ
                arrow_files = [f for f in repo_files if f.endswith('.arrow')]
                parquet_files = [f for f in repo_files if f.endswith('.parquet')]

                if arrow_files:
                    logger.info(f"æ‰¾åˆ° {len(arrow_files)} å€‹ arrow æª”æ¡ˆ")
                    for arrow_file in arrow_files:
                        # è§£ææª”æ¡ˆåç¨±æ ¼å¼: chn_senti_corp-train.arrow -> train.arrow
                        original_name = Path(arrow_file).name
                        if '-' in original_name:
                            split_name = original_name.split('-')[-1]  # å–æœ€å¾Œä¸€éƒ¨åˆ†
                        else:
                            split_name = original_name

                        local_path = dest_dir / split_name

                        if local_path.exists() and not self.force_download:
                            logger.info(f"âœ“ {split_name} å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")
                            success_count += 1
                            continue

                        logger.info(f"ä¸‹è¼‰ {arrow_file} -> {split_name}...")
                        downloaded_path = hf_hub_download(
                            repo_id=dataset_id,
                            filename=arrow_file,
                            local_dir=str(dest_dir),
                            repo_type="dataset"
                        )

                        # é‡æ–°å‘½åç‚ºæ¨™æº–æ ¼å¼
                        Path(downloaded_path).rename(local_path)

                        success_count += 1
                        logger.info(f"âœ“ {split_name} ä¸‹è¼‰å®Œæˆ")

                elif parquet_files:
                    logger.info(f"æ‰¾åˆ° {len(parquet_files)} å€‹ parquet æª”æ¡ˆ")
                    for parquet_file in parquet_files:
                        local_path = dest_dir / Path(parquet_file).name
                        if local_path.exists() and not self.force_download:
                            logger.info(f"âœ“ {local_path.name} å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")
                            success_count += 1
                            continue

                        logger.info(f"ä¸‹è¼‰ {parquet_file}...")
                        hf_hub_download(
                            repo_id=dataset_id,
                            filename=parquet_file,
                            local_dir=str(dest_dir),
                            repo_type="dataset"
                        )
                        success_count += 1
                        logger.info(f"âœ“ {local_path.name} ä¸‹è¼‰å®Œæˆ")

                        # å˜—è©¦è½‰æ› parquet åˆ° arrow
                        try:
                            import pyarrow.parquet as pq
                            import pyarrow as pa

                            split_name = Path(parquet_file).stem
                            arrow_file = dest_dir / f"{split_name}.arrow"

                            if not arrow_file.exists() or self.force_download:
                                table = pq.read_table(local_path)
                                with pa.OSFile(str(arrow_file), 'wb') as sink:
                                    with pa.RecordBatchFileWriter(sink, table.schema) as writer:
                                        writer.write_table(table)
                                logger.info(f"âœ“ è½‰æ› {split_name}.parquet åˆ° {split_name}.arrow")
                        except ImportError:
                            logger.warning("pyarrow æœªå®‰è£ï¼Œç„¡æ³•è½‰æ› parquet åˆ° arrow")
                        except Exception as e:
                            logger.warning(f"è½‰æ› parquet åˆ° arrow å¤±æ•—: {e}")

                else:
                    logger.warning("æœªæ‰¾åˆ° arrow æˆ– parquet æª”æ¡ˆ")

            except Exception as e:
                logger.error(f"Hub æª”æ¡ˆåˆ—è¡¨å¤±æ•—: {e}")

            # æ–¹æ³• 2: å¦‚æœ Hub ä¸‹è¼‰å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ datasets åº«ï¼ˆç„¡è…³æœ¬æ¨¡å¼ï¼‰
            if success_count == 0:
                logger.info("å˜—è©¦ä½¿ç”¨ datasets åº«ä¸‹è¼‰...")

                try:
                    dataset = load_dataset(dataset_id, trust_remote_code=False)

                    for split_name, split_dataset in dataset.items():
                        arrow_file = dest_dir / f"{split_name}.arrow"

                        if arrow_file.exists() and not self.force_download:
                            logger.info(f"âœ“ {split_name}.arrow å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")
                            success_count += 1
                            continue

                        logger.info(f"ä¿å­˜ {split_name} split åˆ° .arrow æ ¼å¼...")

                        # ç›´æ¥å„²å­˜ç‚º arrow æ ¼å¼
                        split_dataset.save_to_disk(str(dest_dir / split_name))

                        # æª¢æŸ¥ä¸¦ç§»å‹• .arrow æª”æ¡ˆ
                        arrow_files = list((dest_dir / split_name).glob("*.arrow"))
                        if arrow_files:
                            main_arrow = arrow_files[0]
                            main_arrow.rename(arrow_file)

                            # æ¸…ç†è‡¨æ™‚ç›®éŒ„
                            import shutil
                            shutil.rmtree(dest_dir / split_name)

                            logger.info(f"âœ“ {split_name}.arrow å„²å­˜å®Œæˆ ({len(split_dataset)} ç­†è³‡æ–™)")
                            success_count += 1
                        else:
                            # å¾Œå‚™æ–¹æ¡ˆï¼šå„²å­˜ç‚º parquet å’Œ JSON
                            parquet_file = dest_dir / f"{split_name}.parquet"
                            json_file = dest_dir / f"{split_name}.json"

                            split_dataset.to_parquet(str(parquet_file))
                            split_dataset.to_json(str(json_file))

                            logger.info(f"âœ“ {split_name} å„²å­˜ç‚º parquet å’Œ JSON")
                            success_count += 1

                except Exception as e:
                    logger.error(f"datasets åº«ä¸‹è¼‰å¤±æ•—: {e}")

            # å„²å­˜å…ƒè³‡æ–™
            if success_count > 0:
                metadata = {
                    "dataset_id": dataset_id,
                    "download_method": "huggingface_hub",
                    "download_time": datetime.now().isoformat(),
                    "success_count": success_count,
                    "format": "parquet+arrow",
                }

                with open(dest_dir / "metadata.json", 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)

            return success_count > 0

        except Exception as e:
            logger.error(f"âœ— Hugging Face ä¸‹è¼‰å¤±æ•—: {e}")
            return False

    def download_zip_dataset(self, url: str, dest_dir: Path) -> bool:
        """ä¸‹è¼‰ä¸¦è§£å£“ç¸® ZIP è³‡æ–™é›†"""
        logger.info(f"="*60)
        logger.info(f"ä¸‹è¼‰ ZIP è³‡æ–™é›†: {url}")
        logger.info(f"="*60)

        dest_dir.mkdir(parents=True, exist_ok=True)
        zip_filename = Path(url).name
        zip_path = dest_dir / zip_filename

        # ä¸‹è¼‰ ZIP æª”æ¡ˆ
        if not zip_path.exists() or self.force_download:
            logger.info(f"ä¸‹è¼‰ {zip_filename}...")
            if not self._download_with_progress(url, zip_path):
                return False
        else:
            logger.info(f"âœ“ {zip_filename} å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")

        # é©—è­‰ ZIP æª”æ¡ˆ
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_info = zip_ref.infolist()
                logger.info(f"ZIP æª”æ¡ˆåŒ…å« {len(zip_info)} å€‹æª”æ¡ˆ")

            # æª¢æŸ¥æ˜¯å¦å·²è§£å£“ç¸®
            csv_files = list(dest_dir.glob("**/*.csv"))
            if csv_files and not self.force_download:
                logger.info(f"âœ“ æª¢æ¸¬åˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆï¼Œè·³éè§£å£“ç¸®")
                return True

            # è§£å£“ç¸®
            logger.info("è§£å£“ç¸®ä¸­...")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                members = zip_ref.namelist()

                with tqdm(total=len(members), desc="è§£å£“ç¸®") as pbar:
                    for member in members:
                        zip_ref.extract(member, dest_dir)
                        pbar.update(1)

            logger.info("âœ“ è§£å£“ç¸®å®Œæˆ")

            # é©—è­‰è§£å£“ç¸®çµæœ
            extracted_files = list(dest_dir.glob("**/*.csv"))
            logger.info(f"è§£å£“ç¸®å¾Œæ‰¾åˆ° {len(extracted_files)} å€‹ CSV æª”æ¡ˆ")

            # é¡¯ç¤ºä¸»è¦æª”æ¡ˆè³‡è¨Š
            for csv_file in extracted_files[:5]:  # é¡¯ç¤ºå‰ 5 å€‹
                size_mb = csv_file.stat().st_size / (1024 * 1024)
                logger.info(f"  - {csv_file.name}: {size_mb:.2f} MB")

            # å„²å­˜å…ƒè³‡æ–™
            metadata = {
                "source_url": url,
                "download_method": "zip_download",
                "download_time": datetime.now().isoformat(),
                "zip_file": zip_filename,
                "extracted_files": [f.name for f in extracted_files],
                "total_size_mb": sum(f.stat().st_size for f in extracted_files) / (1024 * 1024)
            }

            with open(dest_dir / "metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            return len(extracted_files) > 0

        except Exception as e:
            logger.error(f"âœ— ZIP è™•ç†å¤±æ•—: {e}")
            return False

    def clone_git_repository(self, url: str, dest_dir: Path, depth: int = 1) -> bool:
        """å…‹éš† Git å„²å­˜åº«"""
        logger.info(f"="*60)
        logger.info(f"å…‹éš† Git å„²å­˜åº«: {url}")
        logger.info(f"="*60)

        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if dest_dir.exists() and not self.force_download:
            if (dest_dir / ".git").exists():
                logger.info("æª¢æ¸¬åˆ°ç¾æœ‰ Git å„²å­˜åº«ï¼Œå˜—è©¦æ›´æ–°...")
                try:
                    subprocess.run(["git", "pull"], cwd=dest_dir, check=True,
                                 capture_output=True, text=True)
                    logger.info("âœ“ Git å„²å­˜åº«æ›´æ–°å®Œæˆ")
                    return True
                except subprocess.CalledProcessError:
                    logger.warning("Git æ›´æ–°å¤±æ•—ï¼Œå°‡é‡æ–°å…‹éš†...")
                    import shutil
                    shutil.rmtree(dest_dir)
            else:
                logger.info("æª¢æ¸¬åˆ°ç¾æœ‰ç›®éŒ„ä½†é Git å„²å­˜åº«ï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ä¸‹è¼‰...")
                # é€™è£¡å¯ä»¥æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å®Œæ•´ï¼Œå¦‚æœä¸å®Œæ•´å°±é‡æ–°ä¸‹è¼‰
                return True  # æš«æ™‚è·³éé‡æ–°ä¸‹è¼‰

        # å¦‚æœæ˜¯å¼·åˆ¶ä¸‹è¼‰æ¨¡å¼ï¼Œåˆªé™¤ç¾æœ‰ç›®éŒ„
        elif dest_dir.exists() and self.force_download:
            logger.info(f"å¼·åˆ¶æ¨¡å¼ï¼šåˆªé™¤ç¾æœ‰ç›®éŒ„ {dest_dir}")
            import shutil
            shutil.rmtree(dest_dir)

        try:
            # å…‹éš†å„²å­˜åº«
            logger.info(f"å…‹éš†åˆ°: {dest_dir}")

            cmd = ["git", "clone"]
            if depth > 0:
                cmd.extend(["--depth", str(depth)])
            cmd.extend([url, str(dest_dir)])

            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            logger.info("âœ“ Git å…‹éš†å®Œæˆ")

            # é¡¯ç¤ºå„²å­˜åº«è³‡è¨Š
            try:
                # ç²å–æœ€æ–°æäº¤è³‡è¨Š
                commit_result = subprocess.run(
                    ["git", "log", "-1", "--oneline"],
                    cwd=dest_dir,
                    capture_output=True,
                    text=True,
                    check=True
                )
                logger.info(f"æœ€æ–°æäº¤: {commit_result.stdout.strip()}")

                # è¨ˆç®—æª”æ¡ˆçµ±è¨ˆ
                file_count = len(list(dest_dir.rglob("*")))
                total_size = sum(f.stat().st_size for f in dest_dir.rglob("*") if f.is_file())
                logger.info(f"æª”æ¡ˆæ•¸é‡: {file_count}, ç¸½å¤§å°: {total_size / (1024 * 1024):.2f} MB")

            except subprocess.CalledProcessError:
                pass  # ä¸æ˜¯é—œéµéŒ¯èª¤

            # å„²å­˜å…ƒè³‡æ–™
            metadata = {
                "repository_url": url,
                "download_method": "git_clone",
                "download_time": datetime.now().isoformat(),
                "clone_depth": depth,
                "file_count": file_count if 'file_count' in locals() else 0,
                "total_size_mb": total_size / (1024 * 1024) if 'total_size' in locals() else 0
            }

            with open(dest_dir / "metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            return True

        except subprocess.CalledProcessError as e:
            logger.error(f"âœ— Git å…‹éš†å¤±æ•—: {e}")
            if e.stderr:
                logger.error(f"éŒ¯èª¤è©³æƒ…: {e.stderr}")
            return False
        except Exception as e:
            logger.error(f"âœ— Git æ“ä½œå¤±æ•—: {e}")
            return False

    def verify_dataset(self, dataset_name: str) -> Tuple[bool, Dict]:
        """é©—è­‰å–®å€‹è³‡æ–™é›†"""
        config = self.datasets_config.get(dataset_name)
        if not config:
            return False, {"error": f"æœªçŸ¥è³‡æ–™é›†: {dataset_name}"}

        dest_dir = self.base_dir / dataset_name
        if not dest_dir.exists():
            return False, {"error": "è³‡æ–™é›†ç›®éŒ„ä¸å­˜åœ¨"}

        result = {
            "name": config["name"],
            "description": config["description"],
            "path": str(dest_dir),
            "files": [],
            "total_size_mb": 0,
            "status": "unknown"
        }

        try:
            # æª¢æŸ¥é æœŸæª”æ¡ˆ
            found_files = []
            missing_files = []
            total_size = 0

            for expected_file in config["expected_files"]:
                file_patterns = [
                    dest_dir / expected_file,
                ]

                # å¦‚æœåŒ…å«è·¯å¾‘åˆ†éš”ç¬¦ï¼Œå‰‡ä½¿ç”¨éæ­¸æœç´¢
                if '/' in expected_file:
                    file_patterns.append(dest_dir / expected_file)
                else:
                    # å¦å‰‡åœ¨æ‰€æœ‰å­ç›®éŒ„ä¸­æœç´¢
                    file_patterns.extend(list(dest_dir.rglob(expected_file)))

                found = False
                for pattern in file_patterns:
                    if isinstance(pattern, Path) and pattern.exists():
                        file_path = pattern
                        size = file_path.stat().st_size
                        found_files.append({
                            "name": expected_file,
                            "path": str(file_path.relative_to(dest_dir)),
                            "size_mb": size / (1024 * 1024),
                            "exists": True
                        })
                        total_size += size
                        found = True
                        break
                    elif not isinstance(pattern, Path):
                        # è™•ç† glob çµæœ
                        if pattern.exists():
                            file_path = pattern
                            size = file_path.stat().st_size
                            found_files.append({
                                "name": expected_file,
                                "path": str(file_path.relative_to(dest_dir)),
                                "size_mb": size / (1024 * 1024),
                                "exists": True
                            })
                            total_size += size
                            found = True
                            break

                if not found:
                    missing_files.append(expected_file)

            result["files"] = found_files
            result["missing_files"] = missing_files
            result["total_size_mb"] = total_size / (1024 * 1024)

            # åˆ¤æ–·ç‹€æ…‹
            if not missing_files:
                result["status"] = "complete"
            elif found_files:
                result["status"] = "partial"
            else:
                result["status"] = "missing"

            # è®€å–å…ƒè³‡æ–™
            metadata_file = dest_dir / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    result["metadata"] = json.load(f)

            return result["status"] in ["complete", "partial"], result

        except Exception as e:
            result["error"] = str(e)
            result["status"] = "error"
            return False, result

    def download_dataset(self, dataset_name: str) -> bool:
        """ä¸‹è¼‰å–®å€‹è³‡æ–™é›†"""
        config = self.datasets_config.get(dataset_name)
        if not config:
            logger.error(f"æœªçŸ¥è³‡æ–™é›†: {dataset_name}")
            return False

        logger.info(f"\nğŸš€ é–‹å§‹ä¸‹è¼‰: {config['name']}")
        logger.info(f"æè¿°: {config['description']}")
        logger.info(f"é ä¼°å¤§å°: {config['size_estimate']}")

        dest_dir = self.base_dir / dataset_name
        method = config["method"]

        try:
            if method == "huggingface_arrow":
                return self.download_huggingface_arrow(config["dataset_id"], dest_dir)
            elif method == "zip_download":
                return self.download_zip_dataset(config["url"], dest_dir)
            elif method == "git_clone":
                return self.clone_git_repository(config["url"], dest_dir)
            else:
                logger.error(f"ä¸æ”¯æ´çš„ä¸‹è¼‰æ–¹æ³•: {method}")
                return False

        except Exception as e:
            logger.error(f"âœ— ä¸‹è¼‰ {dataset_name} å¤±æ•—: {e}")
            return False

    def download_all_datasets(self, datasets: Optional[List[str]] = None) -> Dict[str, bool]:
        """ä¸‹è¼‰æ‰€æœ‰æˆ–æŒ‡å®šçš„è³‡æ–™é›†"""
        if datasets is None:
            datasets = list(self.datasets_config.keys())

        results = {}

        logger.info(f"\nğŸ”„ é–‹å§‹ä¸‹è¼‰ {len(datasets)} å€‹è³‡æ–™é›†...")
        start_time = time.time()

        for i, dataset_name in enumerate(datasets, 1):
            logger.info(f"\n[{i}/{len(datasets)}] è™•ç†è³‡æ–™é›†: {dataset_name}")
            results[dataset_name] = self.download_dataset(dataset_name)

        elapsed_time = time.time() - start_time
        logger.info(f"\nâ±ï¸ ç¸½ä¸‹è¼‰æ™‚é–“: {elapsed_time:.2f} ç§’")

        return results

    def generate_final_report(self, download_results: Optional[Dict[str, bool]] = None) -> Dict:
        """ç”Ÿæˆæœ€çµ‚é©—è­‰å ±å‘Š"""
        logger.info(f"\n" + "="*80)
        logger.info("ğŸ“Š æœ€çµ‚é©—è­‰å ±å‘Š")
        logger.info("="*80)

        report = {
            "timestamp": datetime.now().isoformat(),
            "base_directory": str(self.base_dir),
            "datasets": {},
            "summary": {
                "total": len(self.datasets_config),
                "complete": 0,
                "partial": 0,
                "missing": 0,
                "error": 0
            }
        }

        for dataset_name in self.datasets_config.keys():
            success, details = self.verify_dataset(dataset_name)
            report["datasets"][dataset_name] = details

            status = details.get("status", "error")
            if status in report["summary"]:
                report["summary"][status] += 1

            # é¡¯ç¤ºè³‡æ–™é›†ç‹€æ…‹
            status_emoji = {
                "complete": "[OK]",
                "partial": "[WARN]",
                "missing": "[FAIL]",
                "error": "[ERROR]"
            }

            emoji = status_emoji.get(status, "[UNKNOWN]")
            size_info = f"({details.get('total_size_mb', 0):.2f} MB)" if details.get('total_size_mb') else ""

            logger.info(f"{emoji} {details.get('name', dataset_name)}: {status.upper()} {size_info}")

            if details.get('missing_files'):
                logger.info(f"   ç¼ºå°‘æª”æ¡ˆ: {', '.join(details['missing_files'])}")

            if download_results and dataset_name in download_results:
                download_status = "æˆåŠŸ" if download_results[dataset_name] else "å¤±æ•—"
                logger.info(f"   ä¸‹è¼‰ç‹€æ…‹: {download_status}")

        # é¡¯ç¤ºç¸½çµ
        logger.info(f"\nğŸ“ˆ ç¸½çµ:")
        logger.info(f"   å®Œæ•´: {report['summary']['complete']}/{report['summary']['total']}")
        logger.info(f"   éƒ¨åˆ†: {report['summary']['partial']}/{report['summary']['total']}")
        logger.info(f"   ç¼ºå¤±: {report['summary']['missing']}/{report['summary']['total']}")
        logger.info(f"   éŒ¯èª¤: {report['summary']['error']}/{report['summary']['total']}")

        # è¨ˆç®—ç¸½å¤§å°
        total_size_mb = sum(
            details.get('total_size_mb', 0)
            for details in report["datasets"].values()
        )
        logger.info(f"   ç¸½å¤§å°: {total_size_mb:.2f} MB")

        # å„²å­˜å ±å‘Š
        report_file = self.base_dir / "dataset_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"\nğŸ“ è©³ç´°å ±å‘Šå·²å„²å­˜è‡³: {report_file}")
        logger.info("="*80)

        return report


def main():
    parser = argparse.ArgumentParser(
        description="å®Œæ•´è³‡æ–™é›†ä¸‹è¼‰èˆ‡ä¿®å¾©è…³æœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ç”¨æ³•:
  python complete_dataset_setup.py --dataset all           # ä¸‹è¼‰æ‰€æœ‰è³‡æ–™é›†
  python complete_dataset_setup.py --dataset chnsenticorp # åªä¸‹è¼‰ ChnSentiCorp
  python complete_dataset_setup.py --verify-only          # åªé©—è­‰ï¼Œä¸ä¸‹è¼‰
  python complete_dataset_setup.py --force                # å¼·åˆ¶é‡æ–°ä¸‹è¼‰
        """
    )

    parser.add_argument(
        "--dataset",
        nargs='+',
        choices=["cold", "chnsenticorp", "dmsc", "ntusd", "all"],
        default=["all"],
        help="è¦ä¸‹è¼‰çš„è³‡æ–™é›† (å¯é¸æ“‡å¤šå€‹)"
    )

    parser.add_argument(
        "--output-dir",
        default="./data/raw",
        help="è³‡æ–™é›†è¼¸å‡ºç›®éŒ„ (é è¨­: ./data/raw)"
    )

    parser.add_argument(
        "--verify-only",
        action="store_true",
        help="åªé©—è­‰ç¾æœ‰è³‡æ–™é›†ï¼Œä¸åŸ·è¡Œä¸‹è¼‰"
    )

    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼·åˆ¶é‡æ–°ä¸‹è¼‰ï¼Œå³ä½¿æª”æ¡ˆå·²å­˜åœ¨"
    )

    parser.add_argument(
        "--report-only",
        action="store_true",
        help="åªç”Ÿæˆé©—è­‰å ±å‘Š"
    )

    args = parser.parse_args()

    # å±•é–‹ "all" é¸é …
    if "all" in args.dataset:
        datasets = ["cold", "chnsenticorp", "dmsc", "ntusd"]
    else:
        datasets = args.dataset

    # å»ºç«‹ä¸‹è¼‰å™¨
    downloader = DatasetDownloader(
        base_dir=args.output_dir,
        force_download=args.force
    )

    # åŸ·è¡Œæ“ä½œ
    download_results = None

    if args.report_only or args.verify_only:
        logger.info("ğŸ” åªåŸ·è¡Œé©—è­‰ï¼Œä¸ä¸‹è¼‰è³‡æ–™é›†")
    else:
        # åŸ·è¡Œä¸‹è¼‰
        download_results = downloader.download_all_datasets(datasets)

    # ç”Ÿæˆæœ€çµ‚å ±å‘Š
    final_report = downloader.generate_final_report(download_results)

    # çµæœç¸½çµ
    if download_results:
        success_count = sum(1 for success in download_results.values() if success)
        total_count = len(download_results)

        if success_count == total_count:
            logger.info("\nğŸ‰ æ‰€æœ‰è³‡æ–™é›†ä¸‹è¼‰å®Œæˆï¼")
            return 0
        else:
            logger.warning(f"\nâš ï¸ {success_count}/{total_count} å€‹è³‡æ–™é›†ä¸‹è¼‰æˆåŠŸ")
            return 1
    else:
        # åªé©—è­‰æ¨¡å¼
        complete_count = final_report["summary"]["complete"]
        total_count = final_report["summary"]["total"]

        if complete_count == total_count:
            logger.info("\nâœ… æ‰€æœ‰è³‡æ–™é›†é©—è­‰é€šéï¼")
            return 0
        else:
            logger.info(f"\nğŸ“Š {complete_count}/{total_count} å€‹è³‡æ–™é›†å®Œæ•´")
            return 0


if __name__ == "__main__":
    sys.exit(main())