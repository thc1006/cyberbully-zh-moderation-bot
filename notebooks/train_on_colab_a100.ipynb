{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ CyberPuppy éœ¸å‡Œåµæ¸¬è¨“ç·´ - A100 å„ªåŒ–ç‰ˆ\n",
    "\n",
    "**GPU**: A100 (40GB)  \n",
    "**ç›®æ¨™**: F1 â‰¥ 0.75  \n",
    "**å„ªåŒ–**: å¤§ batch + bf16 + å¿«é€Ÿè¨“ç·´  \n",
    "\n",
    "**é è¨ˆæ™‚é–“**: 1-2 å°æ™‚ï¼ˆA100 åŠ é€Ÿï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ GPU é©—è­‰ - ç¢ºèª A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    compute_capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"Compute Capability: {compute_capability}\")\n",
    "    print(f\"BF16 Support: {compute_capability[0] >= 8}\")\n",
    "    \n",
    "    if \"A100\" not in gpu_name:\n",
    "        print(f\"\\nâš ï¸ è­¦å‘Š: ç•¶å‰ GPU æ˜¯ {gpu_name}ï¼Œä¸æ˜¯ A100\")\n",
    "        print(\"æ­¤ notebook é‡å° A100 å„ªåŒ–ï¼Œå…¶ä»– GPU å¯èƒ½éœ€è¦èª¿æ•´ batch size\")\n",
    "else:\n",
    "    print(\"âŒ æœªæª¢æ¸¬åˆ° GPUï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ GitHub èªè­‰è¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# å¾ Colab Secrets æˆ–æ‰‹å‹•è¼¸å…¥\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "    print(\"âœ… å¾ Colab Secrets è¼‰å…¥ token\")\n",
    "except:\n",
    "    GITHUB_TOKEN = getpass(\"GitHub Token: \")\n",
    "\n",
    "GITHUB_USERNAME = \"thc1006\"\n",
    "REPO_NAME = \"cyberbully-zh-moderation-bot\"\n",
    "\n",
    "!git config --global user.email \"colab-a100@example.com\"\n",
    "!git config --global user.name \"Colab A100 Training\"\n",
    "\n",
    "print(\"âœ… Git é…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Clone Repository + æ‹‰å– LFS è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# æ¸…ç†èˆŠç›®éŒ„\n",
    "if os.path.exists(REPO_NAME):\n",
    "    shutil.rmtree(REPO_NAME, ignore_errors=True)\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "\n",
    "# Clone\n",
    "print(\"ğŸ“¥ Cloning repository...\")\n",
    "repo_url = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone {repo_url}\n",
    "\n",
    "os.chdir(REPO_NAME)\n",
    "print(f\"ğŸ“ ç›®éŒ„: {os.getcwd()}\")\n",
    "\n",
    "# æ‹‰å– LFS è³‡æ–™\n",
    "print(\"\\nğŸ“¦ æ‹‰å– Git LFS è³‡æ–™...\")\n",
    "!git lfs install\n",
    "!git lfs pull\n",
    "\n",
    "# é©—è­‰è¨“ç·´è³‡æ–™\n",
    "train_file = \"data/processed/training_dataset/train.json\"\n",
    "file_size = os.path.getsize(train_file)\n",
    "print(f\"\\n{'âœ…' if file_size > 1000000 else 'âŒ'} è¨“ç·´è³‡æ–™: {file_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ å®‰è£ä¾è³´ - CUDA 12.x + PyTorch 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¦ å®‰è£å¥—ä»¶ï¼ˆA100 å„ªåŒ–ç‰ˆæœ¬ï¼‰...\")\n",
    "\n",
    "# PyTorch 2.8 + CUDA 12.6\n",
    "!pip install -q --index-url https://download.pytorch.org/whl/cu126 \\\n",
    "  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n",
    "\n",
    "# NumPy 2.x\n",
    "!pip install -q \"numpy>=2,<2.3\"\n",
    "\n",
    "# Transformers + Accelerate\n",
    "!pip install -q transformers==4.46.3 accelerate==1.2.1 datasets==3.2.0\n",
    "!pip install -q scikit-learn==1.6.1 tqdm==4.67.1 pandas==2.2.2\n",
    "\n",
    "# TensorBoard\n",
    "!pip install -q \"tensorboard~=2.19.0\"\n",
    "\n",
    "# é©—è­‰å®‰è£\n",
    "import torch\n",
    "import numpy as np\n",
    "print(f\"\\nâœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… NumPy: {np.__version__}\")\n",
    "print(f\"âœ… CUDA: {torch.version.cuda}\")\n",
    "print(f\"âœ… BF16 å¯ç”¨: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ é©—è­‰è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_dir = \"data/processed/training_dataset\"\n",
    "\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    filepath = os.path.join(data_dir, f\"{split}.json\")\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    \n",
    "    if file_size > 100000:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"âœ… {split}.json: {len(data):,} æ¨£æœ¬ ({file_size / 1024 / 1024:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"âŒ {split}.json æ˜¯ LFS pointer ({file_size} bytes)\")\n",
    "\n",
    "print(\"\\nâœ… è³‡æ–™é›†å°±ç·’ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ A100 å„ªåŒ–è¨“ç·´é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class A100Config:\n",
    "    name: str\n",
    "    base_model: str\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    early_stopping_patience: int\n",
    "    bullying_weight: float\n",
    "    accumulation_steps: int = 1\n",
    "\n",
    "# A100 å„ªåŒ–é…ç½®ï¼šæ›´å¤§ batch + æ›´å¿«è¨“ç·´\n",
    "configs = [\n",
    "    A100Config(\n",
    "        name=\"macbert_a100_optimized\",\n",
    "        base_model=\"hfl/chinese-macbert-base\",\n",
    "        learning_rate=2e-5,\n",
    "        batch_size=32,\n",
    "        num_epochs=15,\n",
    "        early_stopping_patience=4,\n",
    "        bullying_weight=2.5,\n",
    "        accumulation_steps=2,  # ç­‰æ•ˆ batch=64\n",
    "    ),\n",
    "    A100Config(\n",
    "        name=\"roberta_a100_aggressive\",\n",
    "        base_model=\"hfl/chinese-roberta-wwm-ext\",\n",
    "        learning_rate=3e-5,\n",
    "        batch_size=24,\n",
    "        num_epochs=18,\n",
    "        early_stopping_patience=3,\n",
    "        bullying_weight=2.8,\n",
    "        accumulation_steps=3,  # ç­‰æ•ˆ batch=72\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ A100 è¨“ç·´é…ç½®:\")\n",
    "for i, cfg in enumerate(configs, 1):\n",
    "    effective_batch = cfg.batch_size * cfg.accumulation_steps\n",
    "    print(f\"{i}. {cfg.name}\")\n",
    "    print(f\"   - Batch: {cfg.batch_size} Ã— {cfg.accumulation_steps} = {effective_batch}\")\n",
    "    print(f\"   - LR: {cfg.learning_rate}\")\n",
    "    print(f\"   - BF16: Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ è¨“ç·´ Model A - MacBERT (A100 å„ªåŒ–)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(f\"/content/{REPO_NAME}\")\n",
    "\n",
    "config = configs[0]\n",
    "print(f\"ğŸš€ è¨“ç·´: {config.name}\")\n",
    "print(f\"ğŸ’¾ ç­‰æ•ˆ Batch Size: {config.batch_size * config.accumulation_steps}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!python scripts/train_simple_with_args.py \\\n",
    "  --model_name {config.base_model} \\\n",
    "  --output_dir models/experiments/{config.name} \\\n",
    "  --train_file data/processed/training_dataset/train.json \\\n",
    "  --dev_file data/processed/training_dataset/dev.json \\\n",
    "  --test_file data/processed/training_dataset/test.json \\\n",
    "  --learning_rate {config.learning_rate} \\\n",
    "  --batch_size {config.batch_size} \\\n",
    "  --num_epochs {config.num_epochs} \\\n",
    "  --early_stopping_patience {config.early_stopping_patience} \\\n",
    "  --bullying_weight {config.bullying_weight} \\\n",
    "  --accumulation_steps {config.accumulation_steps} \\\n",
    "  --bf16\n",
    "\n",
    "print(f\"\\nâœ… {config.name} è¨“ç·´å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ è¨“ç·´ Model B - RoBERTa (æ¿€é€²é…ç½®)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configs[1]\n",
    "print(f\"ğŸš€ è¨“ç·´: {config.name}\")\n",
    "print(f\"ğŸ’¾ ç­‰æ•ˆ Batch Size: {config.batch_size * config.accumulation_steps}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "!python scripts/train_simple_with_args.py \\\n",
    "  --model_name {config.base_model} \\\n",
    "  --output_dir models/experiments/{config.name} \\\n",
    "  --train_file data/processed/training_dataset/train.json \\\n",
    "  --dev_file data/processed/training_dataset/dev.json \\\n",
    "  --test_file data/processed/training_dataset/test.json \\\n",
    "  --learning_rate {config.learning_rate} \\\n",
    "  --batch_size {config.batch_size} \\\n",
    "  --num_epochs {config.num_epochs} \\\n",
    "  --early_stopping_patience {config.early_stopping_patience} \\\n",
    "  --bullying_weight {config.bullying_weight} \\\n",
    "  --accumulation_steps {config.accumulation_steps} \\\n",
    "  --bf16\n",
    "\n",
    "print(f\"\\nâœ… {config.name} è¨“ç·´å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ è©•ä¼°æ‰€æœ‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nimport glob\n\nresults = []\n\nprint(\"ğŸ” è©•ä¼°è¨“ç·´çµæœ...\\n\")\n\nfor config in configs:\n    model_dir = f\"models/experiments/{config.name}\"\n    \n    print(f\"æª¢æŸ¥: {config.name}\")\n    print(f\"  è·¯å¾‘: {model_dir}\")\n    \n    if not os.path.exists(model_dir):\n        print(f\"  âŒ ç›®éŒ„ä¸å­˜åœ¨\\n\")\n        continue\n    \n    # åˆ—å‡ºç›®éŒ„å…§æ‰€æœ‰æª”æ¡ˆ\n    files_in_dir = os.listdir(model_dir)\n    print(f\"  ğŸ“ ç›®éŒ„å…§å®¹: {files_in_dir}\")\n    \n    # å°‹æ‰¾è©•ä¼°çµæœ\n    eval_file = os.path.join(model_dir, \"eval_results.json\")\n    final_file = os.path.join(model_dir, \"final_results.json\")\n    \n    print(f\"  å°‹æ‰¾: eval_results.json - {'âœ…å­˜åœ¨' if os.path.exists(eval_file) else 'âŒä¸å­˜åœ¨'}\")\n    print(f\"  å°‹æ‰¾: final_results.json - {'âœ…å­˜åœ¨' if os.path.exists(final_file) else 'âŒä¸å­˜åœ¨'}\")\n    \n    metrics = None\n    if os.path.exists(final_file):\n        with open(final_file, 'r') as f:\n            metrics = json.load(f)\n        f1_score = metrics.get('test_bullying_f1', 0.0)\n        print(f\"  âœ… ä½¿ç”¨ final_results.json\")\n    elif os.path.exists(eval_file):\n        with open(eval_file, 'r') as f:\n            metrics = json.load(f)\n        f1_score = metrics.get('bullying_f1', 0.0)\n        print(f\"  âœ… ä½¿ç”¨ eval_results.json\")\n    else:\n        print(f\"  âš ï¸ æœªæ‰¾åˆ°çµæœæª”æ¡ˆ - è¨“ç·´å¯èƒ½å¤±æ•—æˆ–æœªå®Œæˆ\\n\")\n        continue\n    \n    results.append({\n        'name': config.name,\n        'f1': f1_score,\n        'path': model_dir,\n        'metrics': metrics\n    })\n    \n    status = \"âœ…\" if f1_score >= 0.75 else \"âš ï¸\"\n    print(f\"  {status} F1 = {f1_score:.4f}\\n\")\n\n# é¸å‡ºæœ€ä½³æ¨¡å‹\nprint(\"=\"*60)\nif results:\n    best_model = max(results, key=lambda x: x['f1'])\n    print(f\"ğŸ† æœ€ä½³: {best_model['name']}\")\n    print(f\"ğŸ“Š F1: {best_model['f1']:.4f}\")\n    print(\"=\"*60)\n    \n    with open('best_model_a100.json', 'w') as f:\n        json.dump(best_model, f, indent=2, ensure_ascii=False)\nelse:\n    print(\"âŒ æ²’æœ‰å¯ç”¨çµæœ\")\n    print(\"å¯èƒ½åŸå› :\")\n    print(\"  1. è¨“ç·´è…³æœ¬åŸ·è¡Œå¤±æ•—ï¼ˆæª¢æŸ¥ä¸Šé¢çš„è¼¸å‡ºï¼‰\")\n    print(\"  2. ç™¼ç”Ÿ CUDA OOM æˆ–å…¶ä»–éŒ¯èª¤\")\n    print(\"  3. è³‡æ–™è¼‰å…¥å¤±æ•—\")\n    print(\"  4. è«‹å‘ä¸Šæ²å‹•æŸ¥çœ‹è¨“ç·´ cell çš„å®Œæ•´è¼¸å‡º\")\n    print(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ è¤‡è£½æœ€ä½³æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import shutil\nimport pandas as pd\n\nif 'best_model' in locals() and best_model['f1'] >= 0.70:\n    deploy_dir = \"models/bullying_a100_best\"\n    os.makedirs(deploy_dir, exist_ok=True)\n\n    # é©—è­‰æ¨¡å‹æ¬Šé‡æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n    model_files = [f for f in os.listdir(best_model['path'])\n                   if f.endswith(('.safetensors', '.bin', '.pt', '.pth'))]\n\n    if not model_files:\n        print(\"âŒ éŒ¯èª¤: æœªæ‰¾åˆ°æ¨¡å‹æ¬Šé‡æª”æ¡ˆï¼\")\n        print(f\"æª¢æŸ¥ç›®éŒ„: {best_model['path']}\")\n        print(f\"ç›®éŒ„å…§å®¹: {os.listdir(best_model['path'])}\")\n        print(\"\\nå¯èƒ½åŸå› :\")\n        print(\"1. è¨“ç·´è…³æœ¬æ²’æœ‰æ­£ç¢ºä¿å­˜æ¨¡å‹\")\n        print(\"2. æ¨¡å‹ä¿å­˜è·¯å¾‘ä¸æ­£ç¢º\")\n        print(\"3. è¨“ç·´éç¨‹ä¸­å‡ºç¾éŒ¯èª¤\")\n    else:\n        print(f\"âœ… æ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹æª”æ¡ˆ:\")\n        total_size = 0\n        for f in model_files:\n            size = os.path.getsize(os.path.join(best_model['path'], f))\n            total_size += size\n            print(f\"  - {f}: {size / 1024 / 1024:.1f} MB\")\n        print(f\"ç¸½å¤§å°: {total_size / 1024 / 1024:.1f} MB\")\n\n        # è¤‡è£½æ‰€æœ‰æª”æ¡ˆ\n        print(f\"\\nğŸ“¦ è¤‡è£½åˆ° {deploy_dir}...\")\n        for file in os.listdir(best_model['path']):\n            src = os.path.join(best_model['path'], file)\n            if os.path.isfile(src):\n                shutil.copy2(src, os.path.join(deploy_dir, file))\n\n        # ä¿å­˜éƒ¨ç½²è³‡è¨Š\n        deploy_info = {\n            'model_name': best_model['name'],\n            'f1_score': best_model['f1'],\n            'metrics': best_model['metrics'],\n            'trained_on': 'Google Colab A100',\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'model_files': model_files,\n            'total_size_mb': round(total_size / 1024 / 1024, 2)\n        }\n\n        with open(os.path.join(deploy_dir, 'deployment_info.json'), 'w') as f:\n            json.dump(deploy_info, f, indent=2, ensure_ascii=False)\n\n        # é©—è­‰è¤‡è£½çµæœ\n        copied_files = os.listdir(deploy_dir)\n        print(f\"\\nâœ… å·²è¤‡è£½ {len(copied_files)} å€‹æª”æ¡ˆåˆ° {deploy_dir}\")\n        print(f\"ğŸ“Š F1: {best_model['f1']:.4f}\")\n\n        # åˆ—å‡ºè¤‡è£½çš„æª”æ¡ˆ\n        print(\"\\nğŸ“ éƒ¨ç½²ç›®éŒ„å…§å®¹:\")\n        for f in sorted(copied_files):\n            size = os.path.getsize(os.path.join(deploy_dir, f))\n            print(f\"  - {f}: {size / 1024 / 1024:.1f} MB\")\nelse:\n    print(\"âš ï¸ æœªé”æ¨™æˆ–ç„¡å¯ç”¨æ¨¡å‹\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ æ¨é€åˆ° GitHub (å¦‚æœé”æ¨™ â‰¥0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "TARGET_F1 = 0.75\n\nif 'best_model' in locals() and best_model['f1'] >= TARGET_F1:\n    print(f\"ğŸ‰ é”æ¨™ï¼F1 = {best_model['f1']:.4f} â‰¥ {TARGET_F1}\")\n    \n    # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n    deploy_dir = \"models/bullying_a100_best\"\n    model_files = [f for f in os.listdir(deploy_dir) \n                   if f.endswith(('.safetensors', '.bin', '.pt', '.pth'))]\n    \n    if not model_files:\n        print(\"\\nâŒ éŒ¯èª¤: éƒ¨ç½²ç›®éŒ„ä¸­æ²’æœ‰æ¨¡å‹æ¬Šé‡æª”æ¡ˆï¼\")\n        print(\"è«‹æª¢æŸ¥ä¸Šä¸€å€‹ cell çš„è¼¸å‡ºï¼Œç¢ºèªæ¨¡å‹æ˜¯å¦æ­£ç¢ºè¤‡è£½ã€‚\")\n    else:\n        print(f\"\\nâœ… ç¢ºèª {len(model_files)} å€‹æ¨¡å‹æª”æ¡ˆå­˜åœ¨\")\n        for f in model_files:\n            size = os.path.getsize(os.path.join(deploy_dir, f))\n            print(f\"  - {f}: {size / 1024 / 1024:.1f} MB\")\n        \n        print(\"\\næ¨é€åˆ° GitHub...\\n\")\n        \n        # Git LFS è¨­å®š\n        print(\"ğŸ“¦ è¨­å®š Git LFS...\")\n        !git lfs install\n        \n        # ç¢ºä¿ .gitattributes æ­£ç¢ºè¿½è¹¤å¤§æª”æ¡ˆ\n        gitattributes_content = \"\"\"# Model weights (large files)\nmodels/**/*.safetensors filter=lfs diff=lfs merge=lfs -text\nmodels/**/*.bin filter=lfs diff=lfs merge=lfs -text\nmodels/**/*.pt filter=lfs diff=lfs merge=lfs -text\nmodels/**/*.pth filter=lfs diff=lfs merge=lfs -text\nmodels/**/*.ckpt filter=lfs diff=lfs merge=lfs -text\n\n# Training data (large files)\ndata/**/*.json filter=lfs diff=lfs merge=lfs -text\ndata/**/*.csv filter=lfs diff=lfs merge=lfs -text\ndata/**/*.txt filter=lfs diff=lfs merge=lfs -text\n\"\"\"\n        \n        with open('.gitattributes', 'w') as f:\n            f.write(gitattributes_content)\n        \n        print(\"âœ… .gitattributes å·²æ›´æ–°\")\n        \n        # æ·»åŠ åˆ° Git\n        !git add .gitattributes\n        !git add models/bullying_a100_best/\n        \n        # æª¢æŸ¥ Git ç‹€æ…‹\n        print(\"\\nğŸ“‹ Git ç‹€æ…‹:\")\n        !git status --short\n        \n        # æª¢æŸ¥ LFS è¿½è¹¤ç‹€æ…‹\n        print(\"\\nğŸ“¦ Git LFS è¿½è¹¤çš„æª”æ¡ˆ:\")\n        !git lfs ls-files\n        \n        # æäº¤\n        commit_msg = f\"feat: A100 trained model (F1={best_model['f1']:.4f})\"\n        print(f\"\\nğŸ’¾ æäº¤è¨Šæ¯: {commit_msg}\")\n        !git commit -m \"{commit_msg}\"\n        \n        # æ¨é€ï¼ˆåŒ…å« LFS æª”æ¡ˆï¼‰\n        print(\"\\nâ¬†ï¸ æ¨é€åˆ° GitHubï¼ˆåŒ…å« LFS æª”æ¡ˆï¼‰...\")\n        !git push origin main\n        \n        print(\"\\nâœ… æ¨é€å®Œæˆï¼\")\n        print(f\"\\nğŸ¯ è¨“ç·´çµæœ:\")\n        print(f\"  - æ¨¡å‹: {best_model['name']}\")\n        print(f\"  - F1: {best_model['f1']:.4f}\")\n        print(f\"  - æª”æ¡ˆ: {', '.join(model_files)}\")\n        \nelif 'best_model' in locals():\n    print(f\"âš ï¸ æœªé”æ¨™: F1 = {best_model['f1']:.4f} < {TARGET_F1}\")\n    print(\"æ¨¡å‹æœªæ¨é€åˆ° GitHub\")\nelse:\n    print(\"âŒ ç„¡å¯ç”¨æ¨¡å‹\")\n    print(\"è«‹æª¢æŸ¥è¨“ç·´éç¨‹æ˜¯å¦æœ‰éŒ¯èª¤\")"
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ’¾ å‚™ä»½åˆ° Google Drive (å¯é¸)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# å¯é¸ï¼šå‚™ä»½æ¨¡å‹åˆ° Google Drive\n# å¦‚æœ GitHub æ¨é€å¤±æ•—æˆ–æƒ³è¦é¡å¤–å‚™ä»½ï¼Œå¯åŸ·è¡Œæ­¤ cell\n\nBACKUP_TO_DRIVE = False  # è¨­ç‚º True ä¾†å•Ÿç”¨ Drive å‚™ä»½\n\nif BACKUP_TO_DRIVE and 'best_model' in locals():\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive', force_remount=True)\n        \n        # è¨­å®šå‚™ä»½è·¯å¾‘\n        drive_backup_dir = f\"/content/drive/MyDrive/CyberPuppy_Models/{best_model['name']}\"\n        os.makedirs(drive_backup_dir, exist_ok=True)\n        \n        # è¤‡è£½æ¨¡å‹åˆ° Drive\n        deploy_dir = \"models/bullying_a100_best\"\n        print(f\"ğŸ“¦ å‚™ä»½æ¨¡å‹åˆ° Google Drive...\")\n        print(f\"ç›®æ¨™: {drive_backup_dir}\")\n        \n        for file in os.listdir(deploy_dir):\n            src = os.path.join(deploy_dir, file)\n            dst = os.path.join(drive_backup_dir, file)\n            if os.path.isfile(src):\n                shutil.copy2(src, dst)\n                size = os.path.getsize(dst) / 1024 / 1024\n                print(f\"  âœ… {file} ({size:.1f} MB)\")\n        \n        print(f\"\\nâœ… å‚™ä»½å®Œæˆï¼\")\n        print(f\"ğŸ“ ä½ç½®: {drive_backup_dir}\")\n        \n    except Exception as e:\n        print(f\"âŒ Drive å‚™ä»½å¤±æ•—: {e}\")\n        print(\"æ¨¡å‹ä»ä¿å­˜åœ¨ Colab æœ¬åœ°å’Œ GitHubï¼ˆå¦‚æœæ¨é€æˆåŠŸï¼‰\")\nelse:\n    print(\"ğŸ’¡ æç¤º: å¦‚éœ€å‚™ä»½åˆ° Google Driveï¼Œè«‹å°‡ BACKUP_TO_DRIVE è¨­ç‚º True\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ è¨“ç·´ç¸½çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ A100 è¨“ç·´ç¸½çµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(\"\\nè¨“ç·´çµæœ:\")\n",
    "    for model in sorted(results, key=lambda x: x['f1'], reverse=True):\n",
    "        status = \"âœ…\" if model['f1'] >= TARGET_F1 else \"âš ï¸\"\n",
    "        print(f\"  {status} {model['name']}: F1 = {model['f1']:.4f}\")\n",
    "    \n",
    "    if 'best_model' in locals():\n",
    "        print(f\"\\nğŸ† æœ€ä½³: {best_model['name']}\")\n",
    "        print(f\"ğŸ“Š F1: {best_model['f1']:.4f}\")\n",
    "        print(f\"ğŸ¯ ç›®æ¨™: {TARGET_F1}\")\n",
    "        \n",
    "        if best_model['f1'] >= TARGET_F1:\n",
    "            print(\"\\nâœ… æˆåŠŸï¼æ¨¡å‹å·²æ¨é€åˆ° GitHub\")\n",
    "        else:\n",
    "            gap = TARGET_F1 - best_model['f1']\n",
    "            print(f\"\\nâš ï¸ è·é›¢ç›®æ¨™: {gap:.4f}\")\n",
    "else:\n",
    "    print(\"\\nâŒ ç„¡è¨“ç·´çµæœ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}