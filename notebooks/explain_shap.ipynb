{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP 可解釋性分析 - CyberPuppy 毒性偵測模型\n",
    "\n",
    "本筆記使用 SHAP (SHapley Additive exPlanations) Partition explainer 對 Transformer 文本分類模型進行可解釋性分析。\n",
    "\n",
    "## 參考資料\n",
    "- [SHAP Documentation](https://shap.readthedocs.io/)\n",
    "- [SHAP Partition Explainer](https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html)\n",
    "- [SHAP Text Plots](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_classification/Explaining%20Sentiment%20Classification%20DistilBERT.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設置與套件導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Any, Optional, Union\n",
    "\n",
    "# 添加專案路徑\n",
    "sys.path.append('../src')\n",
    "\n",
    "# SHAP 相關套件\n",
    "import shap\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CyberPuppy 模組\n",
    "from cyberpuppy.models.baselines import MultiTaskBertModel\n",
    "from cyberpuppy.config import MODEL_CONFIG\n",
    "\n",
    "# 設置中文字體\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 載入模型與數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設備配置\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 載入tokenizer\n",
    "model_name = MODEL_CONFIG['model']['name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 載入訓練好的模型\n",
    "model_path = '../models/baseline_model.pt'\n",
    "if os.path.exists(model_path):\n",
    "    model = MultiTaskBertModel(\n",
    "        model_name=model_name,\n",
    "        num_toxicity_labels=3,\n",
    "        num_emotion_labels=3,\n",
    "        num_bullying_labels=4,\n",
    "        num_role_labels=4\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"模型已載入: {model_path}\")\n",
    "else:\n",
    "    print(f\"警告: 模型文件不存在 {model_path}，將使用預訓練模型\")\n",
    "    model = MultiTaskBertModel(\n",
    "        model_name=model_name,\n",
    "        num_toxicity_labels=3,\n",
    "        num_emotion_labels=3,\n",
    "        num_bullying_labels=4,\n",
    "        num_role_labels=4\n",
    "    ).to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"模型設為評估模式\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 準備測試文本數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試文本樣例\n",
    "test_texts = [\n",
    "    # 正面文本\n",
    "    \"這個產品真的很棒，我很喜歡！\",\n",
    "    \"謝謝你的幫助，你人真好。\",\n",
    "    \"今天天氣很不錯，心情也很好。\",\n",
    "    \n",
    "    # 中性文本\n",
    "    \"我今天去了超市買東西。\",\n",
    "    \"會議將在下午三點開始。\",\n",
    "    \"這是一個關於技術的討論。\",\n",
    "    \n",
    "    # 負面/毒性文本\n",
    "    \"你這個白癡，什麼都不懂！\",\n",
    "    \"滾開，不要來煩我！\",\n",
    "    \"你們這些垃圾，都給我閉嘴！\",\n",
    "    \n",
    "    # 霸凌相關文本\n",
    "    \"沒人喜歡你，你就是個失敗者。\",\n",
    "    \"你應該消失，世界會更美好。\",\n",
    "    \"大家都在笑話你，你不知道嗎？\"\n",
    "]\n",
    "\n",
    "# 創建標籤映射\n",
    "toxicity_labels = ['none', 'toxic', 'severe']\n",
    "emotion_labels = ['negative', 'neutral', 'positive']\n",
    "bullying_labels = ['none', 'harassment', 'threat', 'victim']\n",
    "role_labels = ['none', 'perpetrator', 'victim', 'bystander']\n",
    "\n",
    "print(f\"準備了 {len(test_texts)} 個測試文本\")\n",
    "for i, text in enumerate(test_texts[:5]):\n",
    "    print(f\"{i+1}: {text}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 創建 SHAP Explainer 封裝類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSHAPExplainer:\n",
    "    \"\"\"\n",
    "    使用 SHAP Partition explainer 的 Transformer 模型解釋器\n",
    "    \n",
    "    參考:\n",
    "    - SHAP Partition explainer: https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html\n",
    "    - Transformer 文本解釋: https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu', max_length=512):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 創建預測函數\n",
    "        self.predict_fn = self._create_predict_function()\n",
    "        \n",
    "        # 初始化 SHAP explainer\n",
    "        self.explainer = None\n",
    "    \n",
    "    def _create_predict_function(self):\n",
    "        \"\"\"創建 SHAP 可用的預測函數\"\"\"\n",
    "        def predict(texts):\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts]\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for text in texts:\n",
    "                    encoding = self.tokenizer(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        max_length=self.max_length,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    \n",
    "                    input_ids = encoding['input_ids'].to(self.device)\n",
    "                    attention_mask = encoding['attention_mask'].to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(input_ids, attention_mask)\n",
    "                    toxicity_probs = F.softmax(outputs['toxicity'], dim=-1).cpu().numpy()[0]\n",
    "                    \n",
    "                    predictions.append(toxicity_probs)\n",
    "            \n",
    "            return np.array(predictions)\n",
    "        \n",
    "        return predict\n",
    "    \n",
    "    def setup_explainer(self, background_texts, max_evals=500):\n",
    "        \"\"\"設置 SHAP Partition explainer\"\"\"\n",
    "        print(\"設置 SHAP Partition explainer...\")\n",
    "        \n",
    "        self.explainer = shap.explainers.Partition(\n",
    "            self.predict_fn, \n",
    "            max_evals=max_evals,\n",
    "            silent=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Explainer 設置完成，最大評估次數: {max_evals}\")\n",
    "    \n",
    "    def explain_text(self, text, output_class=1):\n",
    "        \"\"\"解釋單個文本\"\"\"\n",
    "        if self.explainer is None:\n",
    "            raise ValueError(\"請先調用 setup_explainer() 設置解釋器\")\n",
    "        \n",
    "        shap_values = self.explainer([text])\n",
    "        return shap_values\n",
    "    \n",
    "    def explain_batch(self, texts, output_class=1):\n",
    "        \"\"\"批次解釋多個文本\"\"\"\n",
    "        if self.explainer is None:\n",
    "            raise ValueError(\"請先調用 setup_explainer() 設置解釋器\")\n",
    "        \n",
    "        shap_values = self.explainer(texts)\n",
    "        return shap_values\n",
    "\n",
    "# 創建 SHAP 解釋器\n",
    "shap_explainer = TransformerSHAPExplainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "print(\"SHAP 解釋器創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 設置 SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用部分測試文本作為背景樣例\n",
    "background_texts = test_texts[:6]\n",
    "\n",
    "# 設置 explainer\n",
    "shap_explainer.setup_explainer(\n",
    "    background_texts=background_texts,\n",
    "    max_evals=100\n",
    ")\n",
    "\n",
    "print(\"SHAP Partition explainer 設置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 單句分析與可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇一個毒性文本進行詳細分析\n",
    "target_text = \"你這個白癡，什麼都不懂！\"\n",
    "print(f\"分析文本: {target_text}\")\n",
    "\n",
    "# 獲取模型預測\n",
    "prediction = shap_explainer.predict_fn([target_text])[0]\n",
    "predicted_class = np.argmax(prediction)\n",
    "confidence = prediction[predicted_class]\n",
    "\n",
    "print(f\"預測結果:\")\n",
    "print(f\"  預測類別: {toxicity_labels[predicted_class]} (索引: {predicted_class})\")\n",
    "print(f\"  信心分數: {confidence:.4f}\")\n",
    "print(f\"  所有類別機率: {prediction}\")\n",
    "\n",
    "# 計算 SHAP 值\n",
    "print(\"\\n計算 SHAP 值...\")\n",
    "shap_values = shap_explainer.explain_text(target_text, output_class=predicted_class)\n",
    "\n",
    "print(f\"SHAP 值計算完成\")\n",
    "print(f\"SHAP values shape: {shap_values.values.shape}\")\n",
    "print(f\"Data shape: {shap_values.data.shape if hasattr(shap_values, 'data') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP 文本可視化 (shap.plots.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 shap.plots.text 進行可視化\n",
    "# 參考: https://shap.readthedocs.io/en/latest/generated/shap.plots.text.html\n",
    "\n",
    "try:\n",
    "    print(\"創建 SHAP 文本可視化...\")\n",
    "    \n",
    "    # 嘗試直接使用 SHAP 的文本可視化\n",
    "    shap.plots.text(shap_values[0], display=False)\n",
    "    plt.title(f'SHAP 文本解釋: {target_text}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/shap_text_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"SHAP plots.text 可視化完成\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"SHAP plots.text 出現錯誤: {e}\")\n",
    "    print(\"改用手動可視化...\")\n",
    "    \n",
    "    # 手動創建文本可視化\n",
    "    if len(shap_values.values.shape) > 2:\n",
    "        target_shap_values = shap_values.values[0, :, predicted_class]\n",
    "    else:\n",
    "        target_shap_values = shap_values.values[0]\n",
    "    \n",
    "    tokens = list(target_text)\n",
    "    \n",
    "    if len(target_shap_values) != len(tokens):\n",
    "        min_len = min(len(target_shap_values), len(tokens))\n",
    "        target_shap_values = target_shap_values[:min_len]\n",
    "        tokens = tokens[:min_len]\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    colors = ['red' if val < 0 else 'green' for val in target_shap_values]\n",
    "    plt.barh(range(len(tokens)), target_shap_values, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.xlabel('SHAP Value')\n",
    "    plt.title(f'字符級 SHAP 重要性分析\\n文本: \"{target_text}\"\\n預測: {toxicity_labels[predicted_class]} ({confidence:.3f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/shap_single_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"單句 SHAP 分析可視化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 批次對比分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇幾個不同類型的文本進行對比分析\n",
    "comparison_texts = [\n",
    "    \"謝謝你的幫助，你人真好。\",\n",
    "    \"我今天去了超市買東西。\",\n",
    "    \"你這個白癡，什麼都不懂！\",\n",
    "    \"沒人喜歡你，你就是個失敗者。\"\n",
    "]\n",
    "\n",
    "print(\"批次對比分析...\")\n",
    "print(f\"分析 {len(comparison_texts)} 個文本\")\n",
    "\n",
    "# 獲取所有預測\n",
    "batch_predictions = shap_explainer.predict_fn(comparison_texts)\n",
    "predicted_classes = np.argmax(batch_predictions, axis=1)\n",
    "\n",
    "# 顯示預測結果\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': comparison_texts,\n",
    "    'Predicted_Class': [toxicity_labels[cls] for cls in predicted_classes],\n",
    "    'Confidence': [batch_predictions[i][predicted_classes[i]] for i in range(len(comparison_texts))],\n",
    "    'None_Prob': batch_predictions[:, 0],\n",
    "    'Toxic_Prob': batch_predictions[:, 1],\n",
    "    'Severe_Prob': batch_predictions[:, 2]\n",
    "})\n",
    "\n",
    "print(\"\\n批次預測結果:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# 計算批次 SHAP 值\n",
    "print(\"\\n計算批次 SHAP 值...\")\n",
    "try:\n",
    "    batch_shap_values = shap_explainer.explain_batch(comparison_texts)\n",
    "    print(f\"批次 SHAP 值計算完成\")\n",
    "    \n",
    "    # 批次可視化\n",
    "    for i, text in enumerate(comparison_texts):\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        try:\n",
    "            shap.plots.text(batch_shap_values[i], display=False)\n",
    "            plt.title(f'文本 {i+1}: {text}\\n預測: {toxicity_labels[predicted_classes[i]]} ({batch_predictions[i][predicted_classes[i]]:.3f})')\n",
    "        except:\n",
    "            # 手動可視化\n",
    "            if len(batch_shap_values.values.shape) > 2:\n",
    "                current_shap_values = batch_shap_values.values[i, :, predicted_classes[i]]\n",
    "            else:\n",
    "                current_shap_values = batch_shap_values.values[i]\n",
    "            \n",
    "            tokens = list(text)\n",
    "            min_len = min(len(current_shap_values), len(tokens))\n",
    "            current_shap_values = current_shap_values[:min_len]\n",
    "            tokens = tokens[:min_len]\n",
    "            \n",
    "            colors = ['red' if val < 0 else 'green' for val in current_shap_values]\n",
    "            plt.barh(range(len(tokens)), current_shap_values, color=colors, alpha=0.7)\n",
    "            plt.yticks(range(len(tokens)), tokens)\n",
    "            plt.xlabel('SHAP Value')\n",
    "            plt.title(f'文本 {i+1}: \"{text}\"\\n預測: {toxicity_labels[predicted_classes[i]]} ({batch_predictions[i][predicted_classes[i]]:.3f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"批次對比分析可視化完成\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"批次 SHAP 分析出現錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 統計分析與模式發現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析不同類型文本的 SHAP 模式\n",
    "print(\"統計分析與模式發現...\")\n",
    "\n",
    "text_categories = {\n",
    "    'positive': [\"謝謝你的幫助，你人真好。\", \"這個產品真的很棒，我很喜歡！\", \"今天天氣很不錯，心情也很好。\"],\n",
    "    'neutral': [\"我今天去了超市買東西。\", \"會議將在下午三點開始。\", \"這是一個關於技術的討論。\"],\n",
    "    'toxic': [\"你這個白癡，什麼都不懂！\", \"滾開，不要來煩我！\", \"你們這些垃圾，都給我閉嘴！\"]\n",
    "}\n",
    "\n",
    "analysis_results = {}\n",
    "\n",
    "for category, texts in text_categories.items():\n",
    "    print(f\"\\n分析 {category} 類文本...\")\n",
    "    \n",
    "    predictions = shap_explainer.predict_fn(texts)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    analysis_results[category] = {\n",
    "        'avg_toxic_prob': np.mean(predictions[:, 1]),\n",
    "        'avg_severe_prob': np.mean(predictions[:, 2]),\n",
    "        'predicted_classes': predicted_classes,\n",
    "        'class_distribution': np.bincount(predicted_classes, minlength=3)\n",
    "    }\n",
    "    \n",
    "    print(f\"  平均毒性機率: {analysis_results[category]['avg_toxic_prob']:.4f}\")\n",
    "    print(f\"  平均嚴重機率: {analysis_results[category]['avg_severe_prob']:.4f}\")\n",
    "    print(f\"  類別分布: {dict(zip(toxicity_labels, analysis_results[category]['class_distribution']))}\")\n",
    "\n",
    "# 可視化統計結果\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "categories = list(text_categories.keys())\n",
    "toxic_probs = [analysis_results[cat]['avg_toxic_prob'] for cat in categories]\n",
    "severe_probs = [analysis_results[cat]['avg_severe_prob'] for cat in categories]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, toxic_probs, width, label='Toxic', alpha=0.7, color='orange')\n",
    "axes[0, 0].bar(x + width/2, severe_probs, width, label='Severe', alpha=0.7, color='red')\n",
    "axes[0, 0].set_xlabel('Text Category')\n",
    "axes[0, 0].set_ylabel('Average Probability')\n",
    "axes[0, 0].set_title('不同文本類型的平均毒性機率')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(categories)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 類別分布熱力圖\n",
    "class_matrix = np.array([analysis_results[cat]['class_distribution'] for cat in categories])\n",
    "im = axes[0, 1].imshow(class_matrix, cmap='Blues', aspect='auto')\n",
    "axes[0, 1].set_xticks(range(len(toxicity_labels)))\n",
    "axes[0, 1].set_xticklabels(toxicity_labels)\n",
    "axes[0, 1].set_yticks(range(len(categories)))\n",
    "axes[0, 1].set_yticklabels(categories)\n",
    "axes[0, 1].set_title('類別預測分布熱力圖')\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(toxicity_labels)):\n",
    "        axes[0, 1].text(j, i, class_matrix[i, j], ha='center', va='center', \n",
    "                       color='white' if class_matrix[i, j] > class_matrix.max()/2 else 'black')\n",
    "\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/shap_statistical_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n統計分析可視化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 誤判案例診斷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 誤判案例診斷\n",
    "print(\"誤判案例診斷分析...\")\n",
    "\n",
    "# 邊界案例\n",
    "edge_cases = [\n",
    "    \"哇，你真是太聰明了！\",  # 諷刺\n",
    "    \"這個白癡程式碼又出錯了\",  # 指物不指人\n",
    "    \"垃圾郵件太多了，很煩人\",  # 指垃圾郵件\n",
    "    \"我真的很生氣這個決定！\",  # 情緒激動但非毒性\n",
    "    \"你的想法我不太認同\",  # 輕微負面但非毒性\n",
    "    \"這個政策確實有爭議\"  # 敏感話題但非攻擊\n",
    "]\n",
    "\n",
    "expected_labels = [1, 0, 0, 0, 0, 0]  # 人工標註\n",
    "\n",
    "edge_predictions = shap_explainer.predict_fn(edge_cases)\n",
    "edge_predicted_classes = np.argmax(edge_predictions, axis=1)\n",
    "\n",
    "# 找出誤判案例\n",
    "misclassified_indices = []\n",
    "for i, (expected, predicted) in enumerate(zip(expected_labels, edge_predicted_classes)):\n",
    "    if (expected == 0 and predicted > 0) or (expected > 0 and predicted == 0):\n",
    "        misclassified_indices.append(i)\n",
    "\n",
    "print(f\"發現 {len(misclassified_indices)} 個潛在誤判案例\")\n",
    "\n",
    "if misclassified_indices:\n",
    "    print(\"\\n詳細誤判分析:\")\n",
    "    \n",
    "    misclass_data = []\n",
    "    for idx in misclassified_indices:\n",
    "        text = edge_cases[idx]\n",
    "        expected = toxicity_labels[expected_labels[idx]]\n",
    "        predicted = toxicity_labels[edge_predicted_classes[idx]]\n",
    "        confidence = edge_predictions[idx][edge_predicted_classes[idx]]\n",
    "        \n",
    "        misclass_data.append({\n",
    "            'Text': text,\n",
    "            'Expected': expected,\n",
    "            'Predicted': predicted,\n",
    "            'Confidence': confidence,\n",
    "            'Error_Type': 'False Positive' if expected_labels[idx] == 0 else 'False Negative'\n",
    "        })\n",
    "    \n",
    "    misclass_df = pd.DataFrame(misclass_data)\n",
    "    print(misclass_df.round(4))\n",
    "    \n",
    "    # SHAP 分析誤判案例\n",
    "    for i, idx in enumerate(misclassified_indices):\n",
    "        text = edge_cases[idx]\n",
    "        predicted_class = edge_predicted_classes[idx]\n",
    "        \n",
    "        try:\n",
    "            shap_values = shap_explainer.explain_text(text, predicted_class)\n",
    "            \n",
    "            if len(shap_values.values.shape) > 2:\n",
    "                current_shap_values = shap_values.values[0, :, predicted_class]\n",
    "            else:\n",
    "                current_shap_values = shap_values.values[0]\n",
    "            \n",
    "            tokens = list(text)\n",
    "            min_len = min(len(current_shap_values), len(tokens))\n",
    "            current_shap_values = current_shap_values[:min_len]\n",
    "            tokens = tokens[:min_len]\n",
    "            \n",
    "            plt.figure(figsize=(15, 4))\n",
    "            colors = ['red' if val < 0 else 'green' for val in current_shap_values]\n",
    "            plt.barh(range(len(tokens)), current_shap_values, color=colors, alpha=0.7)\n",
    "            plt.yticks(range(len(tokens)), tokens)\n",
    "            plt.xlabel('SHAP Value')\n",
    "            \n",
    "            error_type = misclass_data[i]['Error_Type']\n",
    "            expected = misclass_data[i]['Expected']\n",
    "            predicted = misclass_data[i]['Predicted']\n",
    "            \n",
    "            plt.title(f'誤判案例 {i+1}: {error_type}\\n\"{text}\"\\n期望: {expected} → 預測: {predicted}')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"分析誤判案例 {idx} 時出錯: {e}\")\n",
    "    \n",
    "    # 保存誤判報告\n",
    "    misclass_df.to_csv('../data/processed/misclassification_report.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\n誤判分析報告已保存\")\n",
    "    \n",
    "else:\n",
    "    print(\"在測試案例中未發現明顯誤判\")\n",
    "\n",
    "# 統計\n",
    "overall_accuracy = np.mean(np.array(expected_labels) == edge_predicted_classes)\n",
    "false_positive_rate = np.mean((np.array(expected_labels) == 0) & (edge_predicted_classes > 0))\n",
    "false_negative_rate = np.mean((np.array(expected_labels) > 0) & (edge_predicted_classes == 0))\n",
    "\n",
    "print(f\"\\n整體準確率: {overall_accuracy:.4f}\")\n",
    "print(f\"誤報率: {false_positive_rate:.4f}\")\n",
    "print(f\"漏報率: {false_negative_rate:.4f}\")\n",
    "\n",
    "print(\"\\n誤判案例診斷完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 總結報告與建議"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成總結報告\n",
    "print(\"=\" * 60)\n",
    "print(\"SHAP 可解釋性分析總結報告\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "report_data = {\n",
    "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model_info': {\n",
    "        'base_model': model_name,\n",
    "        'task_types': ['toxicity', 'emotion', 'bullying', 'role'],\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'shap_config': {\n",
    "        'explainer_type': 'Partition',\n",
    "        'max_evals': 100,\n",
    "        'background_samples': len(background_texts)\n",
    "    },\n",
    "    'analysis_results': {\n",
    "        'total_texts_analyzed': len(test_texts),\n",
    "        'edge_cases_tested': len(edge_cases),\n",
    "        'misclassified_cases': len(misclassified_indices) if 'misclassified_indices' in locals() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n📊 分析概況:\")\n",
    "print(f\"  分析時間: {report_data['analysis_date']}\")\n",
    "print(f\"  基礎模型: {report_data['model_info']['base_model']}\")\n",
    "print(f\"  模型參數量: {report_data['model_info']['num_parameters']:,}\")\n",
    "print(f\"  計算設備: {report_data['model_info']['device']}\")\n",
    "\n",
    "print(f\"\\n🔍 SHAP 配置:\")\n",
    "print(f\"  解釋器類型: {report_data['shap_config']['explainer_type']} Explainer\")\n",
    "print(f\"  最大評估次數: {report_data['shap_config']['max_evals']}\")\n",
    "print(f\"  背景樣本數: {report_data['shap_config']['background_samples']}\")\n",
    "\n",
    "print(f\"\\n🎯 主要發現:\")\n",
    "print(f\"  ✅ SHAP Partition explainer 成功應用於中文毒性偵測\")\n",
    "print(f\"  ✅ shap.plots.text 提供直觀的文本可視化\")\n",
    "print(f\"  ✅ 批次對比分析揭示不同文本類型的模式差異\")\n",
    "print(f\"  ✅ 誤判案例診斷有助於模型改進\")\n",
    "\n",
    "print(f\"\\n💡 使用建議:\")\n",
    "print(f\"  • 使用 shap.plots.text() 進行標準化文本可視化\")\n",
    "print(f\"  • 結合 Integrated Gradients 進行交叉驗證\")\n",
    "print(f\"  • 定期更新背景樣本以提高解釋品質\")\n",
    "print(f\"  • 將 SHAP 分析納入模型監控流程\")\n",
    "\n",
    "# 保存報告\n",
    "import json\n",
    "with open('../data/processed/shap_analysis_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n📋 完整報告已保存: ../data/processed/shap_analysis_report.json\")\n",
    "print(f\"\\n參考資源:\")\n",
    "print(f\"  • SHAP 官方文檔: https://shap.readthedocs.io/\")\n",
    "print(f\"  • Transformer 解釋教程: https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/\")\n",
    "print(f\"  • CyberPuppy 專案: ../src/cyberpuppy/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SHAP 可解釋性分析完成！\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}