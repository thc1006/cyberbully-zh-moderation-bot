{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP å¯è§£é‡‹æ€§åˆ†æ - CyberPuppy æ¯’æ€§åµæ¸¬æ¨¡å‹\n",
    "\n",
    "æœ¬ç­†è¨˜ä½¿ç”¨ SHAP (SHapley Additive exPlanations) Partition explainer å° Transformer æ–‡æœ¬åˆ†é¡æ¨¡å‹é€²è¡Œå¯è§£é‡‹æ€§åˆ†æã€‚\n",
    "\n",
    "## åƒè€ƒè³‡æ–™\n",
    "- [SHAP Documentation](https://shap.readthedocs.io/)\n",
    "- [SHAP Partition Explainer](https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html)\n",
    "- [SHAP Text Plots](https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/text_classification/Explaining%20Sentiment%20Classification%20DistilBERT.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­ç½®èˆ‡å¥—ä»¶å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Any, Optional, Union\n",
    "\n",
    "# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘\n",
    "sys.path.append('../src')\n",
    "\n",
    "# SHAP ç›¸é—œå¥—ä»¶\n",
    "import shap\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CyberPuppy æ¨¡çµ„\n",
    "from cyberpuppy.models.baselines import MultiTaskBertModel\n",
    "from cyberpuppy.config import MODEL_CONFIG\n",
    "\n",
    "# è¨­ç½®ä¸­æ–‡å­—é«”\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Š\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¼‰å…¥æ¨¡å‹èˆ‡æ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å‚™é…ç½®\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# è¼‰å…¥tokenizer\n",
    "model_name = MODEL_CONFIG['model']['name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹\n",
    "model_path = '../models/baseline_model.pt'\n",
    "if os.path.exists(model_path):\n",
    "    model = MultiTaskBertModel(\n",
    "        model_name=model_name,\n",
    "        num_toxicity_labels=3,\n",
    "        num_emotion_labels=3,\n",
    "        num_bullying_labels=4,\n",
    "        num_role_labels=4\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"æ¨¡å‹å·²è¼‰å…¥: {model_path}\")\n",
    "else:\n",
    "    print(f\"è­¦å‘Š: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {model_path}ï¼Œå°‡ä½¿ç”¨é è¨“ç·´æ¨¡å‹\")\n",
    "    model = MultiTaskBertModel(\n",
    "        model_name=model_name,\n",
    "        num_toxicity_labels=3,\n",
    "        num_emotion_labels=3,\n",
    "        num_bullying_labels=4,\n",
    "        num_role_labels=4\n",
    "    ).to(device)\n",
    "\n",
    "model.eval()\n",
    "print(\"æ¨¡å‹è¨­ç‚ºè©•ä¼°æ¨¡å¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æº–å‚™æ¸¬è©¦æ–‡æœ¬æ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦æ–‡æœ¬æ¨£ä¾‹\n",
    "test_texts = [\n",
    "    # æ­£é¢æ–‡æœ¬\n",
    "    \"é€™å€‹ç”¢å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘å¾ˆå–œæ­¡ï¼\",\n",
    "    \"è¬è¬ä½ çš„å¹«åŠ©ï¼Œä½ äººçœŸå¥½ã€‚\",\n",
    "    \"ä»Šå¤©å¤©æ°£å¾ˆä¸éŒ¯ï¼Œå¿ƒæƒ…ä¹Ÿå¾ˆå¥½ã€‚\",\n",
    "    \n",
    "    # ä¸­æ€§æ–‡æœ¬\n",
    "    \"æˆ‘ä»Šå¤©å»äº†è¶…å¸‚è²·æ±è¥¿ã€‚\",\n",
    "    \"æœƒè­°å°‡åœ¨ä¸‹åˆä¸‰é»é–‹å§‹ã€‚\",\n",
    "    \"é€™æ˜¯ä¸€å€‹é—œæ–¼æŠ€è¡“çš„è¨è«–ã€‚\",\n",
    "    \n",
    "    # è² é¢/æ¯’æ€§æ–‡æœ¬\n",
    "    \"ä½ é€™å€‹ç™½ç™¡ï¼Œä»€éº¼éƒ½ä¸æ‡‚ï¼\",\n",
    "    \"æ»¾é–‹ï¼Œä¸è¦ä¾†ç…©æˆ‘ï¼\",\n",
    "    \"ä½ å€‘é€™äº›åƒåœ¾ï¼Œéƒ½çµ¦æˆ‘é–‰å˜´ï¼\",\n",
    "    \n",
    "    # éœ¸å‡Œç›¸é—œæ–‡æœ¬\n",
    "    \"æ²’äººå–œæ­¡ä½ ï¼Œä½ å°±æ˜¯å€‹å¤±æ•—è€…ã€‚\",\n",
    "    \"ä½ æ‡‰è©²æ¶ˆå¤±ï¼Œä¸–ç•Œæœƒæ›´ç¾å¥½ã€‚\",\n",
    "    \"å¤§å®¶éƒ½åœ¨ç¬‘è©±ä½ ï¼Œä½ ä¸çŸ¥é“å—ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "# å‰µå»ºæ¨™ç±¤æ˜ å°„\n",
    "toxicity_labels = ['none', 'toxic', 'severe']\n",
    "emotion_labels = ['negative', 'neutral', 'positive']\n",
    "bullying_labels = ['none', 'harassment', 'threat', 'victim']\n",
    "role_labels = ['none', 'perpetrator', 'victim', 'bystander']\n",
    "\n",
    "print(f\"æº–å‚™äº† {len(test_texts)} å€‹æ¸¬è©¦æ–‡æœ¬\")\n",
    "for i, text in enumerate(test_texts[:5]):\n",
    "    print(f\"{i+1}: {text}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å‰µå»º SHAP Explainer å°è£é¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSHAPExplainer:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ SHAP Partition explainer çš„ Transformer æ¨¡å‹è§£é‡‹å™¨\n",
    "    \n",
    "    åƒè€ƒ:\n",
    "    - SHAP Partition explainer: https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html\n",
    "    - Transformer æ–‡æœ¬è§£é‡‹: https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu', max_length=512):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # å‰µå»ºé æ¸¬å‡½æ•¸\n",
    "        self.predict_fn = self._create_predict_function()\n",
    "        \n",
    "        # åˆå§‹åŒ– SHAP explainer\n",
    "        self.explainer = None\n",
    "    \n",
    "    def _create_predict_function(self):\n",
    "        \"\"\"å‰µå»º SHAP å¯ç”¨çš„é æ¸¬å‡½æ•¸\"\"\"\n",
    "        def predict(texts):\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts]\n",
    "            \n",
    "            predictions = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for text in texts:\n",
    "                    encoding = self.tokenizer(\n",
    "                        text,\n",
    "                        truncation=True,\n",
    "                        padding='max_length',\n",
    "                        max_length=self.max_length,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    \n",
    "                    input_ids = encoding['input_ids'].to(self.device)\n",
    "                    attention_mask = encoding['attention_mask'].to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(input_ids, attention_mask)\n",
    "                    toxicity_probs = F.softmax(outputs['toxicity'], dim=-1).cpu().numpy()[0]\n",
    "                    \n",
    "                    predictions.append(toxicity_probs)\n",
    "            \n",
    "            return np.array(predictions)\n",
    "        \n",
    "        return predict\n",
    "    \n",
    "    def setup_explainer(self, background_texts, max_evals=500):\n",
    "        \"\"\"è¨­ç½® SHAP Partition explainer\"\"\"\n",
    "        print(\"è¨­ç½® SHAP Partition explainer...\")\n",
    "        \n",
    "        self.explainer = shap.explainers.Partition(\n",
    "            self.predict_fn, \n",
    "            max_evals=max_evals,\n",
    "            silent=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Explainer è¨­ç½®å®Œæˆï¼Œæœ€å¤§è©•ä¼°æ¬¡æ•¸: {max_evals}\")\n",
    "    \n",
    "    def explain_text(self, text, output_class=1):\n",
    "        \"\"\"è§£é‡‹å–®å€‹æ–‡æœ¬\"\"\"\n",
    "        if self.explainer is None:\n",
    "            raise ValueError(\"è«‹å…ˆèª¿ç”¨ setup_explainer() è¨­ç½®è§£é‡‹å™¨\")\n",
    "        \n",
    "        shap_values = self.explainer([text])\n",
    "        return shap_values\n",
    "    \n",
    "    def explain_batch(self, texts, output_class=1):\n",
    "        \"\"\"æ‰¹æ¬¡è§£é‡‹å¤šå€‹æ–‡æœ¬\"\"\"\n",
    "        if self.explainer is None:\n",
    "            raise ValueError(\"è«‹å…ˆèª¿ç”¨ setup_explainer() è¨­ç½®è§£é‡‹å™¨\")\n",
    "        \n",
    "        shap_values = self.explainer(texts)\n",
    "        return shap_values\n",
    "\n",
    "# å‰µå»º SHAP è§£é‡‹å™¨\n",
    "shap_explainer = TransformerSHAPExplainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "print(\"SHAP è§£é‡‹å™¨å‰µå»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è¨­ç½® SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨éƒ¨åˆ†æ¸¬è©¦æ–‡æœ¬ä½œç‚ºèƒŒæ™¯æ¨£ä¾‹\n",
    "background_texts = test_texts[:6]\n",
    "\n",
    "# è¨­ç½® explainer\n",
    "shap_explainer.setup_explainer(\n",
    "    background_texts=background_texts,\n",
    "    max_evals=100\n",
    ")\n",
    "\n",
    "print(\"SHAP Partition explainer è¨­ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å–®å¥åˆ†æèˆ‡å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¸æ“‡ä¸€å€‹æ¯’æ€§æ–‡æœ¬é€²è¡Œè©³ç´°åˆ†æ\n",
    "target_text = \"ä½ é€™å€‹ç™½ç™¡ï¼Œä»€éº¼éƒ½ä¸æ‡‚ï¼\"\n",
    "print(f\"åˆ†ææ–‡æœ¬: {target_text}\")\n",
    "\n",
    "# ç²å–æ¨¡å‹é æ¸¬\n",
    "prediction = shap_explainer.predict_fn([target_text])[0]\n",
    "predicted_class = np.argmax(prediction)\n",
    "confidence = prediction[predicted_class]\n",
    "\n",
    "print(f\"é æ¸¬çµæœ:\")\n",
    "print(f\"  é æ¸¬é¡åˆ¥: {toxicity_labels[predicted_class]} (ç´¢å¼•: {predicted_class})\")\n",
    "print(f\"  ä¿¡å¿ƒåˆ†æ•¸: {confidence:.4f}\")\n",
    "print(f\"  æ‰€æœ‰é¡åˆ¥æ©Ÿç‡: {prediction}\")\n",
    "\n",
    "# è¨ˆç®— SHAP å€¼\n",
    "print(\"\\nè¨ˆç®— SHAP å€¼...\")\n",
    "shap_values = shap_explainer.explain_text(target_text, output_class=predicted_class)\n",
    "\n",
    "print(f\"SHAP å€¼è¨ˆç®—å®Œæˆ\")\n",
    "print(f\"SHAP values shape: {shap_values.values.shape}\")\n",
    "print(f\"Data shape: {shap_values.data.shape if hasattr(shap_values, 'data') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP æ–‡æœ¬å¯è¦–åŒ– (shap.plots.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ shap.plots.text é€²è¡Œå¯è¦–åŒ–\n",
    "# åƒè€ƒ: https://shap.readthedocs.io/en/latest/generated/shap.plots.text.html\n",
    "\n",
    "try:\n",
    "    print(\"å‰µå»º SHAP æ–‡æœ¬å¯è¦–åŒ–...\")\n",
    "    \n",
    "    # å˜—è©¦ç›´æ¥ä½¿ç”¨ SHAP çš„æ–‡æœ¬å¯è¦–åŒ–\n",
    "    shap.plots.text(shap_values[0], display=False)\n",
    "    plt.title(f'SHAP æ–‡æœ¬è§£é‡‹: {target_text}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/shap_text_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"SHAP plots.text å¯è¦–åŒ–å®Œæˆ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"SHAP plots.text å‡ºç¾éŒ¯èª¤: {e}\")\n",
    "    print(\"æ”¹ç”¨æ‰‹å‹•å¯è¦–åŒ–...\")\n",
    "    \n",
    "    # æ‰‹å‹•å‰µå»ºæ–‡æœ¬å¯è¦–åŒ–\n",
    "    if len(shap_values.values.shape) > 2:\n",
    "        target_shap_values = shap_values.values[0, :, predicted_class]\n",
    "    else:\n",
    "        target_shap_values = shap_values.values[0]\n",
    "    \n",
    "    tokens = list(target_text)\n",
    "    \n",
    "    if len(target_shap_values) != len(tokens):\n",
    "        min_len = min(len(target_shap_values), len(tokens))\n",
    "        target_shap_values = target_shap_values[:min_len]\n",
    "        tokens = tokens[:min_len]\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    colors = ['red' if val < 0 else 'green' for val in target_shap_values]\n",
    "    plt.barh(range(len(tokens)), target_shap_values, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.xlabel('SHAP Value')\n",
    "    plt.title(f'å­—ç¬¦ç´š SHAP é‡è¦æ€§åˆ†æ\\næ–‡æœ¬: \"{target_text}\"\\né æ¸¬: {toxicity_labels[predicted_class]} ({confidence:.3f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/shap_single_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"å–®å¥ SHAP åˆ†æå¯è¦–åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ‰¹æ¬¡å°æ¯”åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¸æ“‡å¹¾å€‹ä¸åŒé¡å‹çš„æ–‡æœ¬é€²è¡Œå°æ¯”åˆ†æ\n",
    "comparison_texts = [\n",
    "    \"è¬è¬ä½ çš„å¹«åŠ©ï¼Œä½ äººçœŸå¥½ã€‚\",\n",
    "    \"æˆ‘ä»Šå¤©å»äº†è¶…å¸‚è²·æ±è¥¿ã€‚\",\n",
    "    \"ä½ é€™å€‹ç™½ç™¡ï¼Œä»€éº¼éƒ½ä¸æ‡‚ï¼\",\n",
    "    \"æ²’äººå–œæ­¡ä½ ï¼Œä½ å°±æ˜¯å€‹å¤±æ•—è€…ã€‚\"\n",
    "]\n",
    "\n",
    "print(\"æ‰¹æ¬¡å°æ¯”åˆ†æ...\")\n",
    "print(f\"åˆ†æ {len(comparison_texts)} å€‹æ–‡æœ¬\")\n",
    "\n",
    "# ç²å–æ‰€æœ‰é æ¸¬\n",
    "batch_predictions = shap_explainer.predict_fn(comparison_texts)\n",
    "predicted_classes = np.argmax(batch_predictions, axis=1)\n",
    "\n",
    "# é¡¯ç¤ºé æ¸¬çµæœ\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': comparison_texts,\n",
    "    'Predicted_Class': [toxicity_labels[cls] for cls in predicted_classes],\n",
    "    'Confidence': [batch_predictions[i][predicted_classes[i]] for i in range(len(comparison_texts))],\n",
    "    'None_Prob': batch_predictions[:, 0],\n",
    "    'Toxic_Prob': batch_predictions[:, 1],\n",
    "    'Severe_Prob': batch_predictions[:, 2]\n",
    "})\n",
    "\n",
    "print(\"\\næ‰¹æ¬¡é æ¸¬çµæœ:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# è¨ˆç®—æ‰¹æ¬¡ SHAP å€¼\n",
    "print(\"\\nè¨ˆç®—æ‰¹æ¬¡ SHAP å€¼...\")\n",
    "try:\n",
    "    batch_shap_values = shap_explainer.explain_batch(comparison_texts)\n",
    "    print(f\"æ‰¹æ¬¡ SHAP å€¼è¨ˆç®—å®Œæˆ\")\n",
    "    \n",
    "    # æ‰¹æ¬¡å¯è¦–åŒ–\n",
    "    for i, text in enumerate(comparison_texts):\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        try:\n",
    "            shap.plots.text(batch_shap_values[i], display=False)\n",
    "            plt.title(f'æ–‡æœ¬ {i+1}: {text}\\né æ¸¬: {toxicity_labels[predicted_classes[i]]} ({batch_predictions[i][predicted_classes[i]]:.3f})')\n",
    "        except:\n",
    "            # æ‰‹å‹•å¯è¦–åŒ–\n",
    "            if len(batch_shap_values.values.shape) > 2:\n",
    "                current_shap_values = batch_shap_values.values[i, :, predicted_classes[i]]\n",
    "            else:\n",
    "                current_shap_values = batch_shap_values.values[i]\n",
    "            \n",
    "            tokens = list(text)\n",
    "            min_len = min(len(current_shap_values), len(tokens))\n",
    "            current_shap_values = current_shap_values[:min_len]\n",
    "            tokens = tokens[:min_len]\n",
    "            \n",
    "            colors = ['red' if val < 0 else 'green' for val in current_shap_values]\n",
    "            plt.barh(range(len(tokens)), current_shap_values, color=colors, alpha=0.7)\n",
    "            plt.yticks(range(len(tokens)), tokens)\n",
    "            plt.xlabel('SHAP Value')\n",
    "            plt.title(f'æ–‡æœ¬ {i+1}: \"{text}\"\\né æ¸¬: {toxicity_labels[predicted_classes[i]]} ({batch_predictions[i][predicted_classes[i]]:.3f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"æ‰¹æ¬¡å°æ¯”åˆ†æå¯è¦–åŒ–å®Œæˆ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"æ‰¹æ¬¡ SHAP åˆ†æå‡ºç¾éŒ¯èª¤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. çµ±è¨ˆåˆ†æèˆ‡æ¨¡å¼ç™¼ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æä¸åŒé¡å‹æ–‡æœ¬çš„ SHAP æ¨¡å¼\n",
    "print(\"çµ±è¨ˆåˆ†æèˆ‡æ¨¡å¼ç™¼ç¾...\")\n",
    "\n",
    "text_categories = {\n",
    "    'positive': [\"è¬è¬ä½ çš„å¹«åŠ©ï¼Œä½ äººçœŸå¥½ã€‚\", \"é€™å€‹ç”¢å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘å¾ˆå–œæ­¡ï¼\", \"ä»Šå¤©å¤©æ°£å¾ˆä¸éŒ¯ï¼Œå¿ƒæƒ…ä¹Ÿå¾ˆå¥½ã€‚\"],\n",
    "    'neutral': [\"æˆ‘ä»Šå¤©å»äº†è¶…å¸‚è²·æ±è¥¿ã€‚\", \"æœƒè­°å°‡åœ¨ä¸‹åˆä¸‰é»é–‹å§‹ã€‚\", \"é€™æ˜¯ä¸€å€‹é—œæ–¼æŠ€è¡“çš„è¨è«–ã€‚\"],\n",
    "    'toxic': [\"ä½ é€™å€‹ç™½ç™¡ï¼Œä»€éº¼éƒ½ä¸æ‡‚ï¼\", \"æ»¾é–‹ï¼Œä¸è¦ä¾†ç…©æˆ‘ï¼\", \"ä½ å€‘é€™äº›åƒåœ¾ï¼Œéƒ½çµ¦æˆ‘é–‰å˜´ï¼\"]\n",
    "}\n",
    "\n",
    "analysis_results = {}\n",
    "\n",
    "for category, texts in text_categories.items():\n",
    "    print(f\"\\nåˆ†æ {category} é¡æ–‡æœ¬...\")\n",
    "    \n",
    "    predictions = shap_explainer.predict_fn(texts)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    analysis_results[category] = {\n",
    "        'avg_toxic_prob': np.mean(predictions[:, 1]),\n",
    "        'avg_severe_prob': np.mean(predictions[:, 2]),\n",
    "        'predicted_classes': predicted_classes,\n",
    "        'class_distribution': np.bincount(predicted_classes, minlength=3)\n",
    "    }\n",
    "    \n",
    "    print(f\"  å¹³å‡æ¯’æ€§æ©Ÿç‡: {analysis_results[category]['avg_toxic_prob']:.4f}\")\n",
    "    print(f\"  å¹³å‡åš´é‡æ©Ÿç‡: {analysis_results[category]['avg_severe_prob']:.4f}\")\n",
    "    print(f\"  é¡åˆ¥åˆ†å¸ƒ: {dict(zip(toxicity_labels, analysis_results[category]['class_distribution']))}\")\n",
    "\n",
    "# å¯è¦–åŒ–çµ±è¨ˆçµæœ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "categories = list(text_categories.keys())\n",
    "toxic_probs = [analysis_results[cat]['avg_toxic_prob'] for cat in categories]\n",
    "severe_probs = [analysis_results[cat]['avg_severe_prob'] for cat in categories]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, toxic_probs, width, label='Toxic', alpha=0.7, color='orange')\n",
    "axes[0, 0].bar(x + width/2, severe_probs, width, label='Severe', alpha=0.7, color='red')\n",
    "axes[0, 0].set_xlabel('Text Category')\n",
    "axes[0, 0].set_ylabel('Average Probability')\n",
    "axes[0, 0].set_title('ä¸åŒæ–‡æœ¬é¡å‹çš„å¹³å‡æ¯’æ€§æ©Ÿç‡')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(categories)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# é¡åˆ¥åˆ†å¸ƒç†±åŠ›åœ–\n",
    "class_matrix = np.array([analysis_results[cat]['class_distribution'] for cat in categories])\n",
    "im = axes[0, 1].imshow(class_matrix, cmap='Blues', aspect='auto')\n",
    "axes[0, 1].set_xticks(range(len(toxicity_labels)))\n",
    "axes[0, 1].set_xticklabels(toxicity_labels)\n",
    "axes[0, 1].set_yticks(range(len(categories)))\n",
    "axes[0, 1].set_yticklabels(categories)\n",
    "axes[0, 1].set_title('é¡åˆ¥é æ¸¬åˆ†å¸ƒç†±åŠ›åœ–')\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(toxicity_labels)):\n",
    "        axes[0, 1].text(j, i, class_matrix[i, j], ha='center', va='center', \n",
    "                       color='white' if class_matrix[i, j] > class_matrix.max()/2 else 'black')\n",
    "\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/shap_statistical_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nçµ±è¨ˆåˆ†æå¯è¦–åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. èª¤åˆ¤æ¡ˆä¾‹è¨ºæ–·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# èª¤åˆ¤æ¡ˆä¾‹è¨ºæ–·\n",
    "print(\"èª¤åˆ¤æ¡ˆä¾‹è¨ºæ–·åˆ†æ...\")\n",
    "\n",
    "# é‚Šç•Œæ¡ˆä¾‹\n",
    "edge_cases = [\n",
    "    \"å“‡ï¼Œä½ çœŸæ˜¯å¤ªè°æ˜äº†ï¼\",  # è«·åˆº\n",
    "    \"é€™å€‹ç™½ç™¡ç¨‹å¼ç¢¼åˆå‡ºéŒ¯äº†\",  # æŒ‡ç‰©ä¸æŒ‡äºº\n",
    "    \"åƒåœ¾éƒµä»¶å¤ªå¤šäº†ï¼Œå¾ˆç…©äºº\",  # æŒ‡åƒåœ¾éƒµä»¶\n",
    "    \"æˆ‘çœŸçš„å¾ˆç”Ÿæ°£é€™å€‹æ±ºå®šï¼\",  # æƒ…ç·’æ¿€å‹•ä½†éæ¯’æ€§\n",
    "    \"ä½ çš„æƒ³æ³•æˆ‘ä¸å¤ªèªåŒ\",  # è¼•å¾®è² é¢ä½†éæ¯’æ€§\n",
    "    \"é€™å€‹æ”¿ç­–ç¢ºå¯¦æœ‰çˆ­è­°\"  # æ•æ„Ÿè©±é¡Œä½†éæ”»æ“Š\n",
    "]\n",
    "\n",
    "expected_labels = [1, 0, 0, 0, 0, 0]  # äººå·¥æ¨™è¨»\n",
    "\n",
    "edge_predictions = shap_explainer.predict_fn(edge_cases)\n",
    "edge_predicted_classes = np.argmax(edge_predictions, axis=1)\n",
    "\n",
    "# æ‰¾å‡ºèª¤åˆ¤æ¡ˆä¾‹\n",
    "misclassified_indices = []\n",
    "for i, (expected, predicted) in enumerate(zip(expected_labels, edge_predicted_classes)):\n",
    "    if (expected == 0 and predicted > 0) or (expected > 0 and predicted == 0):\n",
    "        misclassified_indices.append(i)\n",
    "\n",
    "print(f\"ç™¼ç¾ {len(misclassified_indices)} å€‹æ½›åœ¨èª¤åˆ¤æ¡ˆä¾‹\")\n",
    "\n",
    "if misclassified_indices:\n",
    "    print(\"\\nè©³ç´°èª¤åˆ¤åˆ†æ:\")\n",
    "    \n",
    "    misclass_data = []\n",
    "    for idx in misclassified_indices:\n",
    "        text = edge_cases[idx]\n",
    "        expected = toxicity_labels[expected_labels[idx]]\n",
    "        predicted = toxicity_labels[edge_predicted_classes[idx]]\n",
    "        confidence = edge_predictions[idx][edge_predicted_classes[idx]]\n",
    "        \n",
    "        misclass_data.append({\n",
    "            'Text': text,\n",
    "            'Expected': expected,\n",
    "            'Predicted': predicted,\n",
    "            'Confidence': confidence,\n",
    "            'Error_Type': 'False Positive' if expected_labels[idx] == 0 else 'False Negative'\n",
    "        })\n",
    "    \n",
    "    misclass_df = pd.DataFrame(misclass_data)\n",
    "    print(misclass_df.round(4))\n",
    "    \n",
    "    # SHAP åˆ†æèª¤åˆ¤æ¡ˆä¾‹\n",
    "    for i, idx in enumerate(misclassified_indices):\n",
    "        text = edge_cases[idx]\n",
    "        predicted_class = edge_predicted_classes[idx]\n",
    "        \n",
    "        try:\n",
    "            shap_values = shap_explainer.explain_text(text, predicted_class)\n",
    "            \n",
    "            if len(shap_values.values.shape) > 2:\n",
    "                current_shap_values = shap_values.values[0, :, predicted_class]\n",
    "            else:\n",
    "                current_shap_values = shap_values.values[0]\n",
    "            \n",
    "            tokens = list(text)\n",
    "            min_len = min(len(current_shap_values), len(tokens))\n",
    "            current_shap_values = current_shap_values[:min_len]\n",
    "            tokens = tokens[:min_len]\n",
    "            \n",
    "            plt.figure(figsize=(15, 4))\n",
    "            colors = ['red' if val < 0 else 'green' for val in current_shap_values]\n",
    "            plt.barh(range(len(tokens)), current_shap_values, color=colors, alpha=0.7)\n",
    "            plt.yticks(range(len(tokens)), tokens)\n",
    "            plt.xlabel('SHAP Value')\n",
    "            \n",
    "            error_type = misclass_data[i]['Error_Type']\n",
    "            expected = misclass_data[i]['Expected']\n",
    "            predicted = misclass_data[i]['Predicted']\n",
    "            \n",
    "            plt.title(f'èª¤åˆ¤æ¡ˆä¾‹ {i+1}: {error_type}\\n\"{text}\"\\næœŸæœ›: {expected} â†’ é æ¸¬: {predicted}')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"åˆ†æèª¤åˆ¤æ¡ˆä¾‹ {idx} æ™‚å‡ºéŒ¯: {e}\")\n",
    "    \n",
    "    # ä¿å­˜èª¤åˆ¤å ±å‘Š\n",
    "    misclass_df.to_csv('../data/processed/misclassification_report.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\nèª¤åˆ¤åˆ†æå ±å‘Šå·²ä¿å­˜\")\n",
    "    \n",
    "else:\n",
    "    print(\"åœ¨æ¸¬è©¦æ¡ˆä¾‹ä¸­æœªç™¼ç¾æ˜é¡¯èª¤åˆ¤\")\n",
    "\n",
    "# çµ±è¨ˆ\n",
    "overall_accuracy = np.mean(np.array(expected_labels) == edge_predicted_classes)\n",
    "false_positive_rate = np.mean((np.array(expected_labels) == 0) & (edge_predicted_classes > 0))\n",
    "false_negative_rate = np.mean((np.array(expected_labels) > 0) & (edge_predicted_classes == 0))\n",
    "\n",
    "print(f\"\\næ•´é«”æº–ç¢ºç‡: {overall_accuracy:.4f}\")\n",
    "print(f\"èª¤å ±ç‡: {false_positive_rate:.4f}\")\n",
    "print(f\"æ¼å ±ç‡: {false_negative_rate:.4f}\")\n",
    "\n",
    "print(\"\\nèª¤åˆ¤æ¡ˆä¾‹è¨ºæ–·å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ç¸½çµå ±å‘Šèˆ‡å»ºè­°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆç¸½çµå ±å‘Š\n",
    "print(\"=\" * 60)\n",
    "print(\"SHAP å¯è§£é‡‹æ€§åˆ†æç¸½çµå ±å‘Š\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "report_data = {\n",
    "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model_info': {\n",
    "        'base_model': model_name,\n",
    "        'task_types': ['toxicity', 'emotion', 'bullying', 'role'],\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'shap_config': {\n",
    "        'explainer_type': 'Partition',\n",
    "        'max_evals': 100,\n",
    "        'background_samples': len(background_texts)\n",
    "    },\n",
    "    'analysis_results': {\n",
    "        'total_texts_analyzed': len(test_texts),\n",
    "        'edge_cases_tested': len(edge_cases),\n",
    "        'misclassified_cases': len(misclassified_indices) if 'misclassified_indices' in locals() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š åˆ†ææ¦‚æ³:\")\n",
    "print(f\"  åˆ†ææ™‚é–“: {report_data['analysis_date']}\")\n",
    "print(f\"  åŸºç¤æ¨¡å‹: {report_data['model_info']['base_model']}\")\n",
    "print(f\"  æ¨¡å‹åƒæ•¸é‡: {report_data['model_info']['num_parameters']:,}\")\n",
    "print(f\"  è¨ˆç®—è¨­å‚™: {report_data['model_info']['device']}\")\n",
    "\n",
    "print(f\"\\nğŸ” SHAP é…ç½®:\")\n",
    "print(f\"  è§£é‡‹å™¨é¡å‹: {report_data['shap_config']['explainer_type']} Explainer\")\n",
    "print(f\"  æœ€å¤§è©•ä¼°æ¬¡æ•¸: {report_data['shap_config']['max_evals']}\")\n",
    "print(f\"  èƒŒæ™¯æ¨£æœ¬æ•¸: {report_data['shap_config']['background_samples']}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ä¸»è¦ç™¼ç¾:\")\n",
    "print(f\"  âœ… SHAP Partition explainer æˆåŠŸæ‡‰ç”¨æ–¼ä¸­æ–‡æ¯’æ€§åµæ¸¬\")\n",
    "print(f\"  âœ… shap.plots.text æä¾›ç›´è§€çš„æ–‡æœ¬å¯è¦–åŒ–\")\n",
    "print(f\"  âœ… æ‰¹æ¬¡å°æ¯”åˆ†ææ­ç¤ºä¸åŒæ–‡æœ¬é¡å‹çš„æ¨¡å¼å·®ç•°\")\n",
    "print(f\"  âœ… èª¤åˆ¤æ¡ˆä¾‹è¨ºæ–·æœ‰åŠ©æ–¼æ¨¡å‹æ”¹é€²\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ä½¿ç”¨å»ºè­°:\")\n",
    "print(f\"  â€¢ ä½¿ç”¨ shap.plots.text() é€²è¡Œæ¨™æº–åŒ–æ–‡æœ¬å¯è¦–åŒ–\")\n",
    "print(f\"  â€¢ çµåˆ Integrated Gradients é€²è¡Œäº¤å‰é©—è­‰\")\n",
    "print(f\"  â€¢ å®šæœŸæ›´æ–°èƒŒæ™¯æ¨£æœ¬ä»¥æé«˜è§£é‡‹å“è³ª\")\n",
    "print(f\"  â€¢ å°‡ SHAP åˆ†æç´å…¥æ¨¡å‹ç›£æ§æµç¨‹\")\n",
    "\n",
    "# ä¿å­˜å ±å‘Š\n",
    "import json\n",
    "with open('../data/processed/shap_analysis_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nğŸ“‹ å®Œæ•´å ±å‘Šå·²ä¿å­˜: ../data/processed/shap_analysis_report.json\")\n",
    "print(f\"\\nåƒè€ƒè³‡æº:\")\n",
    "print(f\"  â€¢ SHAP å®˜æ–¹æ–‡æª”: https://shap.readthedocs.io/\")\n",
    "print(f\"  â€¢ Transformer è§£é‡‹æ•™ç¨‹: https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/\")\n",
    "print(f\"  â€¢ CyberPuppy å°ˆæ¡ˆ: ../src/cyberpuppy/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SHAP å¯è§£é‡‹æ€§åˆ†æå®Œæˆï¼\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}