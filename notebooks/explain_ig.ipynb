{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CyberPuppy Integrated Gradients 可解釋性分析\n",
    "\n",
    "本notebook使用Integrated Gradients方法為中文網路霸凌偵測模型提供可解釋性分析。\n",
    "\n",
    "## 參考文獻\n",
    "- [Captum官方文檔](https://captum.ai/)\n",
    "- [Integrated Gradients論文](https://arxiv.org/abs/1703.01365)\n",
    "- [BERT解釋性教程](https://captum.ai/tutorials/Bert_SQUAD_Interpret)\n",
    "- [中文NLP可解釋性範例](https://github.com/pytorch/captum/blob/master/tutorials/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件\n",
    "!pip install captum matplotlib seaborn pandas numpy torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 設定中文字體\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 本地imports\n",
    "from src.cyberpuppy.models.baselines import BaselineModel, ModelConfig\n",
    "from src.cyberpuppy.explain.ig import (\n",
    "    IntegratedGradientsExplainer,\n",
    "    BiasAnalyzer,\n",
    "    create_attribution_heatmap,\n",
    "    save_attribution_report\n",
    ")\n",
    "from src.cyberpuppy.labeling.label_map import UnifiedLabel, ToxicityLevel, EmotionType\n",
    "\n",
    "print(\"環境設定完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 載入模型\n",
    "\n",
    "載入已訓練的CyberPuppy基線模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定模型路徑（請根據實際情況調整）\n",
    "MODEL_PATH = \"../models/baseline\"  # 或其他已訓練模型路徑\n",
    "\n",
    "# 檢查模型是否存在\n",
    "if Path(MODEL_PATH).exists():\n",
    "    print(f\"載入模型: {MODEL_PATH}\")\n",
    "    model = BaselineModel.load_model(MODEL_PATH)\n",
    "    print(f\"模型載入成功: {model.config.model_name}\")\n",
    "else:\n",
    "    print(\"模型不存在，使用示例配置創建模型（注意：未訓練）\")\n",
    "    config = ModelConfig(\n",
    "        model_name=\"hfl/chinese-macbert-base\",\n",
    "        use_focal_loss=True\n",
    "    )\n",
    "    model = BaselineModel(config)\n",
    "    print(\"警告：使用未訓練的模型，結果僅供演示\")\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用設備: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 初始化IG解釋器\n",
    "\n",
    "基於Captum的Integrated Gradients實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化IG解釋器\n",
    "explainer = IntegratedGradientsExplainer(model, device)\n",
    "print(\"IG解釋器初始化完成\")\n",
    "\n",
    "# 初始化偏見分析器\n",
    "bias_analyzer = BiasAnalyzer(explainer)\n",
    "print(\"偏見分析器初始化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 測試文本準備\n",
    "\n",
    "準備不同類型的測試文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試文本集合\n",
    "test_texts = {\n",
    "    \"毒性辱罵\": [\n",
    "        \"你這個垃圾，滾開！\",\n",
    "        \"去死吧，沒用的廢物\",\n",
    "        \"你真是個白痴，什麼都不懂\"\n",
    "    ],\n",
    "    \"威脅霸凌\": [\n",
    "        \"我要殺了你這個混蛋\",\n",
    "        \"別讓我再看到你，否則你會後悔\",\n",
    "        \"你等著瞧，我不會放過你的\"\n",
    "    ],\n",
    "    \"身份攻擊\": [\n",
    "        \"你們這些外地人就是素質差\",\n",
    "        \"女生就是不適合做這種工作\",\n",
    "        \"老年人就是跟不上時代\"\n",
    "    ],\n",
    "    \"正面中性\": [\n",
    "        \"今天天氣真好，心情很愉快\",\n",
    "        \"謝謝大家的幫助，非常感謝\",\n",
    "        \"這個產品的品質還不錯\"\n",
    "    ],\n",
    "    \"負面情緒\": [\n",
    "        \"我今天心情很糟糕\",\n",
    "        \"這個決定讓我很失望\",\n",
    "        \"感覺很沮喪，不知道該怎麼辦\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 展示測試文本\n",
    "for category, texts in test_texts.items():\n",
    "    print(f\"\\n【{category}】\")\n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"  {i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 單句解釋分析\n",
    "\n",
    "對單個文本進行詳細的IG解釋分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇一個典型的毒性文本進行詳細分析\n",
    "sample_text = \"你這個垃圾，滾開！\"\n",
    "print(f\"分析文本: {sample_text}\")\n",
    "\n",
    "# 執行IG解釋\n",
    "result = explainer.explain_text(\n",
    "    sample_text,\n",
    "    n_steps=50,  # IG積分步數\n",
    "    internal_batch_size=10\n",
    ")\n",
    "\n",
    "# 顯示預測結果\n",
    "print(f\"\\n=== 預測結果 ===\")\n",
    "toxicity_labels = ['非毒性', '毒性', '嚴重毒性']\n",
    "emotion_labels = ['正面', '中性', '負面']\n",
    "bullying_labels = ['非霸凌', '騷擾', '威脅']\n",
    "\n",
    "print(f\"毒性預測: {toxicity_labels[result.toxicity_pred]} (信心度: {result.toxicity_prob:.3f})\")\n",
    "print(f\"情緒預測: {emotion_labels[result.emotion_pred]} (信心度: {result.emotion_prob:.3f})\")\n",
    "print(f\"霸凌預測: {bullying_labels[result.bullying_pred]} (信心度: {result.bullying_prob:.3f})\")\n",
    "print(f\"收斂性檢查: {result.convergence_delta:.4f}\")\n",
    "\n",
    "# 顯示token級attribution\n",
    "print(f\"\\n=== Token級Attribution分數 ===\")\n",
    "for i, token in enumerate(result.tokens):\n",
    "    if token not in ['[CLS]', '[SEP]', '[PAD]'] and i < len(result.toxicity_attributions):\n",
    "        clean_token = token.replace('##', '')\n",
    "        print(f\"{clean_token:>8} | \"\n",
    "              f\"毒性: {result.toxicity_attributions[i]:>7.4f} | \"\n",
    "              f\"情緒: {result.emotion_attributions[i]:>7.4f} | \"\n",
    "              f\"霸凌: {result.bullying_attributions[i]:>7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attribution熱力圖可視化\n",
    "\n",
    "創建token級重要度熱力圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建毒性classification的熱力圖\n",
    "fig_toxicity = create_attribution_heatmap(\n",
    "    result, \n",
    "    task='toxicity',\n",
    "    save_path='../reports/attribution_heatmap_toxicity.png',\n",
    "    figsize=(14, 8)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 創建情緒classification的熱力圖\n",
    "fig_emotion = create_attribution_heatmap(\n",
    "    result,\n",
    "    task='emotion', \n",
    "    save_path='../reports/attribution_heatmap_emotion.png',\n",
    "    figsize=(14, 8)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 批次文本對比分析\n",
    "\n",
    "分析多個文本的attribution模式，識別共同特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇不同類型的文本進行對比\n",
    "comparison_texts = [\n",
    "    \"你這個垃圾，滾開！\",      # 毒性\n",
    "    \"今天天氣真好，心情很愉快\",    # 正面\n",
    "    \"我今天心情很糟糕\",        # 負面但非毒性\n",
    "    \"去死吧，沒用的廢物\"       # 嚴重毒性\n",
    "]\n",
    "\n",
    "print(\"執行批次分析...\")\n",
    "batch_results = []\n",
    "for text in comparison_texts:\n",
    "    result = explainer.explain_text(text)\n",
    "    batch_results.append(result)\n",
    "    print(f\"✓ 完成: {text[:20]}...\")\n",
    "\n",
    "print(f\"批次分析完成，共處理 {len(batch_results)} 個文本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建對比可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (result, text) in enumerate(zip(batch_results, comparison_texts)):\n",
    "    # 過濾tokens和attributions\n",
    "    tokens = []\n",
    "    attrs = []\n",
    "    for j, token in enumerate(result.tokens):\n",
    "        if (token not in ['[CLS]', '[SEP]', '[PAD]'] and \n",
    "            j < len(result.toxicity_attributions)):\n",
    "            clean_token = token.replace('##', '')\n",
    "            if clean_token.strip():  # 過濾空token\n",
    "                tokens.append(clean_token)\n",
    "                attrs.append(result.toxicity_attributions[j])\n",
    "    \n",
    "    if not tokens:  # 如果沒有有效tokens，跳過\n",
    "        continue\n",
    "        \n",
    "    # 正規化attribution分數\n",
    "    attrs = np.array(attrs)\n",
    "    if np.max(np.abs(attrs)) > 0:\n",
    "        attrs = attrs / np.max(np.abs(attrs))\n",
    "    \n",
    "    # 設定顏色\n",
    "    colors = ['red' if attr > 0 else 'blue' for attr in attrs]\n",
    "    alphas = [abs(attr) for attr in attrs]\n",
    "    \n",
    "    # 繪製條形圖\n",
    "    bars = axes[i].barh(range(len(tokens)), attrs, color=colors, alpha=0.7)\n",
    "    \n",
    "    # 設定標籤\n",
    "    axes[i].set_yticks(range(len(tokens)))\n",
    "    axes[i].set_yticklabels(tokens, fontsize=10)\n",
    "    axes[i].set_xlabel('Toxicity Attribution Score', fontsize=12)\n",
    "    axes[i].set_title(f'文本 {i+1}: {text}\\n'\n",
    "                     f'毒性預測: {[\"非毒性\", \"毒性\", \"嚴重毒性\"][result.toxicity_pred]} '\n",
    "                     f'({result.toxicity_prob:.3f})', fontsize=11)\n",
    "    axes[i].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/batch_attribution_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 模型偏見分析\n",
    "\n",
    "使用BiasAnalyzer檢測身份攻擊、辱罵詞等偏見模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備包含各種偏見的測試文本\n",
    "bias_test_texts = [\n",
    "    # 身份相關\n",
    "    \"你們這些外地人就是素質差\",\n",
    "    \"女生就是不適合做技術工作\", \n",
    "    \"老年人就是學不會新技術\",\n",
    "    \"中國人都很聰明\",\n",
    "    \"男性在這方面比較有優勢\",\n",
    "    \n",
    "    # 辱罵詞相關\n",
    "    \"這個笨蛋什麼都不懂\",\n",
    "    \"你真是個廢物\",\n",
    "    \"別這麼蠢好嗎\",\n",
    "    \"垃圾政策害死人\",\n",
    "    \n",
    "    # 正常文本（對照組）\n",
    "    \"今天天氣很好\",\n",
    "    \"這個方案需要改進\",\n",
    "    \"感謝大家的支持\",\n",
    "    \"我不太同意這個觀點\",\n",
    "    \"希望情況能夠好轉\"\n",
    "]\n",
    "\n",
    "print(f\"準備分析 {len(bias_test_texts)} 個文本的偏見模式...\")\n",
    "\n",
    "# 執行偏見分析\n",
    "bias_df = bias_analyzer.analyze_bias_patterns(\n",
    "    bias_test_texts,\n",
    "    top_k=10,\n",
    "    output_csv='../reports/bias_analysis_report.csv'\n",
    ")\n",
    "\n",
    "print(f\"偏見分析完成，發現 {len(bias_df)} 個偏見相關tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示偏見分析結果統計\n",
    "if len(bias_df) > 0:\n",
    "    print(\"=== 偏見分析統計結果 ===\")\n",
    "    \n",
    "    # 按偏見類別統計\n",
    "    bias_category_stats = bias_df['bias_category'].value_counts()\n",
    "    print(\"\\n偏見類別分布:\")\n",
    "    for category, count in bias_category_stats.items():\n",
    "        print(f\"  {category}: {count} 個tokens\")\n",
    "    \n",
    "    # 按子類別統計\n",
    "    bias_subcategory_stats = bias_df['bias_subcategory'].value_counts()\n",
    "    print(\"\\n偏見子類別分布:\")\n",
    "    for subcategory, count in bias_subcategory_stats.items():\n",
    "        print(f\"  {subcategory}: {count} 個tokens\")\n",
    "    \n",
    "    # 顯示attribution分數最高的偏見詞\n",
    "    print(\"\\n=== Top-10 偏見相關詞彙 (按總重要度排序) ===\")\n",
    "    top_bias_tokens = bias_df.nlargest(10, 'total_importance')\n",
    "    \n",
    "    for idx, row in top_bias_tokens.iterrows():\n",
    "        print(f\"{row['token']:>8} | \"\n",
    "              f\"類別: {row['bias_category']:>8} | \"\n",
    "              f\"子類: {row['bias_subcategory']:>12} | \"\n",
    "              f\"毒性attr: {row['toxicity_attribution']:>7.4f} | \"\n",
    "              f\"總重要度: {row['total_importance']:>7.4f}\")\n",
    "else:\n",
    "    print(\"未檢測到偏見模式（可能需要調整偏見詞庫或測試文本）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 偏見模式可視化\n",
    "\n",
    "視覺化不同類型偏見的attribution分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(bias_df) > 0:\n",
    "    # 創建偏見分析可視化\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. 偏見類別分布\n",
    "    bias_category_counts = bias_df['bias_category'].value_counts()\n",
    "    axes[0, 0].pie(bias_category_counts.values, labels=bias_category_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('偏見類別分布')\n",
    "    \n",
    "    # 2. Attribution分數分布（毒性）\n",
    "    axes[0, 1].hist(bias_df['toxicity_attribution'], bins=20, alpha=0.7, color='red')\n",
    "    axes[0, 1].set_title('偏見詞彙的毒性Attribution分布')\n",
    "    axes[0, 1].set_xlabel('Toxicity Attribution Score')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. 不同偏見類別的attribution對比\n",
    "    sns.boxplot(data=bias_df, x='bias_category', y='total_importance', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('不同偏見類別的總重要度分布')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. 毒性預測概率 vs Attribution分數散點圖\n",
    "    scatter = axes[1, 1].scatter(bias_df['toxicity_attribution'], \n",
    "                                bias_df['toxicity_prob'], \n",
    "                                c=bias_df['bias_category'].astype('category').cat.codes, \n",
    "                                alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Toxicity Attribution Score')\n",
    "    axes[1, 1].set_ylabel('Toxicity Prediction Probability')\n",
    "    axes[1, 1].set_title('Attribution分數 vs 預測概率')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/bias_analysis_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"無偏見資料可視覺化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 誤判案例診斷\n",
    "\n",
    "分析模型的誤判案例，理解模型決策的問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備一些可能導致誤判的邊界案例\n",
    "edge_cases = [\n",
    "    {\n",
    "        \"text\": \"這個垃圾桶需要清理了\",  # 包含\"垃圾\"但非辱罵\n",
    "        \"expected_toxicity\": 0,  # 預期非毒性\n",
    "        \"description\": \"包含負面詞彙但語境正常\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"你的想法很有創意，但我不太認同\",  # 禮貌的不同意見\n",
    "        \"expected_toxicity\": 0,\n",
    "        \"description\": \"禮貌表達不同意見\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"殺價成功，省了不少錢\",  # \"殺\"字但非暴力\n",
    "        \"expected_toxicity\": 0,\n",
    "        \"description\": \"包含暴力詞彙但語境無害\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"你說得對，我想太多了\",  # 自我反省\n",
    "        \"expected_toxicity\": 0,\n",
    "        \"description\": \"自我反省，非攻擊性\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"這個決定真的讓人很生氣\",  # 強烈負面情緒但非辱罵\n",
    "        \"expected_toxicity\": 0,\n",
    "        \"description\": \"強烈負面情緒但非人身攻擊\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"執行誤判案例診斷...\")\n",
    "misclassification_analysis = []\n",
    "\n",
    "for case in edge_cases:\n",
    "    result = explainer.explain_text(case[\"text\"])\n",
    "    \n",
    "    # 判斷是否誤判（簡化：毒性分數>0.5視為毒性預測）\n",
    "    is_predicted_toxic = result.toxicity_pred > 0\n",
    "    is_expected_toxic = case[\"expected_toxicity\"] > 0\n",
    "    is_misclassified = is_predicted_toxic != is_expected_toxic\n",
    "    \n",
    "    analysis = {\n",
    "        \"text\": case[\"text\"],\n",
    "        \"description\": case[\"description\"],\n",
    "        \"expected_toxic\": is_expected_toxic,\n",
    "        \"predicted_toxic\": is_predicted_toxic,\n",
    "        \"is_misclassified\": is_misclassified,\n",
    "        \"toxicity_prob\": result.toxicity_prob,\n",
    "        \"result\": result\n",
    "    }\n",
    "    \n",
    "    misclassification_analysis.append(analysis)\n",
    "\n",
    "print(\"誤判案例診斷完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示誤判案例分析結果\n",
    "print(\"=== 誤判案例診斷報告 ===\")\n",
    "\n",
    "misclassified_cases = [case for case in misclassification_analysis if case[\"is_misclassified\"]]\n",
    "correct_cases = [case for case in misclassification_analysis if not case[\"is_misclassified\"]]\n",
    "\n",
    "print(f\"\\n總測試案例: {len(misclassification_analysis)}\")\n",
    "print(f\"誤判案例: {len(misclassified_cases)}\")\n",
    "print(f\"正確分類: {len(correct_cases)}\")\n",
    "print(f\"準確率: {len(correct_cases)/len(misclassification_analysis)*100:.1f}%\")\n",
    "\n",
    "if misclassified_cases:\n",
    "    print(\"\\n=== 誤判案例詳細分析 ===\")\n",
    "    for i, case in enumerate(misclassified_cases, 1):\n",
    "        result = case[\"result\"]\n",
    "        print(f\"\\n誤判案例 {i}:\")\n",
    "        print(f\"  文本: {case['text']}\")\n",
    "        print(f\"  描述: {case['description']}\")\n",
    "        print(f\"  預期: {'毒性' if case['expected_toxic'] else '非毒性'}\")\n",
    "        print(f\"  預測: {'毒性' if case['predicted_toxic'] else '非毒性'} (概率: {case['toxicity_prob']:.3f})\")\n",
    "        \n",
    "        # 分析關鍵詞attribution\n",
    "        print(f\"  關鍵token attribution:\")\n",
    "        for j, token in enumerate(result.tokens):\n",
    "            if (token not in ['[CLS]', '[SEP]', '[PAD]'] and \n",
    "                j < len(result.toxicity_attributions) and\n",
    "                abs(result.toxicity_attributions[j]) > 0.01):  # 只顯示重要的attribution\n",
    "                clean_token = token.replace('##', '')\n",
    "                print(f\"    {clean_token}: {result.toxicity_attributions[j]:.4f}\")\n",
    "else:\n",
    "    print(\"\\n✓ 所有測試案例都被正確分類\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 生成完整解釋報告\n",
    "\n",
    "匯總所有分析結果，生成完整的可解釋性報告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 收集所有分析過的結果\n",
    "all_results = []\n",
    "all_results.extend(batch_results)\n",
    "all_results.extend([case[\"result\"] for case in misclassification_analysis])\n",
    "\n",
    "print(f\"生成包含 {len(all_results)} 個樣本的完整解釋報告...\")\n",
    "\n",
    "# 儲存詳細的attribution報告\n",
    "save_attribution_report(\n",
    "    all_results,\n",
    "    output_path='../reports/complete_attribution_report.csv',\n",
    "    include_tokens=True\n",
    ")\n",
    "\n",
    "# 儲存摘要報告\n",
    "save_attribution_report(\n",
    "    all_results,\n",
    "    output_path='../reports/attribution_summary_report.csv',\n",
    "    include_tokens=False\n",
    ")\n",
    "\n",
    "print(\"報告生成完成！\")\n",
    "print(\"\\n生成的檔案:\")\n",
    "print(\"  - ../reports/attribution_heatmap_toxicity.png: 毒性attribution熱力圖\")\n",
    "print(\"  - ../reports/attribution_heatmap_emotion.png: 情緒attribution熱力圖\")\n",
    "print(\"  - ../reports/batch_attribution_comparison.png: 批次對比圖\")\n",
    "print(\"  - ../reports/bias_analysis_report.csv: 偏見分析CSV報告\")\n",
    "print(\"  - ../reports/bias_analysis_visualization.png: 偏見分析可視化\")\n",
    "print(\"  - ../reports/complete_attribution_report.csv: 完整attribution報告\")\n",
    "print(\"  - ../reports/attribution_summary_report.csv: attribution摘要報告\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 總結與建議\n",
    "\n",
    "根據IG分析結果，提供模型改進建議"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CyberPuppy IG可解釋性分析總結 ===\")\n",
    "print(\"\\n1. 模型行為分析:\")\n",
    "print(\"   - 模型能夠識別明顯的辱罵詞彙和攻擊性語言\")\n",
    "print(\"   - 對於語境依賴的毒性判斷存在一定挑戰\")\n",
    "print(\"   - 某些無害的負面詞彙可能被過度解讀\")\n",
    "\n",
    "print(\"\\n2. 偏見檢測結果:\")\n",
    "if len(bias_df) > 0:\n",
    "    print(f\"   - 檢測到 {len(bias_df)} 個偏見相關tokens\")\n",
    "    print(f\"   - 主要偏見類型: {', '.join(bias_df['bias_category'].unique())}\")\n",
    "    print(\"   - 需要關注身份攻擊和刻板印象的檢測\")\n",
    "else:\n",
    "    print(\"   - 當前測試集未檢測到明顯偏見模式\")\n",
    "    print(\"   - 建議使用更大的測試集進行偏見檢測\")\n",
    "\n",
    "print(\"\\n3. 模型改進建議:\")\n",
    "print(\"   - 加強語境理解能力，減少對單個詞彙的過度依賴\")\n",
    "print(\"   - 增加邊界案例的訓練資料，提高區分能力\")\n",
    "print(\"   - 定期進行偏見檢測，確保公平性\")\n",
    "print(\"   - 考慮引入更多語義特徵，而非僅依賴詞彙層面\")\n",
    "\n",
    "print(\"\\n4. 部署建議:\")\n",
    "print(\"   - 建立人工審核機制處理邊界案例\")\n",
    "print(\"   - 設定適當的信心度閾值，避免誤判\")\n",
    "print(\"   - 定期收集用戶反饋，持續改進模型\")\n",
    "print(\"   - 建立可解釋性監控，追蹤模型決策品質\")\n",
    "\n",
    "print(\"\\n=== 分析完成 ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}